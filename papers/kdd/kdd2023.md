# KDD2023 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[How Transitive Are Real-World Group Interactions? - Measurement and Reproduction](https://doi.org/10.1145/3580305.3599382)|Sunwoo Kim, Fanchen Bu, Minyoung Choe, Jaemin Yoo, Kijung Shin|KAIST; Carnegie Mellon University|Many real-world interactions (e.g., researcher collaborations and email communication) occur among multiple entities. These group interactions are naturally modeled as hypergraphs. In graphs, transitivity is helpful to understand the connections between node pairs sharing a neighbor, and it has extensive applications in various domains. Hypergraphs, an extension of graphs, are designed to represent group relations. However, to the best of our knowledge, there has been no examination regarding the transitivity of real-world group interactions. In this work, we investigate the transitivity of group interactions in real-world hypergraphs. We first suggest intuitive axioms as necessary characteristics of hypergraph transitivity measures. Then, we propose a principled hypergraph transitivity measure HyperTrans, which satisfies all the proposed axioms, with a fast computation algorithm Fast-HyperTrans. After that, we analyze the transitivity patterns in real-world hypergraphs distinguished from those in random hypergraphs. Lastly, we propose a scalable hypergraph generator THera. It reproduces the observed transitivity patterns by leveraging community structures, which are pervasive in real-world hypergraphs. Our code and datasets are available at https://github.com/kswoo97/hypertrans.|许多真实世界的交互(例如，研究人员协作和电子邮件通信)发生在多个实体之间。这些群体的相互作用自然地被建模为超图。在图中，传递性有助于理解共享邻居的节点对之间的联系，在各个领域有着广泛的应用。超图是图的一个扩展，被设计用来表示群关系。然而，据我们所知，还没有关于现实世界群体交互传递性的研究。在这项工作中，我们研究了现实世界超图中群相互作用的传递性。我们首先提出直观公理作为超图传递度量的必要特征。然后，我们提出了一个原则性的超图传递度度量 HyperTrans，它满足所提出的所有公理，并使用快速计算算法 Fast-HyperTrans。在此基础上，分析了现实超图与随机超图的传递模式。最后，我们提出了一个可扩展的超图生成器 THera。它通过利用在现实世界超图中普遍存在的社区结构来重现观察到的传递性模式。我们的代码和数据集 https://github.com/kswoo97/hypertrans 可用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Transitive+Are+Real-World+Group+Interactions?+-+Measurement+and+Reproduction)|1|
|[Temporal Dynamics-Aware Adversarial Attacks on Discrete-Time Dynamic Graph Models](https://doi.org/10.1145/3580305.3599517)|Kartik Sharma, Rakshit Trivedi, Rohit Sridhar, Srijan Kumar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Dynamics-Aware+Adversarial+Attacks+on+Discrete-Time+Dynamic+Graph+Models)|1|
|[Contrastive Cross-scale Graph Knowledge Synergy](https://doi.org/10.1145/3580305.3599286)|Yifei Zhang, Yankai Chen, Zixing Song, Irwin King||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Cross-scale+Graph+Knowledge+Synergy)|1|
|[FedMultimodal: A Benchmark for Multimodal Federated Learning](https://doi.org/10.1145/3580305.3599825)|Tiantian Feng, Digbalay Bose, Tuo Zhang, Rajat Hebbar, Anil Ramakrishna, Rahul Gupta, Mi Zhang, Salman Avestimehr, Shrikanth Narayanan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedMultimodal:+A+Benchmark+for+Multimodal+Federated+Learning)|1|
|[Querywise Fair Learning to Rank through Multi-Objective Optimization](https://doi.org/10.1145/3580305.3599482)|Debabrata Mahapatra, Chaosheng Dong, Michinari Momma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Querywise+Fair+Learning+to+Rank+through+Multi-Objective+Optimization)|0|
|[E-commerce Search via Content Collaborative Graph Neural Network](https://doi.org/10.1145/3580305.3599320)|Guipeng Xv, Chen Lin, Wanxian Guan, Jinping Gou, Xubin Li, Hongbo Deng, Jian Xu, Bo Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=E-commerce+Search+via+Content+Collaborative+Graph+Neural+Network)|0|
|[Cognitive Evolutionary Search to Select Feature Interactions for Click-Through Rate Prediction](https://doi.org/10.1145/3580305.3599277)|Runlong Yu, Xiang Xu, Yuyang Ye, Qi Liu, Enhong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cognitive+Evolutionary+Search+to+Select+Feature+Interactions+for+Click-Through+Rate+Prediction)|0|
|[Binary Embedding-based Retrieval at Tencent](https://doi.org/10.1145/3580305.3599782)|Yukang Gan, Yixiao Ge, Chang Zhou, Shupeng Su, Zhouchuan Xu, Xuyuan Xu, Quanchao Hui, Xiang Chen, Yexin Wang, Ying Shan|Tencent|Large-scale embedding-based retrieval (EBR) is the cornerstone of search-related industrial applications. Given a user query, the system of EBR aims to identify relevant information from a large corpus of documents that may be tens or hundreds of billions in size. The storage and computation turn out to be expensive and inefficient with massive documents and high concurrent queries, making it difficult to further scale up. To tackle the challenge, we propose a binary embedding-based retrieval (BEBR) engine equipped with a recurrent binarization algorithm that enables customized bits per dimension. Specifically, we compress the full-precision query and document embeddings, formulated as float vectors in general, into a composition of multiple binary vectors using a lightweight transformation model with residual multilayer perception (MLP) blocks. We can therefore tailor the number of bits for different applications to trade off accuracy loss and cost savings. Importantly, we enable task-agnostic efficient training of the binarization model using a new embedding-to-embedding strategy. We also exploit the compatible training of binary embeddings so that the BEBR engine can support indexing among multiple embedding versions within a unified system. To further realize efficient search, we propose Symmetric Distance Calculation (SDC) to achieve lower response time than Hamming codes. We successfully employed the introduced BEBR to Tencent products, including Sogou, Tencent Video, QQ World, etc. The binarization algorithm can be seamlessly generalized to various tasks with multiple modalities. Extensive experiments on offline benchmarks and online A/B tests demonstrate the efficiency and effectiveness of our method, significantly saving 30%~50% index costs with almost no loss of accuracy at the system level.|大规模嵌入式检索(EBR)是搜索相关工业应用的基石。给定一个用户查询，EBR 系统的目标是从大量文档中识别相关信息，这些文档的规模可能达到数百亿或数千亿。由于大量文档和高并发查询，存储和计算成本高、效率低，难以进一步扩展。为了解决这一问题，我们提出了一种基于二进制嵌入的检索引擎(BEBR) ，该引擎配备了一个循环二进制算法，可以实现每维定制位。具体地说，我们使用带有剩余多层感知(MLP)块的轻量级变换模型，将通常表示为浮点向量的全精度查询和文档嵌入压缩为多个二进制向量的组合。因此，我们可以为不同的应用程序量身定制位数，以权衡精度损失和成本节约。重要的是，我们使任务无关的二值化模型的有效训练使用一种新的嵌入到嵌入策略。我们还利用二进制嵌入的兼容性训练，使 BEBR 引擎能够在一个统一的系统中支持多个嵌入版本之间的索引。为了进一步实现有效的搜索，我们提出了对称距离计算(SDC)来实现比汉明码更低的响应时间。我们成功地将引进的 BEBR 引入腾讯产品，包括搜狗、腾讯视频、 QQ 世界等。二值化算法可以无缝地推广到具有多种模式的各种任务。对离线基准测试和在线 A/B 测试的大量实验证明了该方法的有效性和有效性，显著节省了30% ~ 50% 的指标成本，在系统级几乎没有准确性的损失。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Binary+Embedding-based+Retrieval+at+Tencent)|0|
|[Optimizing Airbnb Search Journey with Multi-task Learning](https://doi.org/10.1145/3580305.3599881)|Chun How Tan, Austin Chan, Malay Haldar, Jie Tang, Xin Liu, Mustafa Abdool, Huiji Gao, Liwei He, Sanjeev Katariya|Airbnb Inc.|At Airbnb, an online marketplace for stays and experiences, guests often spend weeks exploring and comparing multiple items before making a final reservation request. Each reservation request may then potentially be rejected or cancelled by the host prior to check-in. The long and exploratory nature of the search journey, as well as the need to balance both guest and host preferences, present unique challenges for Airbnb search ranking. In this paper, we present Journey Ranker, a new multi-task deep learning model architecture that addresses these challenges. Journey Ranker leverages intermediate guest actions as milestones, both positive and negative, to better progress the guest towards a successful booking. It also uses contextual information such as guest state and search query to balance guest and host preferences. Its modular and extensible design, consisting of four modules with clear separation of concerns, allows for easy application to use cases beyond the Airbnb search ranking context. We conducted offline and online testing of the Journey Ranker and successfully deployed it in production to four different Airbnb products with significant business metrics improvements.|Airbnb 是一家提供住宿和体验服务的在线市场，在提出最终预订请求之前，客人通常要花费数周时间来探索和比较多个项目。然后，主机可能会在签入之前拒绝或取消每个预订请求。漫长而探索性的搜索旅程，以及平衡客人和主人偏好的需要，为 Airbnb 的搜索排名提出了独特的挑战。在本文中，我们提出了一个新的多任务深度学习模型体系结构 Journey Ranker，以解决这些挑战。Journey Ranker 利用中间的客人行为作为里程碑，包括积极的和消极的，以更好地推动客人成功预订。它还使用上下文信息(如来宾状态和搜索查询)来平衡来宾和主机的首选项。它的模块化和可扩展的设计，由四个模块组成，具有明确的关注点分离，可以很容易地应用到 Airbnb 搜索排名上下文之外的用例。我们对 Journey Ranker 进行了离线和在线测试，并成功地将其部署到四个不同的 Airbnb 产品上，并对业务指标进行了重大改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Airbnb+Search+Journey+with+Multi-task+Learning)|0|
|[User-Regulation Deconfounded Conversational Recommender System with Bandit Feedback](https://doi.org/10.1145/3580305.3599539)|Yu Xia, Junda Wu, Tong Yu, Sungchul Kim, Ryan A. Rossi, Shuai Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User-Regulation+Deconfounded+Conversational+Recommender+System+with+Bandit+Feedback)|0|
|[Contrastive Learning for User Sequence Representation in Personalized Product Search](https://doi.org/10.1145/3580305.3599287)|Shitong Dai, Jiongnan Liu, Zhicheng Dou, Haonan Wang, Lin Liu, Bo Long, JiRong Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Learning+for+User+Sequence+Representation+in+Personalized+Product+Search)|0|
|[LATTE: A Framework for Learning Item-Features to Make a Domain-Expert for Effective Conversational Recommendation](https://doi.org/10.1145/3580305.3599401)|Taeho Kim, Juwon Yu, WonYong Shin, Hyunyoung Lee, JiHui Im, SangWook Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LATTE:+A+Framework+for+Learning+Item-Features+to+Make+a+Domain-Expert+for+Effective+Conversational+Recommendation)|0|
|[An Empirical Study of Selection Bias in Pinterest Ads Retrieval](https://doi.org/10.1145/3580305.3599771)|Yuan Wang, Peifeng Yin, Zhiqiang Tao, Hari Venkatesan, Jin Lai, Yi Fang, PJ Xiao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Empirical+Study+of+Selection+Bias+in+Pinterest+Ads+Retrieval)|0|
|[PIER: Permutation-Level Interest-Based End-to-End Re-ranking Framework in E-commerce](https://doi.org/10.1145/3580305.3599886)|Xiaowen Shi, Fan Yang, Ze Wang, Xiaoxu Wu, Muzhi Guan, Guogang Liao, Yongkang Wang, Xingxing Wang, Dong Wang|Meituan|Re-ranking draws increased attention on both academics and industries, which rearranges the ranking list by modeling the mutual influence among items to better meet users' demands. Many existing re-ranking methods directly take the initial ranking list as input, and generate the optimal permutation through a well-designed context-wise model, which brings the evaluation-before-reranking problem. Meanwhile, evaluating all candidate permutations brings unacceptable computational costs in practice. Thus, to better balance efficiency and effectiveness, online systems usually use a two-stage architecture which uses some heuristic methods such as beam-search to generate a suitable amount of candidate permutations firstly, which are then fed into the evaluation model to get the optimal permutation. However, existing methods in both stages can be improved through the following aspects. As for generation stage, heuristic methods only use point-wise prediction scores and lack an effective judgment. As for evaluation stage, most existing context-wise evaluation models only consider the item context and lack more fine-grained feature context modeling. This paper presents a novel end-to-end re-ranking framework named PIER to tackle the above challenges which still follows the two-stage architecture and contains two mainly modules named FPSM and OCPM. We apply SimHash in FPSM to select top-K candidates from the full permutation based on user's permutation-level interest in an efficient way. Then we design a novel omnidirectional attention mechanism in OCPM to capture the context information in the permutation. Finally, we jointly train these two modules end-to-end by introducing a comparative learning loss. Offline experiment results demonstrate that PIER outperforms baseline models on both public and industrial datasets, and we have successfully deployed PIER on Meituan food delivery platform.|重新排名吸引了越来越多的学术界和行业的关注，它们通过建立项目之间的相互影响模型来重新排列排名列表，以更好地满足用户的需求。许多现有的重新排序方法直接以初始排序列表为输入，通过设计良好的上下文智能模型生成最优排序，从而产生重新排序前的评价问题。同时，评估所有候选排列在实践中带来不可接受的计算成本。因此，为了更好地平衡效率和有效性，在线系统通常采用两阶段的体系结构，使用一些启发式的方法，如束搜索，生成适当数量的候选排列，然后反馈到评估模型，以获得最优的排列。然而，这两个阶段的现有方法可以通过以下几个方面进行改进。对于生成阶段，启发式方法只使用逐点预测得分，缺乏有效的判断。在评价阶段，现有的基于上下文的评价模型大多只考虑项目上下文，缺乏更细粒度的特征上下文建模。本文提出了一种新的端到端重新排序框架 PIER，以解决上述挑战，该框架仍然遵循两阶段的体系结构，包含两个主要模块: FPSM 和 OCPM。将模拟哈希算法应用于基于用户兴趣排列的 FSM 中，有效地从完全排列中选择出最优 K 候选算法。然后在 OCPM 中设计了一种新的全方位注意机制来捕获排列中的上下文信息。最后，通过引入比较学习损失，对这两个模块进行了端到端的联合训练。离线实验结果显示 PIER 在公共和工业数据集上都优于基线模型，我们已经成功地在美团食品配送平台上部署 PIER。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PIER:+Permutation-Level+Interest-Based+End-to-End+Re-ranking+Framework+in+E-commerce)|0|
|[Exploiting Intent Evolution in E-commercial Query Recommendation](https://doi.org/10.1145/3580305.3599821)|Yu Wang, Zhengyang Wang, Hengrui Zhang, Qingyu Yin, Xianfeng Tang, Yinghan Wang, Danqing Zhang, Limeng Cui, Monica Cheng, Bing Yin, Suhang Wang, Philip S. Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Intent+Evolution+in+E-commercial+Query+Recommendation)|0|
|[QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search](https://doi.org/10.1145/3580305.3599891)|Jian Xie, Yidan Liang, Jingping Liu, Yanghua Xiao, Baohua Wu, Shenghua Ni|School of Information Science and Engineering, East China University of Science and Technology; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University; Alibaba Group|In light of the success of the pre-trained language models (PLMs), continual pre-training of generic PLMs has been the paradigm of domain adaption. In this paper, we propose QUERT, A Continual Pre-trained Language Model for QUERy Understanding in Travel Domain Search. QUERT is jointly trained on four tailored pre-training tasks to the characteristics of query in travel domain search: Geography-aware Mask Prediction, Geohash Code Prediction, User Click Behavior Learning, and Phrase and Token Order Prediction. Performance improvement of downstream tasks and ablation experiment demonstrate the effectiveness of our proposed pre-training tasks. To be specific, the average performance of downstream tasks increases by 2.02% and 30.93% in supervised and unsupervised settings, respectively. To check on the improvement of QUERT to online business, we deploy QUERT and perform A/B testing on Fliggy APP. The feedback results show that QUERT increases the Unique Click-Through Rate and Page Click-Through Rate by 0.89% and 1.03% when applying QUERT as the encoder. Our code and downstream task data will be released for future research.|鉴于预训练语言模型(PLM)的成功，通用 PLM 的连续预训练已经成为领域适应的范例。本文提出了一种连续预训练语言模型 QUERT，用于旅游领域搜索中的查询理解。QUERT 针对旅游领域搜索中查询的特点，共同接受了四项量身定制的预先培训任务: 地理感知掩码预测、 Geohash 代码预测、用户点击行为学习以及短语和令牌顺序预测。下游任务的性能改进和烧蚀实验验证了我们提出的预训练任务的有效性。具体来说，在监督和非监督环境下，下游任务的平均性能分别提高了2.02% 和30.93% 。为了检查 QUERT 对在线业务的改进，我们部署 QUERT 并在 Fliggy APP 上进行 A/B 测试。反馈结果显示，当应用 QUERT 作为编码器时，QUERT 增加了0.89% 和1.03% 的唯一点进率和页面点进率。我们的代码和下游任务数据将被公布，以供未来研究使用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QUERT:+Continual+Pre-training+of+Language+Model+for+Query+Understanding+in+Travel+Domain+Search)|0|
|[A Collaborative Transfer Learning Framework for Cross-domain Recommendation](https://doi.org/10.1145/3580305.3599758)|Wei Zhang, Pengye Zhang, Bo Zhang, Xingxing Wang, Dong Wang|Meituan|In the recommendation systems, there are multiple business domains to meet the diverse interests and needs of users, and the click-through rate(CTR) of each domain can be quite different, which leads to the demand for CTR prediction modeling for different business domains. The industry solution is to use domain-specific models or transfer learning techniques for each domain. The disadvantage of the former is that the data from other domains is not utilized by a single domain model, while the latter leverage all the data from different domains, but the fine-tuned model of transfer learning may trap the model in a local optimum of the source domain, making it difficult to fit the target domain. Meanwhile, significant differences in data quantity and feature schemas between different domains, known as domain shift, may lead to negative transfer in the process of transferring. To overcome these challenges, we propose the Collaborative Cross-Domain Transfer Learning Framework (CCTL). CCTL evaluates the information gain of the source domain on the target domain using a symmetric companion network and adjusts the information transfer weight of each source domain sample using the information flow network. This approach enables full utilization of other domain data while avoiding negative migration. Additionally, a representation enhancement network is used as an auxiliary task to preserve domain-specific features. Comprehensive experiments on both public and real-world industrial datasets, CCTL achieved SOTA score on offline metrics. At the same time, the CCTL algorithm has been deployed in Meituan, bringing 4.37% CTR and 5.43% GMV lift, which is significant to the business.|在推荐系统中，有多个业务领域可以满足用户的不同兴趣和需求，而每个领域的点进率可能有很大差异，因此需要为不同的业务领域建立点击率预测模型。行业解决方案是对每个领域使用特定于领域的模型或转移学习技术。前者的缺点是其他领域的数据不能被单一的领域模型所利用，而后者则利用来自不同领域的所有数据，但是经过微调的迁移学习模型可能使模型陷入源领域的局部最优，从而难以适应目标领域。同时，不同领域间数据量和特征模式的显著差异，称为领域移位，可能导致传递过程中的负迁移。为了克服这些挑战，我们提出了协作跨域转移学习框架(CCTL)。CCTL 使用对称伴侣网络对源域在目标域上的信息增益进行评估，并使用信息流网络调整每个源域样本的信息传输权重。这种方法可以充分利用其他域数据，同时避免负迁移。此外，表示增强网络用作辅助任务，以保持特定领域的特征。CCTL 在公共和现实世界的工业数据集上进行了全面的实验，在离线指标上取得了 SOTA 评分。与此同时，CCTL 算法已经在美团中部署，带来了4.37% 的点击率和5.43% 的 GMV 提升，这对业务具有重要意义。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Collaborative+Transfer+Learning+Framework+for+Cross-domain+Recommendation)|0|
|[Towards Disentangling Relevance and Bias in Unbiased Learning to Rank](https://doi.org/10.1145/3580305.3599914)|Yunan Zhang, Le Yan, Zhen Qin, Honglei Zhuang, Jiaming Shen, Xuanhui Wang, Michael Bendersky, Marc Najork|Google; University of Illinois at Urbana-Champaign; Google Research|Unbiased learning to rank (ULTR) studies the problem of mitigating various biases from implicit user feedback data such as clicks, and has been receiving considerable attention recently. A popular ULTR approach for real-world applications uses a two-tower architecture, where click modeling is factorized into a relevance tower with regular input features, and a bias tower with bias-relevant inputs such as the position of a document. A successful factorization will allow the relevance tower to be exempt from biases. In this work, we identify a critical issue that existing ULTR methods ignored - the bias tower can be confounded with the relevance tower via the underlying true relevance. In particular, the positions were determined by the logging policy, i.e., the previous production model, which would possess relevance information. We give both theoretical analysis and empirical results to show the negative effects on relevance tower due to such a correlation. We then propose three methods to mitigate the negative confounding effects by better disentangling relevance and bias. Empirical results on both controlled public datasets and a large-scale industry dataset show the effectiveness of the proposed approaches.|无偏学习排序(ULTR)研究的是如何减轻隐性用户反馈数据(如点击)中的各种偏差，近年来受到了广泛的关注。一种流行的 ULTR 方法用于现实世界的应用程序使用一个双塔架构，其中点击建模被分解为一个具有常规输入特征的相关塔，以及一个具有偏倚相关输入(如文档的位置)的偏倚塔。一个成功的因子分解将使相关塔免于偏见。在这项工作中，我们确定了一个关键问题，现有的 ULTR 方法忽略-偏倚塔可以混淆与相关塔通过潜在的真实相关性。具体来说，位置是由测井策略决定的，即先前的生产模型，它将拥有相关信息。我们给出了理论分析和实证结果来说明这种相关性对关联塔的负面影响。然后，我们提出了三种方法，通过更好地分离相关性和偏倚来减轻负面混杂效应。对受控公共数据集和大规模行业数据集的实证结果表明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Disentangling+Relevance+and+Bias+in+Unbiased+Learning+to+Rank)|0|
|[M5: Multi-Modal Multi-Interest Multi-Scenario Matching for Over-the-Top Recommendation](https://doi.org/10.1145/3580305.3599863)|Pengyu Zhao, Xin Gao, Chunxu Xu, Liang Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M5:+Multi-Modal+Multi-Interest+Multi-Scenario+Matching+for+Over-the-Top+Recommendation)|0|
|[Accelerating Personalized PageRank Vector Computation](https://doi.org/10.1145/3580305.3599251)|Zhen Chen, Xingzhi Guo, Baojian Zhou, Deqing Yang, Steven Skiena|Fudan University; State University of New York at Stony Brook|Personalized PageRank Vectors are widely used as fundamental graph-learning tools for detecting anomalous spammers, learning graph embeddings, and training graph neural networks. The well-known local FwdPush algorithm approximates PPVs and has a sublinear rate of $O\big(\frac{1}{\alpha\epsilon}\big)$. A recent study found that when high precision is required, FwdPush is similar to the power iteration method, and its run time is pessimistically bounded by $O\big(\frac{m}{\alpha} \log\frac{1}{\epsilon}\big)$. This paper looks closely at calculating PPVs for both directed and undirected graphs. By leveraging the linear invariant property, we show that FwdPush is a variant of Gauss-Seidel and propose a Successive Over-Relaxation based method, FwdPushSOR to speed it up by slightly modifying FwdPush. Additionally, we prove FwdPush has local linear convergence rate $O\big(\tfrac{\text{vol}(S)}{\alpha} \log\tfrac{1}{\epsilon}\big)$ enjoying advantages of two existing bounds. We also design a new local heuristic push method that reduces the number of operations by 10-50 percent compared to FwdPush. For undirected graphs, we propose two momentum-based acceleration methods that can be expressed as one-line updates and speed up non-acceleration methods by$\mathcal{O}\big(\tfrac{1}{\sqrt{\alpha}}\big)$. Our experiments on six real-world graph datasets confirm the efficiency of FwdPushSOR and the acceleration methods for directed and undirected graphs, respectively.|个性化 PageRank 向量广泛用作基本的图形学习工具，用于检测异常垃圾邮件发送者、学习图形嵌入和训练图形神经网络。著名的局部 FwdPush 算法近似于 PPV，其次线性速率为 $O big (frac {1}{ alpha epsilon } big) $。最近的一项研究发现，当需要高精度时，FwdPush 类似于幂迭代法，其运行时间悲观地受到 $O big (frac { m }{ alpha } log frac {1}{ epsilon } big) $的限制。本文主要研究有向图和无向图的 PPV 的计算。通过利用线性不变性，我们证明了 FwdPush 是 Gauss-Seidel 的一个变体，并提出了一个基于逐次超松驰法的方法，FwdPushSOR，通过稍微修改 FwdPush 来加速它。另外，我们证明了 FwdPush 具有局部线性收敛速度 $O big (tfrac { text { vol }(S)}{ alpha } log tfrac {1}{ epsilon } big) $具有两个现有界的优点。我们还设计了一种新的局部启发式推送方法，与 FwdPush 相比减少了10-50% 的操作次数。对于无向图，我们提出了两种基于动量的加速方法，它们可以表示为一行更新，并且可以通过 $mathcal { O } big (tfrac {1}{ sqrt { alpha }} big) $来加速非加速方法。我们在六个实际图形数据集上的实验分别证实了 FwdPushSOR 和有向图和无向图加速方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Accelerating+Personalized+PageRank+Vector+Computation)|0|
|[Text Is All You Need: Learning Language Representations for Sequential Recommendation](https://doi.org/10.1145/3580305.3599519)|Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, Julian J. McAuley|University of California, San Diego; Amazon|Sequential recommendation aims to model dynamic user behavior from historical interactions. Existing methods rely on either explicit item IDs or general textual features for sequence modeling to understand user preferences. While promising, these approaches still struggle to model cold-start items or transfer knowledge to new datasets. In this paper, we propose to model user preferences and item features as language representations that can be generalized to new items and datasets. To this end, we present a novel framework, named Recformer, which effectively learns language representations for sequential recommendation. Specifically, we propose to formulate an item as a "sentence" (word sequence) by flattening item key-value attributes described by text so that an item sequence for a user becomes a sequence of sentences. For recommendation, Recformer is trained to understand the "sentence" sequence and retrieve the next "sentence". To encode item sequences, we design a bi-directional Transformer similar to the model Longformer but with different embedding layers for sequential recommendation. For effective representation learning, we propose novel pretraining and finetuning methods which combine language understanding and recommendation tasks. Therefore, Recformer can effectively recommend the next item based on language representations. Extensive experiments conducted on six datasets demonstrate the effectiveness of Recformer for sequential recommendation, especially in low-resource and cold-start settings.|顺序推荐旨在从历史交互中建立动态用户行为模型。现有的方法依赖于显式的项 ID 或一般的文本特性来进行序列建模，以理解用户的首选项。尽管这些方法很有前途，但它们仍然难以对冷启动项目进行建模或将知识转移到新的数据集中。本文提出将用户偏好和项目特征建模为语言表示，并将其推广到新的项目和数据集。为此，我们提出了一个新的框架，称为 Recformer，它有效地学习语言表示顺序推荐。具体来说，我们建议通过将文本所描述的项目键值属性扁平化，将项目表述为“句子”(单词序列) ，从而使用户的项目序列成为一个句子序列。为了便于推荐，Recformer 接受了理解“句子”序列和检索下一个“句子”的训练。为了对项目序列进行编码，我们设计了一个类似于 Longform 模型但具有不同嵌入层的双向变压器用于顺序推荐。为了有效地进行表征学习，我们提出了一种新的预训练和微调方法，将语言理解和推荐任务结合起来。因此，Recformer 可以根据语言表示有效地推荐下一个项目。在六个数据集上进行的大量实验证明了 Recformer 对于顺序推荐的有效性，特别是在低资源和冷启动环境下。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Text+Is+All+You+Need:+Learning+Language+Representations+for+Sequential+Recommendation)|0|
|[MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction](https://doi.org/10.1145/3580305.3599422)|Jianghao Lin, Yanru Qu, Wei Guo, Xinyi Dai, Ruiming Tang, Yong Yu, Weinan Zhang|Shanghai Jiao Tong University; Huawei NoahÊäØ Ark Lab|With the widespread application of personalized online services, click-through rate (CTR) prediction has received more and more attention and research. The most prominent features of CTR prediction are its multi-field categorical data format, and vast and daily-growing data volume. The large capacity of neural models helps digest such massive amounts of data under the supervised learning paradigm, yet they fail to utilize the substantial data to its full potential, since the 1-bit click signal is not sufficient to guide the model to learn capable representations of features and instances. The self-supervised learning paradigm provides a more promising pretrain-finetune solution to better exploit the large amount of user click logs, and learn more generalized and effective representations. However, self-supervised learning for CTR prediction is still an open question, since current works on this line are only preliminary and rudimentary. To this end, we propose a Model-agnostic pretraining (MAP) framework that applies feature corruption and recovery on multi-field categorical data, and more specifically, we derive two practical algorithms: masked feature prediction (MFP) and replaced feature detection (RFD). MFP digs into feature interactions within each instance through masking and predicting a small portion of input features, and introduces noise contrastive estimation (NCE) to handle large feature spaces. RFD further turns MFP into a binary classification mode through replacing and detecting changes in input features, making it even simpler and more effective for CTR pretraining. Our extensive experiments on two real-world large-scale datasets (i.e., Avazu, Criteo) demonstrate the advantages of these two methods on several strong backbones (e.g., DCNv2, DeepFM), and achieve new state-of-the-art performance in terms of both effectiveness and efficiency for CTR prediction.|随着个性化网上服务的广泛应用，点进率预测越来越受到重视和研究。CTR 预测最突出的特点是它的多领域分类数据格式，以及海量和日益增长的数据量。神经模型的巨大容量有助于在监督式学习范式下消化如此大量的数据，但它们未能充分利用大量的数据，因为1位点击信号不足以指导模型学习特征和实例的能力表示。自监督学习范式为更好地利用大量的用户点击日志，学习更广泛和有效的表示提供了一种更有前途的预训练-微调解决方案。然而，自我监督学习的 CTR 预测仍然是一个悬而未决的问题，因为目前在这方面的工作只是初步和基础。为此，我们提出了一个模型无关预训练(model-agnotic pretraining，MAP)框架，该框架将特征损坏和恢复应用于多领域分类数据，更具体地说，我们推导出两种实用算法: 掩盖特征预测(mFP)和替换特征提取(RFD)。MFP 通过屏蔽和预测一小部分输入特征，深入挖掘每个实例中的特征交互，并引入噪声对比估计(NCE)来处理较大的特征空间。RFD 通过替换和检测输入特征的变化，进一步将 MFP 转化为二进制分类模式，使 CTR 预训练更加简单有效。我们在两个真实世界的大规模数据集(例如，Avazu，Criteo)上的广泛实验证明了这两种方法在几个强骨干(例如，dCNv2，DeepFM)上的优势，并在有效性和效率方面实现了新的最先进的 CTR 预测性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MAP:+A+Model-agnostic+Pretraining+Framework+for+Click-through+Rate+Prediction)|0|
|[Learning to Relate to Previous Turns in Conversational Search](https://doi.org/10.1145/3580305.3599411)|Fengran Mo, JianYun Nie, Kaiyu Huang, Kelong Mao, Yutao Zhu, Peng Li, Yang Liu|Tsinghua University; Renmin University of China; University of Montreal|Conversational search allows a user to interact with a search system in multiple turns. A query is strongly dependent on the conversation context. An effective way to improve retrieval effectiveness is to expand the current query with historical queries. However, not all the previous queries are related to, and useful for expanding the current query. In this paper, we propose a new method to select relevant historical queries that are useful for the current query. To cope with the lack of labeled training data, we use a pseudo-labeling approach to annotate useful historical queries based on their impact on the retrieval results. The pseudo-labeled data are used to train a selection model. We further propose a multi-task learning framework to jointly train the selector and the retriever during fine-tuning, allowing us to mitigate the possible inconsistency between the pseudo labels and the changed retriever. Extensive experiments on four conversational search datasets demonstrate the effectiveness and broad applicability of our method compared with several strong baselines.|会话搜索允许用户多次与搜索系统交互。查询强烈依赖于会话上下文。提高检索效率的一个有效方法是使用历史查询扩展当前查询。但是，并非所有以前的查询都与之相关，并且对于展开当前查询非常有用。本文提出了一种新的方法来选择对当前查询有用的相关历史查询。为了解决缺乏标记训练数据的问题，我们使用伪标记方法根据有用的历史查询对检索结果的影响来注释它们。利用伪标记数据训练选择模型。我们进一步提出了一个多任务学习框架，在微调过程中联合训练选择器和检索器，使我们能够减轻伪标签和更改后的检索器之间可能的不一致性。通过对四个会话搜索数据集的大量实验，证明了该方法的有效性和广泛的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Relate+to+Previous+Turns+in+Conversational+Search)|0|
|[PSLOG: Pretraining with Search Logs for Document Ranking](https://doi.org/10.1145/3580305.3599477)|Zhan Su, Zhicheng Dou, Yujia Zhou, Ziyuan Zhao, JiRong Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PSLOG:+Pretraining+with+Search+Logs+for+Document+Ranking)|0|
|[Improving Conversational Recommendation Systems via Counterfactual Data Simulation](https://doi.org/10.1145/3580305.3599387)|Xiaolei Wang, Kun Zhou, Xinyu Tang, Wayne Xin Zhao, Fan Pan, Zhao Cao, JiRong Wen|Renmin University of China; Huawei Poisson Lab|Conversational recommender systems (CRSs) aim to provide recommendation services via natural language conversations. Although a number of approaches have been proposed for developing capable CRSs, they typically rely on sufficient training data for training. Since it is difficult to annotate recommendation-oriented dialogue datasets, existing CRS approaches often suffer from the issue of insufficient training due to the scarcity of training data. To address this issue, in this paper, we propose a CounterFactual data simulation approach for CRS, named CFCRS, to alleviate the issue of data scarcity in CRSs. Our approach is developed based on the framework of counterfactual data augmentation, which gradually incorporates the rewriting to the user preference from a real dialogue without interfering with the entire conversation flow. To develop our approach, we characterize user preference and organize the conversation flow by the entities involved in the dialogue, and design a multi-stage recommendation dialogue simulator based on a conversation flow language model. Under the guidance of the learned user preference and dialogue schema, the flow language model can produce reasonable, coherent conversation flows, which can be further realized into complete dialogues. Based on the simulator, we perform the intervention at the representations of the interacted entities of target users, and design an adversarial training method with a curriculum schedule that can gradually optimize the data augmentation strategy. Extensive experiments show that our approach can consistently boost the performance of several competitive CRSs, and outperform other data augmentation methods, especially when the training data is limited. Our code is publicly available at https://github.com/RUCAIBox/CFCRS.|会话推荐系统(CRS)旨在通过自然语言对话提供推荐服务。虽然已经提出了一些开发有能力的 CRS 的方法，但它们通常依赖于足够的培训数据进行培训。由于很难对面向建议的对话数据集进行注释，现有的 CRS 方法往往因缺乏培训数据而面临培训不足的问题。为了解决这一问题，本文提出了一种 CRS 的 CounterFact 数据模拟方法 CFCRS，以缓解 CRS 中的数据稀缺问题。我们的方法是在反事实数据增强框架的基础上发展起来的，该框架在不干扰整个会话流程的情况下，逐渐将真实对话中的用户偏好重写纳入其中。为了开发这种方法，我们描述了用户偏好的特征，并根据对话所涉及的实体组织了对话流程，设计了一个基于对话流程语言模型的多阶段推荐对话模拟器。在用户偏好和对话模式的指导下，流语言模型可以产生合理、连贯的会话流，进一步实现完整的对话。在该模拟器的基础上，对目标用户交互实体的表示进行干预，设计了一种基于课程表的对抗性训练方法，可以逐步优化数据增强策略。大量实验表明，该方法可以持续提高多个竞争性 CRS 的性能，并且优于其他数据增强方法，特别是在训练数据有限的情况下。我们的代码可以在 https://github.com/rucaibox/cfcrs 上公开获取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Conversational+Recommendation+Systems+via+Counterfactual+Data+Simulation)|0|
|[Efficient and Joint Hyperparameter and Architecture Search for Collaborative Filtering](https://doi.org/10.1145/3580305.3599322)|Yan Wen, Chen Gao, Lingling Yi, Liwei Qiu, Yaqing Wang, Yong Li|Tsinghua University; Tencent Inc.; Baidu Inc.|Automated Machine Learning (AutoML) techniques have recently been introduced to design Collaborative Filtering (CF) models in a data-specific manner. However, existing works either search architectures or hyperparameters while ignoring the fact they are intrinsically related and should be considered together. This motivates us to consider a joint hyperparameter and architecture search method to design CF models. However, this is not easy because of the large search space and high evaluation cost. To solve these challenges, we reduce the space by screening out usefulness yperparameter choices through a comprehensive understanding of individual hyperparameters. Next, we propose a two-stage search algorithm to find proper configurations from the reduced space. In the first stage, we leverage knowledge from subsampled datasets to reduce evaluation costs; in the second stage, we efficiently fine-tune top candidate models on the whole dataset. Extensive experiments on real-world datasets show better performance can be achieved compared with both hand-designed and previous searched models. Besides, ablation and case studies demonstrate the effectiveness of our search framework.|自动机器学习(AutoML)技术最近被引入到设计特定数据的协同过滤模型(CF)中。然而，现有的工作要么搜索体系结构或超参数，而忽略了这些内在联系的事实，应该一起考虑。这促使我们考虑联合使用超参数和体系结构搜索方法来设计 CF 模型。然而，这并不容易，因为大搜索空间和高评价成本。为了解决这些挑战，我们通过全面理解各个超参数筛选出有用的超参数选择来减少空间。接下来，我们提出了一个两阶段的搜索算法，以找到适当的配置从缩减的空间。在第一阶段，我们利用次采样数据集的知识来降低评估成本; 在第二阶段，我们有效地微调整整个数据集上的顶级候选模型。在真实世界数据集上的大量实验表明，与手工设计和以前的搜索模型相比，该算法可以获得更好的性能。此外，消融和案例研究证明了我们的搜索框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Joint+Hyperparameter+and+Architecture+Search+for+Collaborative+Filtering)|0|
|[Efficient Single-Source SimRank Query by Path Aggregation](https://doi.org/10.1145/3580305.3599328)|Mingxi Zhang, Yanghua Xiao, Wei Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Single-Source+SimRank+Query+by+Path+Aggregation)|0|
|[Adaptive Disentangled Transformer for Sequential Recommendation](https://doi.org/10.1145/3580305.3599253)|Yipeng Zhang, Xin Wang, Hong Chen, Wenwu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Disentangled+Transformer+for+Sequential+Recommendation)|0|
|[CADENCE: Offline Category Constrained and Diverse Query Generation for E-commerce Autosuggest](https://doi.org/10.1145/3580305.3599787)|Abhinav Anand, Surender Kumar, Nandeesh Kumar, Samir Shah||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CADENCE:+Offline+Category+Constrained+and+Diverse+Query+Generation+for+E-commerce+Autosuggest)|0|
|[PEPNet: Parameter and Embedding Personalized Network for Infusing with Personalized Prior Information](https://doi.org/10.1145/3580305.3599884)|Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, Kun Gai|Unaffiliated; Kuaishou Technology|With the increase of content pages and display styles in online services such as online-shopping and video-watching websites, industrial-scale recommender systems face challenges in multi-domain and multi-task recommendations. The core of multi-task and multi-domain recommendation is to accurately capture user interests in different domains given different user behaviors. In this paper, we propose a plug-and-play \textit{\textbf{P}arameter and \textbf{E}mbedding \textbf{P}ersonalized \textbf{Net}work (\textbf{PEPNet})} for multi-task recommendation in the multi-domain setting. PEPNet takes features with strong biases as input and dynamically scales the bottom-layer embeddings and the top-layer DNN hidden units in the model through a gate mechanism. By mapping personalized priors to scaling weights ranging from 0 to 2, PEPNet introduces both parameter personalization and embedding personalization. Embedding Personalized Network (EPNet) selects and aligns embeddings with different semantics under multiple domains. Parameter Personalized Network (PPNet) influences DNN parameters to balance interdependent targets in multiple tasks. We have made a series of special engineering optimizations combining the Kuaishou training framework and the online deployment environment. We have deployed the model in Kuaishou apps, serving over 300 million daily users. Both online and offline experiments have demonstrated substantial improvements in multiple metrics. In particular, we have seen a more than 1\% online increase in three major scenarios.|随着在线购物和视频观看网站等在线服务内容页面和显示方式的增加，工业规模的推荐系统面临着多领域、多任务推荐的挑战。多任务、多领域推荐的核心是根据不同的用户行为准确捕获不同领域的用户兴趣。本文针对多领域环境下的多任务推荐问题，提出了一种即插即用的文本参数{ textbf { P }参数和 textbf { E }嵌入式 textbf { P }个性化 textbf { Net } work (textbf { PEPNet })}。PEPNet 以具有强偏差的特征作为输入，通过门机制动态扩展模型中的底层嵌入和顶层 DNN 隐藏单元。通过将个性化前期映射到0到2之间的权重，PEPNet 引入了参数个性化和嵌入个性化。嵌入式个性化网络(EPNet)在多个域下选择和对齐具有不同语义的嵌入式。参数个性化网络(PPNet)影响 DNN 参数以平衡多任务中相互依赖的目标。我们结合快手培训框架和在线部署环境，进行了一系列特殊的工程优化。我们在 Kuaishou 的应用程序中采用了这种模式，每天为超过3亿用户提供服务。这两个在线和离线的实验都显示了在多个指标方面的重大改进。特别是，我们已经看到在三种主要情况下在线增长超过1% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PEPNet:+Parameter+and+Embedding+Personalized+Network+for+Infusing+with+Personalized+Prior+Information)|0|
|[Controllable Multi-Objective Re-ranking with Policy Hypernetworks](https://doi.org/10.1145/3580305.3599796)|Sirui Chen, Yuan Wang, Zijing Wen, Zhiyu Li, Changshuo Zhang, Xiao Zhang, Quan Lin, Cheng Zhu, Jun Xu||Multi-stage ranking pipelines have become widely used strategies in modern recommender systems, where the final stage aims to return a ranked list of items that balances a number of requirements such as user preference, diversity, novelty etc. Linear scalarization is arguably the most widely used technique to merge multiple requirements into one optimization objective, by summing up the requirements with certain preference weights. Existing final-stage ranking methods often adopt a static model where the preference weights are determined during offline training and kept unchanged during online serving. Whenever a modification of the preference weights is needed, the model has to be re-trained, which is time and resources inefficient. Meanwhile, the most appropriate weights may vary greatly for different groups of targeting users or at different time periods (e.g., during holiday promotions). In this paper, we propose a framework called controllable multi-objective re-ranking (CMR) which incorporates a hypernetwork to generate parameters for a re-ranking model according to different preference weights. In this way, CMR is enabled to adapt the preference weights according to the environment changes in an online manner, without retraining the models. Moreover, we classify practical business-oriented tasks into four main categories and seamlessly incorporate them in a new proposed re-ranking model based on an Actor-Evaluator framework, which serves as a reliable real-world testbed for CMR. Offline experiments based on the dataset collected from Taobao App showed that CMR improved several popular re-ranking models by using them as underlying models. Online A/B tests also demonstrated the effectiveness and trustworthiness of CMR.|多阶段排名管道已经成为现代推荐系统中广泛使用的策略，最后阶段的目标是返回一个项目的排名列表，平衡用户偏好、多样性、新颖性等要求。线性标量可以说是最广泛使用的技术合并多个需求到一个优化目标，通过总结需求与一定的偏好权重。现有的最后阶段排序方法通常采用静态模型，在离线训练时确定偏好权重，在线服务时保持不变。当需要修改偏好权重时，模型必须重新训练，这是时间和资源效率低下的。与此同时，最合适的权重可能会因不同的目标用户群体或在不同的时间段(例如，在假日促销期间)而有很大差异。本文提出了一种可控的多目标重排序(CMR)框架，该框架结合了一个超网络，根据不同的偏好权重为重排序模型生成参数。通过这种方式，CMR 能够根据环境变化在线调整偏好权重，而不需要重新训练模型。此外，我们将面向业务的实际任务分为四个主要类别，并将它们无缝地纳入一个新提出的重新排序模型，该模型基于一个演员-评估者框架，作为一个可靠的现实世界的 CMR 测试平台。基于从淘宝应用收集的数据集的离线实验表明，CMR 通过使用它们作为基础模型改进了几个流行的重新排名模型。在线 A/B 测试也证明了 CMR 的有效性和可信性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Controllable+Multi-Objective+Re-ranking+with+Policy+Hypernetworks)|0|
|[CT4Rec: Simple yet Effective Consistency Training for Sequential Recommendation](https://doi.org/10.1145/3580305.3599798)|Liu Chong, Xiaoyang Liu, Rongqin Zheng, Lixin Zhang, Xiaobo Liang, Juntao Li, Lijun Wu, Min Zhang, Leyu Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CT4Rec:+Simple+yet+Effective+Consistency+Training+for+Sequential+Recommendation)|0|
|[S2phere: Semi-Supervised Pre-training for Web Search over Heterogeneous Learning to Rank Data](https://doi.org/10.1145/3580305.3599935)|Yuchen Li, Haoyi Xiong, Linghe Kong, Qingzhong Wang, Shuaiqiang Wang, Guihai Chen, Dawei Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=S2phere:+Semi-Supervised+Pre-training+for+Web+Search+over+Heterogeneous+Learning+to+Rank+Data)|0|
|[Multi-Label Learning to Rank through Multi-Objective Optimization](https://doi.org/10.1145/3580305.3599870)|Debabrata Mahapatra, Chaosheng Dong, Yetian Chen, Michinari Momma||Learning to Rank (LTR) technique is ubiquitous in the Information Retrieval system nowadays, especially in the Search Ranking application. The query-item relevance labels typically used to train the ranking model are often noisy measurements of human behavior, e.g., product rating for product search. The coarse measurements make the ground truth ranking non-unique with respect to a single relevance criterion. To resolve ambiguity, it is desirable to train a model using many relevance criteria, giving rise to Multi-Label LTR (MLLTR). Moreover, it formulates multiple goals that may be conflicting yet important to optimize for simultaneously, e.g., in product search, a ranking model can be trained based on product quality and purchase likelihood to increase revenue. In this research, we leverage the Multi-Objective Optimization (MOO) aspect of the MLLTR problem and employ recently developed MOO algorithms to solve it. Specifically, we propose a general framework where the information from labels can be combined in a variety of ways to meaningfully characterize the trade-off among the goals. Our framework allows for any gradient based MOO algorithm to be used for solving the MLLTR problem. We test the proposed framework on two publicly available LTR datasets and one e-commerce dataset to show its efficacy.|学习排名(LTR)技术在当今的信息检索系统中无处不在，特别是在搜索排名应用程序中。通常用于训练排名模型的查询条目相关标签通常是对人类行为的嘈杂测量，例如，产品搜索的产品评级。粗测量使得地面真实度排序相对于单一的相关准则是非唯一的。为了解决模糊问题，需要使用多个相关准则来训练模型，从而产生多标签 LTR (MLLTR)。此外，它制定了多个目标，可能是冲突的，但重要的优化同时进行，例如，在产品搜索，排名模型可以训练基于产品质量和购买可能性，以增加收入。在这项研究中，我们利用多目标优化(MOO)方面的 MLLTR 问题，并采用最近开发的 MOO 算法来解决它。具体而言，我们提出了一个总体框架，在这个框架中，可以通过各种方式组合来自标签的信息，以有意义地描述目标之间的权衡。我们的框架允许使用任何基于梯度的 MOO 算法来解决 MLLTR 问题。我们在两个公开的 LTR 数据集和一个电子商务数据集上测试了该框架，以验证其有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Label+Learning+to+Rank+through+Multi-Objective+Optimization)|0|
|[Entity-aware Multi-task Learning for Query Understanding at Walmart](https://doi.org/10.1145/3580305.3599816)|Zhiyuan Peng, Vachik Dave, Nicole McNabb, Rahul Sharnagat, Alessandro Magnani, Ciya Liao, Yi Fang, Sravanthi Rajanala||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Entity-aware+Multi-task+Learning+for+Query+Understanding+at+Walmart)|0|
|[Improving Training Stability for Multitask Ranking Models in Recommender Systems](https://doi.org/10.1145/3580305.3599846)|Jiaxi Tang, Yoel Drori, Daryl Chang, Maheswaran Sathiamoorthy, Justin Gilmer, Li Wei, Xinyang Yi, Lichan Hong, Ed H. Chi|Google Inc; Google Deepmind; Google Research|Recommender systems play an important role in many content platforms. While most recommendation research is dedicated to designing better models to improve user experience, we found that research on stabilizing the training for such models is severely under-explored. As recommendation models become larger and more sophisticated, they are more susceptible to training instability issues, \emph{i.e.}, loss divergence, which can make the model unusable, waste significant resources and block model developments. In this paper, we share our findings and best practices we learned for improving the training stability of a real-world multitask ranking model for YouTube recommendations. We show some properties of the model that lead to unstable training and conjecture on the causes. Furthermore, based on our observations of training dynamics near the point of training instability, we hypothesize why existing solutions would fail, and propose a new algorithm to mitigate the limitations of existing solutions. Our experiments on YouTube production dataset show the proposed algorithm can significantly improve training stability while not compromising convergence, comparing with several commonly used baseline methods.|推荐系统在许多内容平台中发挥着重要作用。虽然大多数推荐研究致力于设计更好的模型来改善用户体验，但是我们发现，关于稳定此类模型的训练的研究严重不足。随着推荐模型的不断扩大和复杂化，它们更容易受到训练不稳定性问题的影响，如损失发散等，这些问题会导致模型无法使用，浪费大量资源和阻塞模型的发展。在本文中，我们分享了我们的发现和最佳实践，我们学到了提高训练的稳定性的一个真实世界的多任务排名模型 YouTube 的建议。我们给出了模型的一些性质，这些性质导致了训练的不稳定性和对原因的猜测。此外，基于我们对训练不稳定点附近的训练动力学的观察，我们假设为什么现有的解决方案会失败，并提出了一个新的算法来减轻现有解决方案的局限性。我们在 YouTube 生产数据集上的实验表明，与几种常用的基线方法相比，该算法能够在不影响收敛性的前提下显著提高训练的稳定性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Training+Stability+for+Multitask+Ranking+Models+in+Recommender+Systems)|0|
|[PASS: Personalized Advertiser-aware Sponsored Search](https://doi.org/10.1145/3580305.3599882)|Zhoujin Tian, Chaozhuo Li, Zhiqiang Zuo, Zengxuan Wen, Lichao Sun, Xinyue Hu, Wen Zhang, Haizhen Huang, Senzhang Wang, Weiwei Deng, Xing Xie, Qi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PASS:+Personalized+Advertiser-aware+Sponsored+Search)|0|
|[Towards Fairness in Personalized Ads Using Impression Variance Aware Reinforcement Learning](https://doi.org/10.1145/3580305.3599916)|Aditya Srinivas Timmaraju, Mehdi Mashayekhi, Mingliang Chen, Qi Zeng, Quintin Fettes, Wesley Cheung, Yihan Xiao, Manojkumar Rangasamy Kannadasan, Pushkar Tripathi, Sean Gahagan, Miranda Bogen, Rob Roudani|Meta|Variances in ad impression outcomes across demographic groups are increasingly considered to be potentially indicative of algorithmic bias in personalized ads systems. While there are many definitions of fairness that could be applicable in the context of personalized systems, we present a framework which we call the Variance Reduction System (VRS) for achieving more equitable outcomes in Meta's ads systems. VRS seeks to achieve a distribution of impressions with respect to selected protected class (PC) attributes that more closely aligns the demographics of an ad's eligible audience (a function of advertiser targeting criteria) with the audience who sees that ad, in a privacy-preserving manner. We first define metrics to quantify fairness gaps in terms of ad impression variances with respect to PC attributes including gender and estimated race. We then present the VRS for re-ranking ads in an impression variance-aware manner. We evaluate VRS via extensive simulations over different parameter choices and study the effect of the VRS on the chosen fairness metric. We finally present online A/B testing results from applying VRS to Meta's ads systems, concluding with a discussion of future work. We have deployed the VRS to all users in the US for housing ads, resulting in significant improvement in our fairness metric. VRS is the first large-scale deployed framework for pursuing fairness for multiple PC attributes in online advertising.|不同人群的广告印象结果的差异越来越被认为是个性化广告系统中算法偏差的潜在指示。虽然有许多公平的定义，可以适用于个性化系统的背景下，我们提出了一个框架，我们称之为方差减少系统(VRS) ，以实现更公平的结果在元数据的广告系统。VRS 试图通过选定的受保护类别(PC)属性来实现印象的分布，从而以保护隐私的方式将广告合格受众的人口统计数据(广告客户定位标准的功能)与看到该广告的受众的人口统计数据更紧密地联系起来。我们首先定义指标来量化广告印象差异的公平性差距方面的个人电脑属性，包括性别和估计的种族。然后，我们提出了一个印象方差感知的方式重新排名广告的 VRS。我们通过对不同参数选择的大量仿真来评估 VRS，并研究 VRS 对选择的公平性度量的影响。最后给出了 VRS 应用于 Meta 广告系统的在线 A/B 测试结果，并对今后的工作进行了讨论。我们已在美国所有用户的住房广告中部署了 VRS，从而显著改善了我们的公平性指标。VRS 是第一个大规模部署的框架，以追求公平的多个个人电脑属性在网上广告。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Fairness+in+Personalized+Ads+Using+Impression+Variance+Aware+Reinforcement+Learning)|0|
|[PlanRanker: Towards Personalized Ranking of Train Transfer Plans](https://doi.org/10.1145/3580305.3599887)|Jia Xu, Wanjie Tao, Zulong Chen, Jin Huang, Huihui Liu, Hong Wen, Shenghua Ni, Qun Dai, Yu Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PlanRanker:+Towards+Personalized+Ranking+of+Train+Transfer+Plans)|0|
|[Multi-factor Sequential Re-ranking with Perception-Aware Diversification](https://doi.org/10.1145/3580305.3599869)|Yue Xu, Hao Chen, Zefan Wang, Jianwen Yin, Qijie Shen, Dimin Wang, Feiran Huang, Lixiang Lai, Tao Zhuang, Junfeng Ge, Xia Hu|Jinan University; The Hong Kong Polytechnic University; Rice University; Alibaba Group|Feed recommendation systems, which recommend a sequence of items for users to browse and interact with, have gained significant popularity in practical applications. In feed products, users tend to browse a large number of items in succession, so the previously viewed items have a significant impact on users' behavior towards the following items. Therefore, traditional methods that mainly focus on improving the accuracy of recommended items are suboptimal for feed recommendations because they may recommend highly similar items. For feed recommendation, it is crucial to consider both the accuracy and diversity of the recommended item sequences in order to satisfy users' evolving interest when consecutively viewing items. To this end, this work proposes a general re-ranking framework named Multi-factor Sequential Re-ranking with Perception-Aware Diversification (MPAD) to jointly optimize accuracy and diversity for feed recommendation in a sequential manner. Specifically, MPAD first extracts users' different scales of interests from their behavior sequences through graph clustering-based aggregations. Then, MPAD proposes two sub-models to respectively evaluate the accuracy and diversity of a given item by capturing users' evolving interest due to the ever-changing context and users' personal perception of diversity from an item sequence perspective. This is consistent with the browsing nature of the feed scenario. Finally, MPAD generates the return list by sequentially selecting optimal items from the candidate set to maximize the joint benefits of accuracy and diversity of the entire list. MPAD has been implemented in Taobao's homepage feed to serve the main traffic and provide services to recommend billions of items to hundreds of millions of users every day.|提要推荐系统为用户推荐了一系列可供浏览和交互的条目，在实际应用中得到了广泛的应用。在提要产品中，用户倾向于连续浏览大量条目，因此以前查看的条目对用户对下列条目的行为有显著影响。因此，主要侧重于提高推荐项目准确性的传统方法对于饲料推荐是次优的，因为它们可能推荐高度相似的项目。为了满足用户在连续查看条目时不断变化的兴趣，对推荐条目序列的准确性和多样性进行考虑是至关重要的。为此，本文提出了一种基于感知多样化的多因素序贯推荐(MPAD)的通用推荐框架，该框架以序贯方式对推荐的准确性和多样性进行联合优化。具体来说，MPAD 首先通过基于图聚类的聚合从用户的行为序列中提取出用户不同尺度的兴趣。然后，MPAD 提出了两个子模型，分别从项目序列的角度通过捕获不断变化的用户兴趣和用户个人对多样性的感知来评价项目的准确性和多样性。这与提要场景的浏览特性一致。最后，MPAD 通过从候选集中依次选择最优项目来生成返回列表，以最大限度地提高整个列表的准确性和多样性。MPAD 已经在淘宝网的主页 feed 中实现，为主流流量提供服务，每天向数亿用户推荐数十亿个项目。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-factor+Sequential+Re-ranking+with+Perception-Aware+Diversification)|0|
|[TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou](https://doi.org/10.1145/3580305.3599922)|Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, Kun Gai|Unaffiliated; Kuaishou Technology|Life-long user behavior modeling, i.e., extracting a user's hidden interests from rich historical behaviors in months or even years, plays a central role in modern CTR prediction systems. Conventional algorithms mostly follow two cascading stages: a simple General Search Unit (GSU) for fast and coarse search over tens of thousands of long-term behaviors and an Exact Search Unit (ESU) for effective Target Attention (TA) over the small number of finalists from GSU. Although efficient, existing algorithms mostly suffer from a crucial limitation: the \textit{inconsistent} target-behavior relevance metrics between GSU and ESU. As a result, their GSU usually misses highly relevant behaviors but retrieves ones considered irrelevant by ESU. In such case, the TA in ESU, no matter how attention is allocated, mostly deviates from the real user interests and thus degrades the overall CTR prediction accuracy. To address such inconsistency, we propose \textbf{TWo-stage Interest Network (TWIN)}, where our Consistency-Preserved GSU (CP-GSU) adopts the identical target-behavior relevance metric as the TA in ESU, making the two stages twins. Specifically, to break TA's computational bottleneck and extend it from ESU to GSU, or namely from behavior length $10^2$ to length $10^4-10^5$, we build a novel attention mechanism by behavior feature splitting. For the video inherent features of a behavior, we calculate their linear projection by efficient pre-computing \& caching strategies. And for the user-item cross features, we compress each into a one-dimentional bias term in the attention score calculation to save the computational cost. The consistency between two stages, together with the effective TA-based relevance metric in CP-GSU, contributes to significant performance gain in CTR prediction.|终身用户行为建模，即在数月甚至数年内从丰富的历史行为中提取用户隐藏的兴趣，在现代 CTR 预测系统中起着核心作用。传统的算法大多遵循两个级联阶段: 一个简单的通用搜索单元(GSU)用于快速和粗略搜索成千上万的长期行为和一个精确搜索单元(ESU)用于有效的目标注意(TA)在少数决赛选手从 GSU。虽然有效，但现有的算法大多受到一个关键的限制: 文本{不一致}目标行为相关度量 GSU 和 ESU 之间。因此，他们的 GSU 通常会错过高度相关的行为，但检索被 ESU 认为无关的行为。在这种情况下，ESU 中的 TA，无论如何分配注意力，大多偏离了真实用户的兴趣，从而降低了整体 CTR 预测的准确性。为了解决这种不一致性，我们提出 textbf { TWo-stage Interest Network (TWIN)} ，其中我们的保持一致性的 GSU (CP-GSU)采用与 ESU 中的 TA 相同的目标行为相关度量，使两个阶段成为孪生的。具体来说，为了打破 TA 的计算瓶颈，将其从 ESU 扩展到 GSU，或者说从行为长度 $10 ^ 2 $扩展到长度 $10 ^ 4-10 ^ 5 $，我们通过行为特征分裂构建了一种新的注意机制。对于视频行为的固有特征，我们通过有效的预计算和缓存策略来计算它们的线性投影。对于用户-项目交叉特征，在注意得分计算中将每个特征压缩为一维偏差项，以节省计算成本。两个阶段之间的一致性，加上 CP-GSU 中有效的基于 TA 的相关度量，有助于提高 CTR 预测的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TWIN:+TWo-stage+Interest+Network+for+Lifelong+User+Behavior+Modeling+in+CTR+Prediction+at+Kuaishou)|0|
|[On Manipulating Signals of User-Item Graph: A Jacobi Polynomial-based Graph Collaborative Filtering](https://doi.org/10.1145/3580305.3599450)|Jiayan Guo, Lun Du, Xu Chen, Xiaojun Ma, Qiang Fu, Shi Han, Dongmei Zhang, Yan Zhang|Microsoft; Peking University|Collaborative filtering (CF) is an important research direction in recommender systems that aims to make recommendations given the information on user-item interactions. Graph CF has attracted more and more attention in recent years due to its effectiveness in leveraging high-order information in the user-item bipartite graph for better recommendations. Specifically, recent studies show the success of graph neural networks (GNN) for CF is attributed to its low-pass filtering effects. However, current researches lack a study of how different signal components contributes to recommendations, and how to design strategies to properly use them well. To this end, from the view of spectral transformation, we analyze the important factors that a graph filter should consider to achieve better performance. Based on the discoveries, we design JGCF, an efficient and effective method for CF based on Jacobi polynomial bases and frequency decomposition strategies. Extensive experiments on four widely used public datasets show the effectiveness and efficiency of the proposed methods, which brings at most 27.06% performance gain on Alibaba-iFashion. Besides, the experimental results also show that JGCF is better at handling sparse datasets, which shows potential in making recommendations for cold-start users.|协同过滤(CF)是推荐系统的一个重要研究方向，其目的是根据用户项目交互的信息提供推荐。近年来，Graph CF 由于能够有效地利用用户-项目双向图中的高阶信息来获得更好的建议而引起了越来越多的关注。具体来说，最近的研究表明，图神经网络(GNN)对 CF 的成功归功于其低通滤波效果。然而，目前的研究缺乏研究不同的信号成分如何有助于推荐，以及如何设计策略，以适当地使用它们。为此，本文从谱变换的角度出发，分析了图形滤波器要获得更好的性能所应考虑的重要因素。基于这些发现，我们设计了一种基于 Jacobi 多项式基和频率分解策略的高效率和有效的协同过滤方法。在四个广泛使用的公共数据集上进行的大量实验表明了该方法的有效性和效率，在阿里巴巴-iFashion 平台上最多获得27.06% 的性能增益。此外，实验结果还表明，JGCF 在处理稀疏数据集方面有较好的表现，可以为冷启动用户提供建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Manipulating+Signals+of+User-Item+Graph:+A+Jacobi+Polynomial-based+Graph+Collaborative+Filtering)|0|
|[Off-Policy Evaluation of Ranking Policies under Diverse User Behavior](https://doi.org/10.1145/3580305.3599447)|Haruka Kiyohara, Masatoshi Uehara, Yusuke Narita, Nobuyuki Shimizu, Yasuo Yamamoto, Yuta Saito|Yale University; Cornell University; Yahoo Japan Corporation; Hanjuku-Kaso Co., Ltd.|Ranking interfaces are everywhere in online platforms. There is thus an ever growing interest in their Off-Policy Evaluation (OPE), aiming towards an accurate performance evaluation of ranking policies using logged data. A de-facto approach for OPE is Inverse Propensity Scoring (IPS), which provides an unbiased and consistent value estimate. However, it becomes extremely inaccurate in the ranking setup due to its high variance under large action spaces. To deal with this problem, previous studies assume either independent or cascade user behavior, resulting in some ranking versions of IPS. While these estimators are somewhat effective in reducing the variance, all existing estimators apply a single universal assumption to every user, causing excessive bias and variance. Therefore, this work explores a far more general formulation where user behavior is diverse and can vary depending on the user context. We show that the resulting estimator, which we call Adaptive IPS (AIPS), can be unbiased under any complex user behavior. Moreover, AIPS achieves the minimum variance among all unbiased estimators based on IPS. We further develop a procedure to identify the appropriate user behavior model to minimize the mean squared error (MSE) of AIPS in a data-driven fashion. Extensive experiments demonstrate that the empirical accuracy improvement can be significant, enabling effective OPE of ranking systems even under diverse user behavior.|在线平台中，排序界面无处不在。因此，人们对非策略评估(OPE)越来越感兴趣，其目标是使用日志数据对策略进行准确的性能评估。OPE 的一个事实上的方法是反倾向评分(IPS) ，它提供了一个无偏和一致的价值估计。然而，它变得非常不准确的排名设置，由于其高方差下的大行动空间。为了解决这个问题，以前的研究假设独立或级联用户行为，导致一些排名版本的 IPS。虽然这些估计量在减少方差方面有一定的效果，但是所有现有的估计量都对每个用户适用一个统一的假设，从而导致过度的偏差和方差。因此，这项工作探索了一个更一般的公式，其中用户行为是多样的，可以根据用户上下文而变化。我们证明了所得到的估计量，我们称之为自适应 IPS (AIPS) ，在任何复杂的用户行为下都是无偏的。此外，AIPS 在所有基于 IPS 的无偏估计量之间实现了最小方差。我们进一步开发了一个程序，以确定适当的用户行为模型，从而以数据驱动的方式最大限度地减少 AIPS 的均方差。大量的实验表明，经验的准确性改善可以是显着的，使有效的排名系统的 OPE 即使在不同的用户行为。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Off-Policy+Evaluation+of+Ranking+Policies+under+Diverse+User+Behavior)|0|
|[Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay](https://doi.org/10.1145/3580305.3599386)|Thomas M. McDonald, Lucas Maystre, Mounia Lalmas, Daniel Russo, Kamil Ciosek||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Impatient+Bandits:+Optimizing+Recommendations+for+the+Long-Term+Without+Delay)|0|
|[Unbiased Delayed Feedback Label Correction for Conversion Rate Prediction](https://doi.org/10.1145/3580305.3599536)|Yifan Wang, Peijie Sun, Min Zhang, Qinglin Jia, Jingjie Li, Shaoping Ma|Tsinghua University; Noah’s Ark Lab, Huawei|Conversion rate prediction is critical to many online applications such as digital display advertising. To capture dynamic data distribution, industrial systems often require retraining models on recent data daily or weekly. However, the delay of conversion behavior usually leads to incorrect labeling, which is called delayed feedback problem. Existing work may fail to introduce the correct information about false negative samples due to data sparsity and dynamic data distribution. To directly introduce the correct feedback label information, we propose an Unbiased delayed feedback Label Correction framework (ULC), which uses an auxiliary model to correct labels for observed negative feedback samples. Firstly, we theoretically prove that the label-corrected loss is an unbiased estimate of the oracle loss using true labels. Then, as there are no ready training data for label correction, counterfactual labeling is used to construct artificial training data. Furthermore, since counterfactual labeling utilizes only partial training data, we design an embedding-based alternative training method to enhance performance. Comparative experiments on both public and private datasets and detailed analyses show that our proposed approach effectively alleviates the delayed feedback problem and consistently outperforms the previous state-of-the-art methods.|转化率预测是许多在线应用程序，如数字显示广告的关键。为了捕获动态数据分布，工业系统通常需要每天或每周对最近的数据进行再训练。然而，转换行为的延迟通常会导致不正确的标记，这就是所谓的延迟反馈问题。由于数据稀疏和数据分布的动态性，现有的工作可能无法引入正确的假阴性样本信息。为了直接引入正确的反馈标签信息，我们提出了一种无偏的延迟反馈标签校正框架(ULC) ，它使用一个辅助模型对观测到的负反馈样本进行标签校正。首先，我们从理论上证明了标签校正损失是使用真实标签对甲骨文损失进行的无偏估计。然后，由于没有现成的训练数据用于标签校正，采用反事实标注来构造人工训练数据。此外，由于反事实标注只利用部分训练数据，我们设计了一个基于嵌入的替代训练方法来提高性能。对公共和私人数据集的比较实验和详细的分析表明，我们提出的方法有效地缓解了延迟反馈问题，并始终优于以前的最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Delayed+Feedback+Label+Correction+for+Conversion+Rate+Prediction)|0|
|[PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement](https://doi.org/10.1145/3580305.3599473)|Wanqi Xue, Qingpeng Cai, Zhenghai Xue, Shuo Sun, Shuchang Liu, Dong Zheng, Peng Jiang, Kun Gai, Bo An||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PrefRec:+Recommender+Systems+with+Human+Preferences+for+Reinforcing+Long-term+User+Engagement)|0|
|[Sequence As Genes: An User Behavior Modeling Framework for Fraud Transaction Detection in E-commerce](https://doi.org/10.1145/3580305.3599905)|Ziming Wang, Qianru Wu, Baolin Zheng, Junjie Wang, Kaiyu Huang, Yanjie Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sequence+As+Genes:+An+User+Behavior+Modeling+Framework+for+Fraud+Transaction+Detection+in+E-commerce)|0|
|[TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest](https://doi.org/10.1145/3580305.3599918)|Xue Xia, Pong Eksombatchai, Nikil Pancha, Dhruvil Deven Badani, PoWei Wang, Neng Gu, Saurabh Vishwas Joshi, Nazanin Farahpour, Zhiyuan Zhang, Andrew Zhai|Pinterest|Sequential models that encode user activity for next action prediction have become a popular design choice for building web-scale personalized recommendation systems. Traditional methods of sequential recommendation either utilize end-to-end learning on realtime user actions, or learn user representations separately in an offline batch-generated manner. This paper (1) presents Pinterest's ranking architecture for Homefeed, our personalized recommendation product and the largest engagement surface; (2) proposes TransAct, a sequential model that extracts users' short-term preferences from their realtime activities; (3) describes our hybrid approach to ranking, which combines end-to-end sequential modeling via TransAct with batch-generated user embeddings. The hybrid approach allows us to combine the advantages of responsiveness from learning directly on realtime user activity with the cost-effectiveness of batch user representations learned over a longer time period. We describe the results of ablation studies, the challenges we faced during productionization, and the outcome of an online A/B experiment, which validates the effectiveness of our hybrid ranking model. We further demonstrate the effectiveness of TransAct on other surfaces such as contextual recommendations and search. Our model has been deployed to production in Homefeed, Related Pins, Notifications, and Search at Pinterest.|为下一步行动预测编码用户活动的序列模型已成为建立网络规模个性化推荐系统的流行设计选择。传统的顺序推荐方法要么利用实时用户操作的端到端学习，要么以离线批量生成的方式单独学习用户表示。本文(1)介绍了 Pinterest 针对 Homefeed 的排名体系结构，这是我们的个性化推荐产品，也是最大的参与表面; (2)提出了 TransAct，一个从用户的实时活动中提取用户短期偏好的顺序模型; (3)描述了我们的混合排名方法，它结合了通过 TransAct 的端到端顺序建模和批量生成的用户嵌入。这种混合方法使我们能够将直接学习实时用户活动的响应性优势与长期学习的批量用户表示的成本效益结合起来。我们描述了烧蚀研究的结果，我们在生产过程中面临的挑战，以及一个在线 A/B 实验的结果，它验证了我们的混合排序模型的有效性。我们进一步展示了 TransAct 在上下文推荐和搜索等其他表面上的有效性。我们的模型已经部署到 Homefeed 的生产环境中，相关的 Pins，通知，和在 Pinterest 上的搜索。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TransAct:+Transformer-based+Realtime+User+Action+Model+for+Recommendation+at+Pinterest)|0|
|[A Personalized Automated Bidding Framework for Fairness-aware Online Advertising](https://doi.org/10.1145/3580305.3599765)|Haoqi Zhang, Lvyin Niu, Zhenzhe Zheng, Zhilin Zhang, Shan Gu, Fan Wu, Chuan Yu, Jian Xu, Guihai Chen, Bo Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Personalized+Automated+Bidding+Framework+for+Fairness-aware+Online+Advertising)|0|
|[Privacy Matters: Vertical Federated Linear Contextual Bandits for Privacy Protected Recommendation](https://doi.org/10.1145/3580305.3599475)|Zeyu Cao, Zhipeng Liang, Bingzhe Wu, Shu Zhang, Hangyu Li, Ouyang Wen, Yu Rong, Peilin Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy+Matters:+Vertical+Federated+Linear+Contextual+Bandits+for+Privacy+Protected+Recommendation)|0|
|[Approximation Algorithms for Size-Constrained Non-Monotone Submodular Maximization in Deterministic Linear Time](https://doi.org/10.1145/3580305.3599259)|Yixin Chen, Alan Kuhnle||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Approximation+Algorithms+for+Size-Constrained+Non-Monotone+Submodular+Maximization+in+Deterministic+Linear+Time)|0|
|[Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference](https://doi.org/10.1145/3580305.3599284)|Junyan Li, Li Lyna Zhang, Jiahang Xu, Yujing Wang, Shaoguang Yan, Yunqing Xia, Yuqing Yang, Ting Cao, Hao Sun, Weiwei Deng, Qi Zhang, Mao Yang|Microsoft Research; Microsoft; Zhejiang University|Deploying pre-trained transformer models like BERT on downstream tasks in resource-constrained scenarios is challenging due to their high inference cost, which grows rapidly with input sequence length. In this work, we propose a constraint-aware and ranking-distilled token pruning method ToP, which selectively removes unnecessary tokens as input sequence passes through layers, allowing the model to improve online inference speed while preserving accuracy. ToP overcomes the limitation of inaccurate token importance ranking in the conventional self-attention mechanism through a ranking-distilled token distillation technique, which distills effective token rankings from the final layer of unpruned models to early layers of pruned models. Then, ToP introduces a coarse-to-fine pruning approach that automatically selects the optimal subset of transformer layers and optimizes token pruning decisions within these layers through improved $L_0$ regularization. Extensive experiments on GLUE benchmark and SQuAD tasks demonstrate that ToP outperforms state-of-the-art token pruning and model compression methods with improved accuracy and speedups. ToP reduces the average FLOPs of BERT by 8.1x while achieving competitive accuracy on GLUE, and provides a real latency speedup of up to 7.4x on an Intel CPU.|在资源受限的情况下，在下游任务中部署像 BERT 这样的预先训练的变压器模型是具有挑战性的，因为它们的推理成本很高，并且随着输入序列长度的增长而迅速增长。本文提出了一种基于约束和排序的令牌剪枝方法 TOP，该方法在输入序列通过层的同时选择性地去除不必要的令牌，使模型在保持精度的同时提高了在线推理速度。TOP 通过排序-提取令牌精馏技术克服了传统自注意机制中不准确的令牌重要性排序的局限性，该技术将有效的令牌排序从未修剪模型的最后一层提取到修剪模型的早期层。然后，TOP 引入了一种从粗到精的剪枝方法，该方法自动选择变压器层的最优子集，并通过改进的 $L _ 0 $正则化来优化这些变压器层内的令牌剪枝决策。在 GLUE 基准测试和 SQuAD 任务上的大量实验表明，ToP 优于最先进的令牌剪枝和模型压缩方法，具有更高的准确性和加速性。在 GLUE 上，TOP 减少了 BERT 的平均 FLOP 8.1 x，同时实现了具有竞争力的准确性，并且在 Intel CPU 上提供了高达7.4 x 的实际延迟加速。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Constraint-aware+and+Ranking-distilled+Token+Pruning+for+Efficient+Transformer+Inference)|0|
|[Learning Balanced Tree Indexes for Large-Scale Vector Retrieval](https://doi.org/10.1145/3580305.3599406)|Wuchao Li, Chao Feng, Defu Lian, Yuxin Xie, Haifeng Liu, Yong Ge, Enhong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Balanced+Tree+Indexes+for+Large-Scale+Vector+Retrieval)|0|
|[Generative Flow Network for Listwise Recommendation](https://doi.org/10.1145/3580305.3599364)|Shuchang Liu, Qingpeng Cai, Zhankui He, Bowen Sun, Julian J. McAuley, Dong Zheng, Peng Jiang, Kun Gai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Flow+Network+for+Listwise+Recommendation)|0|
|[Hyper-USS: Answering Subset Query Over Multi-Attribute Data Stream](https://doi.org/10.1145/3580305.3599383)|Ruijie Miao, Yiyao Zhang, Guanyu Qu, Kaicheng Yang, Tong Yang, Bin Cui||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyper-USS:+Answering+Subset+Query+Over+Multi-Attribute+Data+Stream)|0|
|[Reconsidering Learning Objectives in Unbiased Recommendation: A Distribution Shift Perspective](https://doi.org/10.1145/3580305.3599487)|Teng Xiao, Zhengyu Chen, Suhang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reconsidering+Learning+Objectives+in+Unbiased+Recommendation:+A+Distribution+Shift+Perspective)|0|
|[VQNE: Variational Quantum Network Embedding with Application to Network Alignment](https://doi.org/10.1145/3580305.3599542)|Xinyu Ye, Ge Yan, Junchi Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VQNE:+Variational+Quantum+Network+Embedding+with+Application+to+Network+Alignment)|0|
|[Debiasing Recommendation by Learning Identifiable Latent Confounders](https://doi.org/10.1145/3580305.3599296)|Qing Zhang, Xiaoying Zhang, Yang Liu, Hongning Wang, Min Gao, Jiheng Zhang, Ruocheng Guo|Hong Kong University of Science and Technology; University of Virginia; ByteDance Research; Chongqing University|Recommendation systems aim to predict users' feedback on items not exposed to them.   Confounding bias arises due to the presence of unmeasured variables (e.g., the socio-economic status of a user) that can affect both a user's exposure and feedback. Existing methods either (1) make untenable assumptions about these unmeasured variables or (2) directly infer latent confounders from users' exposure. However, they cannot guarantee the identification of counterfactual feedback, which can lead to biased predictions. In this work, we propose a novel method, i.e., identifiable deconfounder (iDCF), which leverages a set of proxy variables (e.g., observed user features) to resolve the aforementioned non-identification issue. The proposed iDCF is a general deconfounded recommendation framework that applies proximal causal inference to infer the unmeasured confounders and identify the counterfactual feedback with theoretical guarantees. Extensive experiments on various real-world and synthetic datasets verify the proposed method's effectiveness and robustness.|推荐系统旨在预测用户对未接触到的项目的反馈。由于存在不可测量的变量(例如，用户的社会经济地位) ，可以影响用户的曝光和反馈，混淆偏见就会产生。现有的方法要么(1)对这些未测量的变量做出不可靠的假设，要么(2)直接从用户的暴露中推断出潜在的混杂因素。然而，他们不能保证识别反事实反馈，这可能导致偏见的预测。在这项工作中，我们提出了一种新的方法，即可识别的解构者(iDCF) ，它利用一组代理变量(例如，观察到的用户特征)来解决上述非识别问题。提出的 iDCF 是一个通用的解构推荐框架，它应用近因推理来推断不可测量的混杂因素，并用理论保证来识别反事实反馈。在各种真实世界和合成数据集上的大量实验验证了该方法的有效性和鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Debiasing+Recommendation+by+Learning+Identifiable+Latent+Confounders)|0|
|[Hierarchical Invariant Learning for Domain Generalization Recommendation](https://doi.org/10.1145/3580305.3599377)|Zeyu Zhang, Heyang Gao, Hao Yang, Xu Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Invariant+Learning+for+Domain+Generalization+Recommendation)|0|
|[Narrow the Input Mismatch in Deep Graph Neural Network Distillation](https://doi.org/10.1145/3580305.3599442)|Qiqi Zhou, Yanyan Shen, Lei Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Narrow+the+Input+Mismatch+in+Deep+Graph+Neural+Network+Distillation)|0|
|[Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction](https://doi.org/10.1145/3580305.3599491)|Zhangchi Zhu, Lu Wang, Pu Zhao, Chao Du, Wei Zhang, Hang Dong, Bo Qiao, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang|Microsoft Research; East China Normal University; Microsoft 365|Learning from positive and unlabeled data is known as positive-unlabeled (PU) learning in literature and has attracted much attention in recent years. One common approach in PU learning is to sample a set of pseudo-negatives from the unlabeled data using ad-hoc thresholds so that conventional supervised methods can be applied with both positive and negative samples. Owing to the label uncertainty among the unlabeled data, errors of misclassifying unlabeled positive samples as negative samples inevitably appear and may even accumulate during the training processes. Those errors often lead to performance degradation and model instability. To mitigate the impact of label uncertainty and improve the robustness of learning with positive and unlabeled data, we propose a new robust PU learning method with a training strategy motivated by the nature of human learning: easy cases should be learned first. Similar intuition has been utilized in curriculum learning to only use easier cases in the early stage of training before introducing more complex cases. Specifically, we utilize a novel ``hardness'' measure to distinguish unlabeled samples with a high chance of being negative from unlabeled samples with large label noise. An iterative training strategy is then implemented to fine-tune the selection of negative samples during the training process in an iterative manner to include more ``easy'' samples in the early stage of training. Extensive experimental validations over a wide range of learning tasks show that this approach can effectively improve the accuracy and stability of learning with positive and unlabeled data. Our code is available at https://github.com/woriazzc/Robust-PU|从阳性和未标记数据中学习被称为阳性-未标记(PU)学习，近年来引起了人们的广泛关注。PU 学习中常用的一种方法是使用自组织阈值从未标记的数据中抽取一组伪阴性样本，这样传统的监督方法就可以同时应用于正样本和负样本。由于未标记数据之间存在标记不确定性，训练过程中不可避免地会出现将未标记阳性样本错误分类为阴性样本的错误，甚至可能累积。这些错误经常导致性能下降和模型不稳定。为了减轻标签不确定性的影响，提高正数和未标签数据学习的鲁棒性，我们提出了一种新的鲁棒性 PU 学习方法，其训练策略受人类学习的本质驱动: 应该首先学习简单的情况。在课程学习中也使用了类似的直觉，即在培训的早期阶段只使用较容易的案例，然后再引入更复杂的案例。具体来说，我们利用一种新的“硬度”测量方法来区分未标记样品与具有较大标记噪声的未标记样品。然后采用迭代训练策略，以迭代的方式对训练过程中的负样本选择进行微调，以便在训练的早期阶段包含更多的“简单”样本。通过对大量学习任务的大量实验验证表明，该方法能够有效地提高正数和未标记数据学习的准确性和稳定性。我们的代码可以在 https://github.com/woriazzc/robust-pu 找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Positive-Unlabeled+Learning+via+Noise+Negative+Sample+Self-correction)|0|
|[RankFormer: Listwise Learning-to-Rank Using Listwide Labels](https://doi.org/10.1145/3580305.3599892)|Maarten Buyl, Paul Missault, PierreAntoine Sondag|Amazon|Web applications where users are presented with a limited selection of items have long employed ranking models to put the most relevant results first. Any feedback received from users is typically assumed to reflect a relative judgement on the utility of items, e.g. a user clicking on an item only implies it is better than items not clicked in the same ranked list. Hence, the objectives optimized in Learning-to-Rank (LTR) tend to be pairwise or listwise.   Yet, by only viewing feedback as relative, we neglect the user's absolute feedback on the list's overall quality, e.g. when no items in the selection are clicked. We thus reconsider the standard LTR paradigm and argue the benefits of learning from this listwide signal. To this end, we propose the RankFormer as an architecture that, with a Transformer at its core, can jointly optimize a novel listwide assessment objective and a traditional listwise LTR objective.   We simulate implicit feedback on public datasets and observe that the RankFormer succeeds in benefitting from listwide signals. Additionally, we conduct experiments in e-commerce on Amazon Search data and find the RankFormer to be superior to all baselines offline. An online experiment shows that knowledge distillation can be used to find immediate practical use for the RankFormer.|在 Web 应用程序中，用户只能看到有限的条目，这种情况长期以来一直采用排名模型，将最相关的结果放在第一位。从用户收到的任何反馈通常被认为反映了对项目效用的相对判断，例如，用户点击一个项目只意味着它比没有在同一排名列表中点击的项目要好。因此，学习排名(Learning-to-Rank，LTR)中优化的目标往往是成对的或列表的。然而，由于只把反馈看作是相对的，我们忽略了用户对列表总体质量的绝对反馈，例如，当选择中没有项被点击的时候。因此，我们重新考虑标准的 LTR 范式，并讨论从这个列表范围的信号中学习的好处。为此，我们提出 RankForm 作为一种体系结构，其核心是一个 Transformer，可以联合优化一个新的列表范围评估目标和一个传统的列表式 LTR 目标。我们模拟公共数据集上的隐式反馈，并观察到 RankForm 成功地从列表宽信号中受益。此外，我们在亚马逊搜索数据上进行电子商务实验，发现排名前优于所有离线基线。一个在线实验表明，知识提取可以用来找到直接的实际应用的秩次前。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RankFormer:+Listwise+Learning-to-Rank+Using+Listwide+Labels)|0|
|[Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems](https://doi.org/10.1145/3580305.3599834)|Xiaohui Chen, Jiankai Sun, Taiqing Wang, Ruocheng Guo, LiPing Liu, Aonan Zhang|Apple Inc.; ByteDance Research; Tufts University; ByteDance Inc.|Data subsampling is widely used to speed up the training of large-scale recommendation systems. Most subsampling methods are model-based and often require a pre-trained pilot model to measure data importance via e.g. sample hardness. However, when the pilot model is misspecified, model-based subsampling methods deteriorate. Since model misspecification is persistent in real recommendation systems, we instead propose model-agnostic data subsampling methods by only exploring input data structure represented by graphs. Specifically, we study the topology of the user-item graph to estimate the importance of each user-item interaction (an edge in the user-item graph) via graph conductance, followed by a propagation step on the network to smooth out the estimated importance value.   Since our proposed method is model-agnostic, we can marry the merits of both model-agnostic and model-based subsampling methods. Empirically, we show that combing the two consistently improves over any single method on the used datasets.   Experimental results on KuaiRec and MIND datasets demonstrate that our proposed methods achieve superior results compared to baseline approaches.|数据子采样被广泛用于加速大规模推荐系统的训练。大多数次抽样方法是基于模型的，通常需要一个预先训练的试点模型来通过例如样本硬度来测量数据的重要性。然而，当导频模型被错误指定时，基于模型的子抽样方法就会变质。由于模型错误说明在实际推荐系统中一直存在，因此我们提出了模型无关的数据子抽样方法，只是探讨了用图表示的输入数据结构。具体来说，我们研究了用户项目图的拓扑结构，通过图电导来估计每个用户项目交互(用户项目图中的一条边)的重要性，然后通过网络上的传播步骤来平滑估计的重要性值。由于我们提出的方法是模型不可知的，我们可以结合模型不可知和基于模型的子抽样方法的优点。经验上，我们表明，结合使用这两种方法比使用的数据集上的任何单一方法都要好。在 KuaiRec 和 MIND 数据集上的实验结果表明，与基线方法相比，我们提出的方法取得了更好的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-Based+Model-Agnostic+Data+Subsampling+for+Recommendation+Systems)|0|
|[BOSS: A Bilateral Occupational-Suitability-Aware Recommender System for Online Recruitment](https://doi.org/10.1145/3580305.3599783)|Xiao Hu, Yuan Cheng, Zhi Zheng, Yue Wang, Xinxin Chi, Hengshu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BOSS:+A+Bilateral+Occupational-Suitability-Aware+Recommender+System+for+Online+Recruitment)|0|
|[Real Time Index and Search Across Large Quantities of GNN Experts for Low Latency Online Learning](https://doi.org/10.1145/3580305.3599893)|Johan Kok Zhi Kang, Sien Yi Tan, Bingsheng He, Zhen Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Real+Time+Index+and+Search+Across+Large+Quantities+of+GNN+Experts+for+Low+Latency+Online+Learning)|0|
|[A Preference-aware Meta-optimization Framework for Personalized Vehicle Energy Consumption Estimation](https://doi.org/10.1145/3580305.3599767)|Siqi Lai, Weijia Zhang, Hao Liu|The Hong Kong University of Science and Technology (Guangzhou)|Vehicle Energy Consumption (VEC) estimation aims to predict the total energy required for a given trip before it starts, which is of great importance to trip planning and transportation sustainability. Existing approaches mainly focus on extracting statistically significant factors from typical trips to improve the VEC estimation. However, the energy consumption of each vehicle may diverge widely due to the personalized driving behavior under varying travel contexts. To this end, this paper proposes a preference-aware meta-optimization framework Meta-Pec for personalized vehicle energy consumption estimation. Specifically, we first propose a spatiotemporal behavior learning module to capture the latent driver preference hidden in historical trips. Moreover, based on the memorization of driver preference, we devise a selection-based driving behavior prediction module to infer driver-specific driving patterns on a given route, which provides additional basis and supervision signals for VEC estimation. Besides, a driver-specific meta-optimization scheme is proposed to enable fast model adaption by learning and sharing transferable knowledge globally. Extensive experiments on two real-world datasets show the superiority of our proposed framework against ten numerical and data-driven machine learning baselines. The source code is available at https://github.com/usail-hkust/Meta-Pec.|车辆能耗(VEC)估算的目的是在出行前预测出行所需的总能量，这对出行规划和交通可持续性有重要意义。现有的方法主要集中在从典型行程中提取统计学显著因子，以改善 VEC 估计。然而，在不同的出行环境下，由于个性化驾驶行为的影响，每辆车的能源消耗可能会有很大的差异。为此，本文提出了一个基于偏好感知的元优化框架 Meta-Pec，用于个性化车辆能耗估算。具体来说，我们首先提出了一个时空行为学习模块来捕捉隐藏在历史行程中的潜在驱动偏好。此外，基于驾驶员偏好的记忆，我们设计了一个基于选择的驾驶行为预测模块，以推断特定路线上驾驶员的驾驶模式，为 VEC 估计提供额外的依据和监控信号。此外，提出了一种特定于驱动程序的元优化方案，通过全局学习和共享可转移知识来实现模型的快速自适应。在两个实际数据集上的大量实验表明，我们提出的框架对于十个数字和数据驱动的机器学习基线具有优越性。源代码可在 https://github.com/usail-hkust/meta-pec 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Preference-aware+Meta-optimization+Framework+for+Personalized+Vehicle+Energy+Consumption+Estimation)|0|
|[MUSER: A MUlti-Step Evidence Retrieval Enhancement Framework for Fake News Detection](https://doi.org/10.1145/3580305.3599873)|Hao Liao, Jiahao Peng, Zhanyi Huang, Wei Zhang, Guanghua Li, Kai Shu, Xing Xie|Illinois Institute of Technology; Shenzhen University; Microsoft Research Asia|The ease of spreading false information online enables individuals with malicious intent to manipulate public opinion and destabilize social stability. Recently, fake news detection based on evidence retrieval has gained popularity in an effort to identify fake news reliably and reduce its impact. Evidence retrieval-based methods can improve the reliability of fake news detection by computing the textual consistency between the evidence and the claim in the news. In this paper, we propose a framework for fake news detection based on MUlti-Step Evidence Retrieval enhancement (MUSER), which simulates the steps of human beings in the process of reading news, summarizing, consulting materials, and inferring whether the news is true or fake. Our model can explicitly model dependencies among multiple pieces of evidence, and perform multi-step associations for the evidence required for news verification through multi-step retrieval. In addition, our model is able to automatically collect existing evidence through paragraph retrieval and key evidence selection, which can save the tedious process of manual evidence collection. We conducted extensive experiments on real-world datasets in different languages, and the results demonstrate that our proposed model outperforms state-of-the-art baseline methods for detecting fake news by at least 3% in F1-Macro and 4% in F1-Micro. Furthermore, it provides interpretable evidence for end users.|在网上传播虚假信息的便利使得有恶意的个人能够操纵公众舆论，破坏社会稳定。近年来，基于证据检索的假新闻检测技术在可靠识别假新闻、减少假新闻影响等方面得到了广泛的应用。基于证据检索的方法通过计算新闻中证据与索赔之间的文本一致性，提高了假新闻检测的可靠性。本文提出了一种基于多步证据检索增强(MUSER)的假新闻检测框架，该框架模拟了人类在阅读新闻、总结新闻、查阅资料、推断新闻是真是假的过程中的步骤。该模型可以显式地对多个证据之间的依赖关系进行建模，并通过多步检索对新闻验证所需的证据进行多步关联。此外，该模型通过段落检索和关键证据选择，能够自动收集现有证据，节省了繁琐的人工证据收集过程。我们在不同语言的真实世界数据集上进行了广泛的实验，结果表明，我们提出的模型比最先进的基线方法在 F1-Macro 中检测假新闻的性能至少高出3% ，在 F1-Micro 中高出4% 。此外，它还为最终用户提供了可解释的证据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MUSER:+A+MUlti-Step+Evidence+Retrieval+Enhancement+Framework+for+Fake+News+Detection)|0|
|[PrivateRec: Differentially Private Model Training and Online Serving for Federated News Recommendation](https://doi.org/10.1145/3580305.3599889)|Ruixuan Liu, Yang Cao, Yanlin Wang, Lingjuan Lyu, Yun Chen, Hong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PrivateRec:+Differentially+Private+Model+Training+and+Online+Serving+for+Federated+News+Recommendation)|0|
|[Hierarchical Projection Enhanced Multi-behavior Recommendation](https://doi.org/10.1145/3580305.3599838)|Chang Meng, Hengyu Zhang, Wei Guo, Huifeng Guo, Haotian Liu, Yingxue Zhang, Hongkun Zheng, Ruiming Tang, Xiu Li, Rui Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Projection+Enhanced+Multi-behavior+Recommendation)|0|
|[End-to-End Query Term Weighting](https://doi.org/10.1145/3580305.3599815)|Karan Samel, Cheng Li, Weize Kong, Tao Chen, Mingyang Zhang, Shaleen Kumar Gupta, Swaraj Khadanga, Wensong Xu, Xingyu Wang, Kashyap Kolipaka, Michael Bendersky, Marc Najork||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=End-to-End+Query+Term+Weighting)|0|
|[UnifieR: A Unified Retriever for Large-Scale Retrieval](https://doi.org/10.1145/3580305.3599927)|Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Guodong Long, Kai Zhang, Daxin Jiang||Large-scale retrieval is to recall relevant documents from a huge collection given a query. It relies on representation learning to embed documents and queries into a common semantic encoding space. According to the encoding space, recent retrieval methods based on pre-trained language models (PLM) can be coarsely categorized into either dense-vector or lexicon-based paradigms. These two paradigms unveil the PLMs' representation capability in different granularities, i.e., global sequence-level compression and local word-level contexts, respectively. Inspired by their complementary global-local contextualization and distinct representing views, we propose a new learning framework, UnifieR, which unifies dense-vector and lexicon-based retrieval in one model with a dual-representing capability. Experiments on passage retrieval benchmarks verify its effectiveness in both paradigms. A uni-retrieval scheme is further presented with even better retrieval quality. We lastly evaluate the model on BEIR benchmark to verify its transferability.|大规模检索是从给定查询的大量集合中回收相关文档。它依靠表示学习将文档和查询嵌入到一个公共的语义编码空间中。根据编码空间的不同，现有的基于预训练语言模型(PLM)的检索方法可以粗略地分为基于密集向量的检索方法和基于词典的检索方法。这两种范式分别揭示了 PLM 在不同粒度上的表示能力，即全局序列级压缩和局部词级上下文。受到它们互补的全局-局部上下文化和不同表示视图的启发，我们提出了一种新的学习框架 UnifieR，它将密集向量检索和基于词典的检索结合在一个具有双重表示能力的模型中。文章检索基准的实验结果验证了该方法在两种范式下的有效性。进一步提出了一种单一检索方案，检索效果更好。最后通过对 BEIR 基准测试模型的评估，验证了该模型的可推广性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UnifieR:+A+Unified+Retriever+for+Large-Scale+Retrieval)|0|
|[Counterfactual Video Recommendation for Duration Debiasing](https://doi.org/10.1145/3580305.3599797)|Shisong Tang, Qing Li, Dingmin Wang, Ci Gao, Wentao Xiao, Dan Zhao, Yong Jiang, Qian Ma, Aoyang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Video+Recommendation+for+Duration+Debiasing)|0|
|[Semantic-Enhanced Differentiable Search Index Inspired by Learning Strategies](https://doi.org/10.1145/3580305.3599903)|Yubao Tang, Ruqing Zhang, Jiafeng Guo, Jiangui Chen, Zuowei Zhu, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic-Enhanced+Differentiable+Search+Index+Inspired+by+Learning+Strategies)|0|
|[Doctor Specific Tag Recommendation for Online Medical Record Management](https://doi.org/10.1145/3580305.3599810)|Yejing Wang, Shen Ge, Xiangyu Zhao, Xian Wu, Tong Xu, Chen Ma, Zhi Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Doctor+Specific+Tag+Recommendation+for+Online+Medical+Record+Management)|0|
|[On-device Integrated Re-ranking with Heterogeneous Behavior Modeling](https://doi.org/10.1145/3580305.3599878)|Yunjia Xi, Weiwen Liu, Yang Wang, Ruiming Tang, Weinan Zhang, Yue Zhu, Rui Zhang, Yong Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On-device+Integrated+Re-ranking+with+Heterogeneous+Behavior+Modeling)|0|
|[Empowering Long-tail Item Recommendation through Cross Decoupling Network (CDN)](https://doi.org/10.1145/3580305.3599814)|Yin Zhang, Ruoxi Wang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Lichan Hong, James Caverlee, Ed H. Chi|Texas AM University; Google Research, Brain Team|Recommenders provide personalized content recommendations to users. They often suffer from highly skewed long-tail item distributions, with a small fraction of the items receiving most of the user feedback. This hurts model quality especially for the slices without much supervision. Existing work in both academia and industry mainly focuses on re-balancing strategies (e.g., up-sampling and up-weighting), leveraging content features, and transfer learning. However, there still lacks of a deeper understanding of how the long-tail distribution influences the recommendation performance.   In this work, we theoretically demonstrate that the prediction of user preference is biased under the long-tail distributions. This bias comes from the discrepancy of both the prior and conditional probabilities between training data and test data. Most existing methods mainly attempt to reduce the bias from the prior perspective, which ignores the discrepancy in the conditional probability. This leads to a severe forgetting issue and results in suboptimal performance. To address the problem, we design a novel Cross Decoupling Network (CDN) to reduce the differences in both prior and conditional probabilities. Specifically, CDN (i) decouples the learning process of memorization and generalization on the item side through a mixture-of-expert structure; (ii) decouples the user samples from different distributions through a regularized bilateral branch network. Finally, a novel adapter is introduced to aggregate the decoupled vectors, and softly shift the training attention to tail items. Extensive experimental results show that CDN significantly outperforms state-of-the-art approaches on popular benchmark datasets, leading to an improvement in HR@50 (hit ratio) of 8.7\% for overall recommendation and 12.4\% for tail items.|推荐程序向用户提供个性化内容推荐。他们经常受到高度扭曲的长尾条目分布的影响，其中一小部分条目接受了大部分用户反馈。这会损害模型的质量，特别是对于没有很多监督的切片。学术界和业界现有的工作主要集中在重新平衡策略(例如，上调样本和上调权重)、利用内容特性和转移学习。然而，对于长尾分布是如何影响推荐性能的，目前还缺乏更深入的理解。本文从理论上证明了在长尾分布下，用户偏好的预测是有偏差的。这种偏差来自于训练数据和测试数据之间先验概率和条件概率的差异。大多数现有的方法主要试图从先验的角度减少偏差，而忽略了条件概率的差异。这会导致严重的遗忘问题，并导致次优性能。为了解决这一问题，我们设计了一种新的交叉解耦网络(CDN) ，以减少先验概率和条件概率的差异。具体来说，CDN (i)通过混合专家结构解耦项目侧记忆和概括的学习过程; (ii)通过正则化的双边分支网络解耦来自不同分布的用户样本。最后，引入一种新的适配器对解耦后的向量进行聚合，并将训练注意力柔和地转移到尾项上。广泛的实验结果表明，CDN 在流行的基准数据集上显着优于最先进的方法，导致总体推荐的 HR@50(命中率)改善为8.7% ，尾部项目的改善为12.4% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empowering+Long-tail+Item+Recommendation+through+Cross+Decoupling+Network+(CDN))|0|
|[PDAS: A Practical Distributed ADMM System for Large-Scale Linear Programming Problems at Alipay](https://doi.org/10.1145/3580305.3599883)|Jun Zhou, Yang Bao, Daohong Jian, Hua Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PDAS:+A+Practical+Distributed+ADMM+System+for+Large-Scale+Linear+Programming+Problems+at+Alipay)|0|
|[Practical Design of Performant Recommender Systems using Large-scale Linear Programming-based Global Inference](https://doi.org/10.1145/3580305.3599183)|Aman Gupta, S. Sathiya Keerthi, Ayan Acharya, Miao Cheng, Borja Ocejo Elizondo, Rohan Ramanath, Rahul Mazumder, Kinjal Basu, J. Kenneth Tay, Rupesh Gupta|Norton Healthcare, University of Kentucky, USA.; University of Virginia School of Medicine, USA.; Alzheimer's Association, USA.; University of Colorado at Denver, USA.; Burke Rehabilitation Hospital, USA.; The Hastings Center, USA.; Novant Health Sports Medicine, USA.; Metro Orthopedics & Sports Therapy, USA.; Banyan Biomarkers, USA.; National Collegiate Athletic Association, USA.; Children's Hospital, Harvard Medical School, USA.; Icahn School of Medicine at Mount Sinai, USA.; Stanford Center on Longevity, USA.; George Washington School of Medicine, USA.; Andrews Institute for Orthopaedics and Sports Medicine, USA.; Alzheimer's Drug Discovery Foundation, 57 West 57th Street, Suite 904, New York, NY 10019, USA.; Baylor College of Medicine, USA.; Safe Kids Worldwide, Inc., USA.; University of California, USA.; Boston University Medical Center, USA.; CrowdOptic, Inc., USA.|Sports-related concussions and repetitive subconcussive exposure are increasingly recognized as potential dangers to paediatric populations, but much remains unknown about the short-term and long-term consequences of these events, including potential cognitive impairment and risk of later-life dementia. This Expert Consensus Document is the result of a 1-day meeting convened by Safe Kids Worldwide, the Alzheimer's Drug Discovery Foundation, and the Andrews Institute for Orthopaedics and Sports Medicine. The goal is to highlight knowledge gaps and areas of critically needed research in the areas of concussion science, dementia, genetics, diagnostic and prognostic biomarkers, neuroimaging, sports injury surveillance, and information sharing. For each of these areas, we propose clear and achievable paths to improve the understanding, treatment and prevention of youth sports-related concussions. In 2009, around 250,000 nonfatal traumatic brain injuries (TBIs) were recorded among individuals aged <19 years in the USA.1 The Centers for Disease Control and Prevention estimate that young people aged 5–18 years sustain 65% of all sports-related concussions.2 Despite recent advances in diagnostic brain imaging and in our understanding of the physics of concussion, long-term cognitive outcomes remain poorly understood. As the physical, cognitive and emotional consequences of concussion gain wider public attention, our incomplete knowledge of how to prevent, diagnose and treat such injuries endangers the health of our children in general and the health of their brains in particular. This Expert Consensus Document is the result of a 1-day meeting of experts in the fields of paediatric and adult TBI, Alzheimer disease (AD) research, genetics, epidemiology, bioethics and sports medicine (Box 1), which was convened in November 2013 by Safe Kids Worldwide, the Alzheimer's Drug Discovery Foundation and the Andrews Institute for Orthopaedics and Sports Medicine. Our primary goal is to highlight critical gaps in our knowledge of child and adolescent concussion. We emphasize areas where research is needed, such as development of diagnostic and predictive biomarkers, elucidation of genetic risk factors, and prediction of short-term and long-term outcomes. In our conclusions, we suggest paths toward improving our understanding of the long-term consequences of sports-related paediatric concussion. The term 'concussion' is often used interchangeably with the term 'mild TBI' (mTBI), a potentially misleading practice considering the possible extent of brain damage and potential for chronic neuropsychological dysfunction following concussion. We should stress, however, that most concussions resolve without sequelae. The American Congress of Rehabilitative Medicine defines mTBI as a Glasgow Coma Scale3 score of 13–15, with loss of consciousness for <30 min and post-traumatic amnesia lasting <24 h.4 Concussion describes a heterogeneous mixture of injury phenotypes that depends on many factors, including the magnitude, location and direction of head impact. Despite a lack of macroscopic structural findings, concussive brain injury involves primary neuronal injury caused by linear and rotational shear forces that disrupt axonal and membrane function (diffuse axonal injury,5 ionic flux and glutamate excitotoxicity), followed by secondary pathophysiological effects including mitochondrial oxidative stress, disruption of cerebral blood flow, compromised blood–brain barrier (BBB) integrity, synaptic dysfunction, and neuroinflammation.6, 7 Lasting neuropsychological post-concussion symptoms (post-concussion syndrome) comprise mood disorders (for example, depression), difficulty concentrating, and memory problems (Box 2).8 Both physical and physiological components of concussive injury can damage the developing brain, putting youths engaged in impact sports at particular risk. The necks and torsos of young athletes are weaker than those of older individuals and, consequently, less force is required to cause brain injury. The developing brain might also be particularly vulnerable to axonal damage caused by the shearing forces of head trauma, which, in youth American football, can exceed linear acceleration forces of 100 g.9 However, the average forces sustained in youth sports will generally be smaller than at higher levels of sport. Proper synaptic development is critical to cognitive and behavioural health.10, 11, 12, 13, 14, 15 Processes such as neurogenesis, competitive synaptic elimination ('pruning'), myelination, and axonal and dendritic arborization continue from prenatal development throughout the lifespan.14 The frontal and temporal lobes are the last areas to mature, and humans experience pruning in these regions into their early 20s,16 so damage to these still-developing areas may have pathophysiological effects on the brain that increase the potential for neuropsychological problems later in life.17 Axonal myelination continues through adolescence into the early 20s, and is susceptible to disruption by injury.10, 18, 19, 20, 21, 22 Early results from the Professional Fighters Brain Health Study, a 5-year longitudinal study of boxers and mixed martial arts fighters, who experienced repetitive subconcussive injuries as well as concussions, indicate that earlier age of first exposure to competitive boxing correlates with greater loss of caudate volume and greater axonal damage in the frontal lobe.23, 24 The young brain also has features that contribute to its resilience. Increased neuroplasticity in this age group has been shown to contribute to better outcomes after focal injuries.25 In addition, developing animals display a shorter window of glucose metabolic impairment in response to repeat TBI than do adult animals.26 Overall, the developing brain shows both vulnerability and resilience after TBI. These interwoven factors are likely to account for differences in the effects of concussion and repeat mTBI on young versus adult brains. A conservative approach to concussion risk and greater efforts to investigate these developmental differences should be given high priority. Most people—both young and old—recover fully from concussions. In children, factors potentially influencing recovery include age and history of concussions.27, 28 In one study, approximately 90% of young adult male athletes experienced symptomatic recovery within 21 days.29 However, in an emergency department study of patients aged 11–22 years (including all causes of concussion, not just sports-related), 15% of the sample still exhibited post-concussion symptoms, including headache, dizziness, 'mental fogginess' and depression, 90 days after injury.30 Several studies suggest that high school American football players are slower to recover from concussion than are college31, 32 and professional players.33 No direct comparisons with adolescents below high school age have yet been published, although a recent study that included a pre-adolescent age group (11–12 years) suggested that post-concussion recovery duration may not exhibit a linear relationship with age,30 as adolescents in this sample took longer to recover than did the pre-adolescent children. These findings, taken together, imply a unique risk of lengthier recovery in the male adolescent age group. Further studies of younger children and females would add greatly to our ability to assess and mitigate risk across the full paediatric and adolescent age span. Youths who sustained one or more concussions within 1 year prior to a new concussion reported more-prolonged symptoms,30 suggesting a possible 'window of vulnerability', and placing previously injured youths at higher risk of protracted recovery. Adolescents aged 11–18 years were nearly 80% more likely to develop post-concussion syndrome after presenting in emergency rooms than were children aged 5–10 years; similarly, presentation with headache doubled the risk of post-concussion syndrome in both children and adolescents.34 Among children treated in an emergency room after mTBI, those aged >6 years reported higher rates of persistent symptoms 3 months post injury than did those aged <6 years.35 Of course, the ability to acquire accurate information about concussion symptoms in children <6 years of age may be limited by a lack of self-awareness of symptoms and the necessary verbal skills to effectively communicate those symptoms. Also, direct comparison of injury severity is not possible from these reports; in fact, the physical heterogeneity of various injuries, taken together with the individual's innate capacity to recover from concussion, makes such comparisons highly challenging. 'Smart helmets' are being used in some speciality research centres to standardize the physical force and angular acceleration that accompanies head hits, and the utility of these helmets to measure and predict impacts that may result in concussion is currently under investigation.36, 37 Young people recovering from concussion can experience important challenges, including altered social and academic development,38, 39, 40 lower scores on general intelligence tests, and decreased school performance (measured by grade-point average).39 Lower levels of parental education and child academic achievement both correlate with poorer concussion recovery.41 Personality traits also play a part; for example, pre-injury anxiety is a risk factor for prolonged recovery periods after sports-related concussion.42 Young athletes of both sexes are at risk of concussion, but girls report higher concussion rates than boys, particularly in high school and college soccer, basketball, and baseball or softball.28, 43, 44, 45 The factors that account for these differences remain uncertain, but might include quality of protective gear, recognition and reporting of concussion symptoms, and neck length and neck muscle strength.46 Differences in recovery trajectories between males and females are also poorly understood. However, one recent study suggested that progesterone levels in females influence post-concussion recovery.47 Hormonal changes during puberty that contribute to migraine headaches might also contribute to sex differences in concussion recovery. Migraine headaches are up to fourfold more common in females than in males after puberty,48, 49 and some evidence suggests that migraineurs recover more slowly after concussion.50, 51 Research is warranted to further delineate sex differences in concussion risk and recovery. In general, adult concussive brain injury is much better understood than its counterpart in children and adolescents. Several points are important to note. First, concussion has multiple, non-harmonized definitions. Second, concussion diagnosis is an imperfect art. Last, in the absence of rapid and inexpensive objective diagnostic measures, concussion remains a clinical diagnosis that is subject to variability—including different thresholds for diagnosis across various subspecialities and across individual physicians, neuropsychologists and athletic trainers—and under-reporting by coaches, parents and young athletes. Without validated diagnostics, concussion will remain a nebulous and under-reported entity, and the accuracy of incidence estimates will continue to be tainted by the differential application of inexact criteria. Repetitive subconcussive trauma can result in structural and functional brain changes.52 White matter abnormalities detected by diffusion tensor imaging (DTI) have been reported in professional soccer players even in the absence of any obvious history of concussions. Compared with swimmers, male professional soccer players showed DTI signal changes suggestive of decreased white matter integrity in several brain regions, which might indicate loss of axonal myelination, similar to changes seen in individuals with mTBI.53 Collegiate ice hockey players exhibited similar white matter changes over the course of a season.54, 55, 56, 57 In addition, repetitive subconcussive head impacts in collegiate American football players have been linked, in a dose-dependent manner, to deficits in BBB integrity, potential loss of white matter integrity, and cognitive dysfunction.58 These findings probably reflect some level of risk for youths who sustain repetitive subconcussive head impacts, although little research has been devoted specifically to this topic. A metric to track head impacts—that is, a 'hit count'—has been proposed,59 and could serve as one factor to determine cumulative risk exposure. One challenge of this approach is to accurately define the parameters of a 'hit', but improved biosensors show some promise in this regard. Similar to a 'pitch count' in baseball, this concept has also recently been proposed for boxers.24 No evidence is currently available to show a causal link between repetitive subconcussive head impacts in youth and dementia later in life, and such metrics could prove invaluable if validated by future studies correlating head impacts with subsequent neuropsychological dysfunction. In adults, TBI, including concussion,60, 61, 62 might increase an individual's risk of developing neurodegenerative disease,63, 64 including AD and chronic traumatic encephalopathy (CTE), a disease associated exclusively with repetitive head trauma.65, 66 TBI may also increase the risk of developing Parkinson disease (PD),67 although the relationship between mTBI and PD risk remains uncertain.68 In paediatric populations, particularly young athletes, the effects of single or repetitive concussions on the risk of later-life neurodegeneration and dementia are unknown. CTE was first described symptomatically in the late 1920s as 'punch-drunk' dementia in boxers,69 was later described as 'dementia pugilistica',70 and was first described pathologically in 1973.71 Since the identification of CTE in a former professional American football player in 2005,72 and additional intensive pathological studies, this condition has gained widespread public attention, and has now been identified in brains of former ice hockey, baseball, rugby and soccer players,73 wrestlers,74 and military veterans.75, 76 The prevalence and incidence of CTE in amateur and professional athletes is still unknown, adding to difficulties in discussing its epidemiology and population risks for athletes. Although CTE is primarily considered to be a neurodegenerative disease that sometimes results from a career of either collegiate or professional contact sports, cases of CTE have been reported in high school athletes.77 This finding suggests that long sporting careers are not required for CTE development, and that youth athletes represent an at-risk population. Emerging evidence suggests that clinical CTE symptoms can be grouped into two common presentations: cognitive and mood–behavioural.78, 79 Subjective memory complaints such as anterograde amnesia are common, as are mood disorders including anxiety or depression,79 and reduced executive function, which can result in disinhibition and impaired decision-making skills.80 These clinical symptoms define disease severity.81 The neurodegenerative pathophysiology of CTE is complex, and the neurological sequelae are poorly understood. In severe cases, the cerebral cortex and medial temporal lobes seem most profoundly affected,81, 82 with pathology characterized by neurofibrillary tangles composed of phosphorylated tau79 and, in some cases, TAR DNA-binding protein 43 pathology.83 CTE is also associated with marked atrophy, notably in the frontal cortex and medial temporal lobe, as well as in the mammillary bodies, thalamus and hypothalamus.79 Confirmed clinical diagnosis of CTE remains autopsy-based.84 Given the uncertainty over whether the tauopathy described in CTE is causative of the clinical phenotype, and the fact that most professional and collegiate athletes do not develop CTE, it is vital to understand whether early exposure to concussion is associated with other forms of neurodegeneration and cognitive dysfunction, including chronic neurocognitive impairment (CNI). Important clinical distinctions exist between CTE and CNI,28, 51 some of which make direct comparisons difficult. CTE is an emerging clinical and pathological condition that involves progressive deterioration of neurological and cognitive function in multiple domains, and is diagnosed primarily at autopsy. Conversely, the CNI phenotype is not necessarily progressive, and is characterized by functional decline from group averages or baseline functioning established before TBI. CNI can be diagnosed clinically through neuropsychological testing. No causal link between CNI and head trauma has yet been confirmed, but a dose-dependent risk has consistently been found in professional athletes.28 In addition, almost half of the studies conducted in amateur athletes have found an elevated risk of CNI.28 Whether similar risk associations are present in younger populations remains to be determined. One hypothesis is that CNI represents a prodromal—but not inevitable—step toward CTE, analogous to the relationship between mild cognitive impairment (MCI) and AD.85, 86 Alternatively, CNI may represent static impairment without degeneration. Our current lack of understanding of the basic biological underpinnings of CNI and CTE underscores the need for more research. Increased knowledge of the biology of both conditions, as well as early detection of CNI in athletes (in particular, youth athletes), may drive interventions to stem the development of further cognitive impairment, and could also aid validation of putative biomarkers. Assessment of CNI via tau imaging may help determine the likelihood of progression to CTE. The field of concussion genetics, especially in paediatric populations, is still in its infancy. Although repetitive head impacts seem necessary for the development of CTE, other factors, including genetics, are likely to have an important role, as most concussed athletes do not develop CTE.87 The genetic risk factors for CTE probably overlap with those that influence susceptibility to and recovery from concussion, and genetic risk factors for AD are providing important clues to the identity of these factors. The ε4 allele of apolipoprotein E (APOE ε4), the most important genetic risk factor for AD identified to date,88 critically affects the CNS injury response,89 in particular, amyloid-β (Aβ) clearance from the brain. The three alleles of APOE confer varying degrees of AD risk: APOE ε2 reduces the risk, APOE ε3, the most common allele, represents baseline risk with which other variants are compared, and APOE ε4 increases the risk.90, 91 Studies suggest an interaction between APOE ε4 and sex, such that APOE ε4-related risk of AD is more prominent in women than in men.92, 93 The APOE genotype acts synergistically with TBI in increasing the risk of AD,94 although its hypothesized risk association with CTE as an outcome of repetitive mTBI requires more study.95 No consensus has yet been reached on the effects of APOE isotype on the outcome of paediatric TBI, but data from adults suggest that APOE ε4 negatively influences concussion outcomes. Several studies indicate that possession of at least one APOE ε4 allele is associated with poorer cognition and lasting neuropsychological impairment after concussion in professional American football players,96 boxers95 and other adults,97, 98, 99, 100 although other studies found no such association.101, 102 Some evidence points to polymorphisms in both the APOE gene and its promoter as contributory factors to concussion risk in college athletes.103, 104 Another study did not identify a role for APOE ε4 in concussion risk,105 although this allele might increase the risk of dementia following midlife or late-life mTBI.106 Drawing conclusions from these conflicting studies is difficult, owing to small sample sizes and differing methodologies. In children, little is known about the relationship between APOE ε4 and neuropsychological outcomes after concussion, and APOE ε4 testing is not routine in paediatric TBI studies. In 2012, Kurowski reviewed the few existing studies and combined the results of three studies107, 108, 109 that used the Glasgow Outcome Scale.110 In the combined sample (252 children), the risk of poor clinical outcomes after 6–12 months was over twofold higher in APOE ε4 carriers than in noncarriers (19% versus 9%). However, these studies included a broad developmental range of children with heterogeneous injuries, and did not account for a possible interaction between age and genotype. In addition, the interaction between APOE and sex has not been studied in the context of concussion. Improved prospective studies are warranted to clarify these connections. Incorporation of genetics into paediatric concussion research is fraught with complicated challenges, including acquisition of parental consent and informed consent for a child, perceived stigmatization of clinical study participants, the actionability of the genetic knowledge obtained, and potential concerns regarding insurability (particularly long-term care insurance). Studies of adults who learn of their APOE ε4+ status demonstrate that many are willing to make lifestyle modifications, including increased exercise and improved medication management,111 as well as increased purchases of health and long-term care insurance.112, 113 Education about new genetic knowledge and corresponding disease risk is essential, as demonstrated by the substantial discordance between an individual's personal feelings about the implications of the acquired knowledge and the actual consequences of increased dementia risk.114 The effects of APOE genetic knowledge on children, their families and decision-making processes regarding participation in impact sports remain unclear. The influence of APOE genotype on concussion risk and recovery in this age group also needs further elucidation. If future studies find that, for any particular level of impact, children with APOE ε4+ status are at greater risk of concussion or poor recovery than are their APOE ε4− peers, consideration should be given to genetic testing of school-age athletes before participation in impact sports. Careful studies of high school and younger athletes are required to fully understand the nuances of genetic influences. Future research into youth concussion outcomes, including cognitive outcomes and risk of dementia, should include APOE genotyping wherever possible. New APOE studies should standardize research methodologies and reporting measures, including the collection of 'common data elements', to ensure valid comparison across studies.110, 115 The APOE genotype is not necessarily a non-modifiable risk factor for concussion recovery: therapies being developed for AD include drugs that modify the interaction between the ApoE4 protein and Aβ, which might also be applicable to paediatric concussion.116, 117 The Val66Met polymorphism in the gene encoding brain-derived neurotrophic factor has been linked to better outcomes after mTBI,118 but worse outcomes after focal penetrating brain injury.119 Polymorphisms in genes involved in dopaminergic signalling may also help to account for the wide range of TBI outcomes.120 In addition, the Rep1 polymorphism in the promoter region of the α-synuclein gene might increase the risk of PD after head injury.121 To advance our understanding of concussion risk and management, large, prospective, population-based genome-wide association studies (GWAS) and whole-genome sequencing studies should be conducted to identify other genetic variants—possibly of low frequency or low penetrance—that modify the risk of prolonged recovery, poor cognitive outcomes or dementia.122 Such studies will require large-scale data sharing, and must address issues of ethics, privacy, and potential implications for insurability and employability. Despite progress in identifying possible cerebrospinal fluid (CSF) and blood-based biomarkers that might be applied to adult TBI management, no clinically validated biomarkers are available for either the adult or the paediatric population. Paediatric concussions present with even greater clinical variability than do adult concussions; therefore, biomarkers have special potential for improving concussion diagnosis in children. Of note, most TBI biomarkers have been studied in the context of moderate to severe TBI, leaving us with obvious gaps in our knowledge of mTBI biomarkers, especially in children. Biomarker development has been critical to the advancement of AD therapeutics. CSF-based biomarkers are already being employed to identify at-risk patients and to improve the design of both epidemiological studies and clinical trials.123 New PET radioligands, such as amyloid-labelling agents (three of which are now FDA-approved), can be used both diagnostically and to improve neuropathology-based patient stratification for clinical trials. Several tau imaging agents are also in human trials, and their utility in tauopathies, including CTE, is rapidly being established. As with fluid-based biomarkers, there are currently no neuroimaging biomarkers sensitive or specific enough to diagnose concussion or CTE in either adults or children. No TBI diagnostic or therapeutic agents have yet been approved by the FDA, and validation of concussion biomarkers could accelerate the development of such agents. Efforts must be made, however, to ensure the cost-effectiveness and wide availability of clinical biomarker testing. Also, given the risks associated with lumbar puncture, ethical concerns regarding sampling of CSF from concussed youths for biomarker research should be addressed. Promising findings in adult fluid-based biomarker research must be explored in paediatric populations. Putative concussion biomarkers have emerged sporadically in the scientific literature over the past few decades, the most prominent being S100 calcium-binding protein B (S100B), a nonspecific marker of astrocyte activation. The presence of S100B in serum may indicate loss of BBB integrity. Elevated serum and CSF levels of S100B have been observed in adult boxers after matches, and correlate positively with the number and severity of head impacts.124, 125 Increased serum S100B levels have also been observed in concussed professional ice hockey players,126 with levels measured 1 h post-concussion predicting symptomatic recovery time. However, S100B levels were also raised after controlled play where no concussions occurred, indicating that this marker is not injury-specific.126 Indeed, S100B serum levels are elevated in adult trauma patients without head injury.127, 128, 129 Other research suggests that initial post-concussion S100B levels are poor predictors of recovery.130 As with all biomarkers, the role of S100B in TBI management in children is even less clear,131 with some arguing that this marker has little diagnostic or prognostic utility in paediatric populations.132 In a study of children with TBI aged ≤15 years, those <5 years or >9 years of age had higher serum levels of S100B than did those aged 5–9 years.133 S100B may, therefore, be an inadequate marker to distinguish between symptomatic and asymptomatic children with concussion,133 and the utility of S100B in diagnostics and outcome prognosis is questionable.134, 135, 136 Neuron-specific enolase (NSE) is a marker of neuronal injury, but its usefulness as a serum or CSF biomarker remains uncertain.133, 134, 135, 136, 137 Elevated serum NSE levels have been observed after head impacts in boxers,124 but were also seen in ice hockey players after a match where no concussions occurred.126 Serum NSE levels failed to predict recovery time after concussion,126 and might not correlate with injury severity in children.133 In children aged ≤15 years, serum NSE levels correlate inversely with age.133 Once released into the blood, NSE has slow elimination kinetics, making it difficult to distinguish primary from secondary neuronal injuries on the basis of NSE levels.138, 139 Neurofilament light chain and glial fibrillary acidic protein (GFAP) are CSF neuron-specific and glial-specific damage markers, respectively, and are both elevated in CSF in adult boxers after fights.125, 137, 140 Little is known about either marker in the context of paediatric concussion, but a preliminary study in children and young adults suggested that serum GFAP levels within 72 h after concussion correlate with symptom burden up to 1 month post injury.141 The neuron-specific protein UCH-L1 (ubiquitin carboxyl-terminal hydrolase isozyme L1) was first linked to neurodegenerative pathology through its involvement in PD,142 and its presence in serum was later identified as a biomarker for severe TBI.143, 144, 145 Serum levels of UCH-L1 may have diagnostic utility in concussion,146 but recent evidence suggests a lack of correlation between elevated serum levels and subconcussive hits.147 The clinical utility of UCH-L1 in paediatric populations warrants further study. Perhaps the most promising advances in adult fluid-based TBI biomarkers concern tau protein. Serum or CSF tau levels are thought to indicate axonal damage, as tau normally resides in axons, where it stabilizes microtubules. Serum tau is proteolytically cleaved,148 and in patients with AD, levels of cleaved tau in CSF might correlate with cognitive function.149 Tau levels in CSF and blood are elevated in boxers after a match, and CSF tau levels correlate with the quality and quantity of head impacts.125, 150 Recent evidence suggests that tau levels are elevated in the blood of ice hockey players after concussion, and may be useful in predicting recovery time.126 Questions remain, however, with several studies reporting little or no value of serum cleaved tau for predicting post-concussion syndrome or long-term outcomes.130, 151 The potential of tau as a biomarker in children remains unclear, with no studies conducted to date. In fact, the reliability of serum tau as a biomarker has not yet been established for any indication. The likelihood is that no single biomarker will suffice to diagnose paediatric concussion or predict outcomes. In addition, few studies have examined the interactions between genetic make-up and putative biomarkers. As our understanding of the relationships of biomarkers to injury severity and to each other increases, development of biomarker panels, perhaps incorporating inflammatory and oxidative markers,152 should be considered. Future studies should attempt to further define these relationships and establish the clinical value of biomarker panels, factoring in commercial cost and practical feasibility. Recent advances in metabolomics, lipidomics and proteomics—in particular, the search for metabolomic and lipidomic markers for AD—might inform future research into biomarkers for concussion and subconcussive injuries. Several recent studies propose altered metabolite and lipid profiles associated with MCI and AD.153, 154, 155, 156 Data from animal models suggest that lipid and metabolite changes accompany both acute and chronic post-concussion periods, and could be useful for predicting recovery trajectory,157, 158 but these findings have yet to be validated in humans. Expanding the biomarker search beyond blood and CSF to saliva and urine159 might improve the ability to obtain measurements rapidly and noninvasively, particularly from children. Sampling of CSF from children, particularly when rapid assessment is desirable, is largely impractical. Mondello et al. proposed a set of useful criteria for evaluating TBI biomarkers that should allow more-streamlined development and validation.137 Any validated biomarker panel must, inevitably, be a component of a larger, multimodal diagnostic suite that may include structural and functional imaging and neuropsychological testing. When designing future biomarker studies, the potential for FDA approval should be considered, in order to expedite approval for clinical use. Although concussion remains a clinical diagnosis, neuroimaging techniques are improving our understanding of the structural and functional consequences in adults. Neuroimaging in paediatric populations may be limited by several factors; for example, measurements of longitudinal changes after concussion are complicated by the background of a dynamic, immature brain. No imaging techniques have been validated as diagnostic tools for concussion, and the correlation between imaging findings and clinically measurable cognitive or behavioural functions is variable. Tools such as volumetric imaging, DTI and functional MRI (fMRI)—in particular, arterial spin labelling—are currently being explored.160, 161 Fractional anisotropy (FA), as measured by DTI, allows inference of the structural integrity of white matter tracts, which are commonly disrupted after TBI. The clinical implications of FA change remain controversial, as both increased and decreased FA has been observed in concussion studies.162, 163, 164, 165, 166 These discrepancies may be due, in part, to the considerable spatial heterogeneity in the brain areas examined,167 as well as differences in the post-injury interval. FA may still have prognostic value, with evidence suggesting that the direction and magnitude of change correlates with clinical outcomes;166, 168 however, this idea awaits validation in both paediatric and adult populations. FA might lack the necessary sensitivity to fully appreciate changes in white matter tract integrity following brain injury, and measures of diffusivity may be more appropriate.169 The DTI field would benefit greatly from the development of normative data sets against which to gauge observed changes. Pre-game versus post-game and season-long studies of young athletes could employ serial DTI imaging to establish normative data for a particular individual, but the utility of the data when pooled is unclear. The scarcity of normative paediatric data severely limits the clinical usefulness of neuroimaging techniques, including DTI. Studies of 'return-to-baseline' neuroimaging after paediatric concussion are also needed, as they could greatly improve prediction of recovery. Although automation has increased reproducibility, DTI measurements remain sensitive to the hardware and software specifics, acquisition parameters and analysis software, which limit reproducibility, standardization and comparison between centres and across studies. Efforts to standardize DTI across imaging centres are underway.170 MRI has been particularly successful in mapping the brain's 'connectome'—the collection of structural and functional neural connectivity networks and their respective focal nodes—and for studying how concussion affects these networks. Focal or diffuse TBI can disrupt the brain's functional connectivity, resulting in dysfunction of multiple networks including the default mode and salience networks, which have been implicated in memory, emotion and mood.171 Network dysfunction might have a stronger influence on recovery than does lesion location,171, 172, 173 but the long-term implications for brain development and cognitive function remain unclear.26, 174 Further studies of network connectivity dysfunction in children after concussion will be critical to improve injury prognostication and management. Radiotracers for PET imaging have the potential to advance the diagnosis and treatment of concussion and CTE, but their use in paediatric populations is purely investigational at present. Three FDA-approved radiolabelled imaging agents are currently available for detecting brain amyloid in patients with suspected AD.175 In adults, some cases of concussion are associated with acute Aβ pathology. PET scanning could enable paediatric patients to be monitored for the presence and persistence of acute post-concussion amyloid, and to determine whether scan positivity and negativity predict different outcomes.176, 177 Other PET imaging agents with potential utility in paediatric populations include new tracers that bind neurofibrillary tangles composed of tau. Early imaging results with 18F-T807, 18F-T808 and 18F-THK5105 are proving to be useful in confirming the presence of tauopathy in various clinical situations, including AD.178, 179, 180 In a recent AD study, the magnitude of tau tracer signal correlated positively with the stage of disease and severity of cognitive impairment.180 A third tau PET tracer, 11C-PBB3, has been tested in healthy individuals and patients with AD, and may be able to detect non-AD conformations of tau.181 In addition, a recent report contains the first description of tauopathy imaging in a living person with suspected sports-associated CTE.177 Given the extent of chronic tau pathology in concussion, repetitive subconcussive injury and CTE, tau tracers may be useful as diagnostic and prognostic biomarkers (for example, to distinguish CNI from CTE). Studies with these tracers in adults with CTE are underway, but their use in paediatric populations will depend on future research to determine whether tau pathology is present in young patients after TBI or concussion. A PET tracer for the microglial cholesterol transporter protein might be useful for imaging of neuroinflammation associated with TBI.182 New PET ligands to image brain microglia, which are being developed with potential utility in neurodegenerative diseases, may also prove useful in concussion and CTE management. Exploration of these PET ligands in paediatric populations with concussion and TBI would be informative, but risk–benefit analyses must be performed before embarking on studies involving radiotracers in this age group. The ultimate utility of any PET imaging agent will depend on its diagnostic and prognostic value as part of a multimodal panel of biomarkers and neuroimaging techniques. Noninvasive techniques such as transcranial magnetic stimulation (TMS) have also uncovered changes in synaptic plasticity following TBI and concussion,183 particularly in asymptomatic individuals.184, 185, 186 Several small TMS studies of young athletes in their early 20s with a history of concussion suggest imbalances in γ-aminobutyric acid and/or glutamate neurotransmission in the motor cortex that are associated with deficits in synaptic long-term potentiation and depression.184, 185, 187, 188 TMS has also revealed that concussion-related impairments in synaptic plasticity can impair aspects of motor learning,188 and that these deficits are detectable decades after an individual's last concussion.189 Another crucial noninvasive tool for detecting neurochemical dysfunction associated with concussion is proton magnetic resonance spectroscopy (MRS). Reports specifically addressing the use of spectroscopy following sports-related concussion suggest various abnormalities consistent with neurochemical alterations.190 In younger (high school) athletes, increased glutamate and glutamine levels were detected by MRS at post-season versus pre-season evaluation, even in players who had not experienced clinically significant concussion during the season.191 Such findings suggest that even subconcussive head impacts can result in the activation of glutamate pathways, implying cellular injury or neuronal death, despite the absence of symptoms. Levels of creatinine and myoinositol (an organic osmolyte located in astrocytes192, 193) were also significantly altered in a subset of the participants in the aforementioned study. In a rare longitudinal study utilizing MRS,194 individuals who sustained a single sports-related concussion exhibited significantly reduced levels of N-acetylaspartate (NAA, a marker of neuronal and axonal health, integrity and functioning195) in the brain 3 days after injury. Levels were increased at 15 days post injury, and reverted to control values at 30 days post injury. By contrast, participants who sustained a second concussion 10–13 days after their initial concussion displayed a prolonged reduction in NAA levels, which had not normalized even 45 days post injury. These results suggest that repeated injury within a short time frame increases the likelihood of protracted or incomplete recovery. In addition to the acute and subacute alterations detected by MRS, other studies of the long-term effects of concussion have disclosed increased myoinositol (associated with glial proliferation) and decreased choline (associated with membrane turnover195) levels in the medial temporal lobe in otherwise healthy former athletes who sustained their last concussion more than three decades prior to testing.196 Another recent study examined a cohort of symptomatic retired National Football League players, using an advanced MRS method called correlated spectroscopy (COSY), which can measure additional metabolites.197 The authors identified increased choline and glutamate–glutamine levels (indicative of diffuse axonal injury and excitotoxicity, respectively), consistent with previous mTBI MRS studies, as well as additional cerebral metabolites that were indicative of neuroinflammatory changes. These metabolic changes may provide insight into mechanisms of injury, such as excitotoxicity and/or inflammation, which could underlie the reported structural changes. Overall, the available data support the use of MRS as a research tool to identify altered neurophysiology and monitor recovery in adult athletes, even following resolution of post-concussive symptoms. At present, MRS-detected biochemical alterations may enhance our understanding of the underlying pathophysiology, but do not yet provide specific diagnostic information. Larger cross-sectional, prospective and longitudinal studies are needed to determine the sensitivity and prognostic value of MRS within the field of sports-related concussion.190 Because the interpretation of MRS in the immature brain requires certain developmental considerations, appropriate comparison samples will be needed for future work in children. MRS techniques with greater spectral resolution, including COSY, might provide additional biochemical specificity.197 Other advances in spatial resolution, such as 3D chemical shift imaging, may also provide greater specificity by allowing the investigation of metabolic alterations throughout the brain rather than in specific regions of interest. Finally, MRS could have a role in measurement of treatment effects, such as those induced by transcranial direct current stimulation198 and TMS.199 The mechanisms and surveillance infrastructure for sports-related injury measurement, reporting, tracking and data sharing are insufficient for current needs and objectives. Concussion research and clinical efforts are hindered by a lack of concussion data across sports and playing levels. A 2014 Institute of Medicine report identified only three national sports injury surveillance systems: the National Electronic Injury Surveillance System—All Injury Program (NEISS-AIP), the National Collegiate Athletic Association Injury Surveillance System (NCAA ISS), and the High School Reporting Injury Online (RIO™).1 These systems can be supplemented with clinical data (for example, from emergency departments, hospitalized inpatients and sports clinics), but these data are biased toward more-severe injuries and patients of higher socioeconomic status. Indeed, schools in rural areas or communities with lower socioeconomic status often have limited access to sports medicine care professionals and facilities. Several emerging programmes may improve surveillance. Regional efforts such as Clinical Outcomes Research Education for Athletic Trainers (CORE-AT) and national efforts such as the National Athletic Trainers' Association National Athletic Treatment, Injury and Outcomes Network (NATA NATION™) attempt to integrate injury tracking with treatment and outcomes data at the high school and collegiate levels. However, none of these systems specifically capture injuries to younger athletes, those participating in non-school sponsored sports, or those at schools without athletic trainers. Sports injury databases also rarely account for demographic factors including socioeconomic status, race or ethnicity, and health-care coverage. Currently, no effective mechanisms exist to consistently and inexpensively link various surveillance data sets, or to follow up individual athletes across sports, tracking systems or the age continuum. There is a considerable need for a system that tracks individual athletes through their playing careers and beyond. Each individual should be tracked for several decades to establish if, when and how a given burden of TBI evolves into CTE, and to assess all the possible negative health outcomes associated with concussion. Such a system would also provide more-accurate descriptions of concussion history and exposure to risk factors, and could capture both short-term and long-term outcomes, including measures of physical and mental health, academic and career success, quality of life and social connectivity, and evolving socioeconomic status. Such efforts are challenged by a variety of issues, including a lack of mandatory reporting of concussion at any level. Mandatory concussion reporting, funding for surveillance efforts, and provision of training to data reporters (for example, coaches and athletic trainers) would greatly improve epidemiological research. However, mandatory reporting will not provide meaningful results without validated, consensus definitions for concussions, and development of a universal data repository and a global unique identifier (GUID) system. Data sets from standardized surveillance efforts could then be linked, thereby improving data sharing for research and clinical care. Coupling of surveillance data with standardized collection, storage and curation infrastructures for biobanking of tissue and fluid samples could dramatically improve injury and outcomes research.200 These efforts might be catalyzed by funding from public–private partnerships, and made actionable by setting realistic short-term and long-term goals to create a multi-year plan. However, in the USA at least, such efforts are currently hampered by misunderstanding of Health Insurance Portability and Accountability Act (HIPAA) regulations and general concerns for athlete confidentiality. Wider use of computerized neurocognitive testing (CNT) for athletes could improve concussion surveillance, as well as diagnosis and management. However, several important challenges must be overcome before CNT becomes routine. These challenges include a lack of standardized administration protocols, the potential for technological errors arising from different computer hardware, limits in the types of cognitive functions assessed, and a lack of qualified test administrators and data interpreters.201 Despite these shortcomings, however, CNT is already used by approximately 40% of US high schools that employ athletic trainers.202 Though not affordable for all schools, CNT could enhance ground-level data collection and aid risk-exposure estimation and post-concussion recovery tracking, as well as increasing the quality of data reported to sports injury surveillance networks. CNT may be also useful in evaluating and tracking post-concussion cognitive improvement or decline, and could have utility in predicting outcomes.203, 204 Whether CNT data collected in the school setting will reach the validation and reproducibility standards achieved by CNT conducted by a clinical research team remains to be seen. Importantly, CNT needs standardization and guidelines for determining 'return to play' and 'return to learn' for athletes who show recovery in one domain but are still symptomatic in others. More research is required on the utility of CNT, both in the clinic and for concussion surveillance and management of youth athletes. In several critical areas, incomplete knowledge hampers meaningful advances in the field of paediatric concussion. At the molecular and cellular levels, research that focuses on axonal damage after concussion and repetitive subconcussive injury is urgently needed to elucidate changes in axonal trafficking and repair, and to better define the role of transient Aβ accumulation as a potential driver of downstream and/or future pathology. Concussion researchers may need to identify more-suitable animal models to study molecular pathology, including tau and its contribution to post-concussion and CTE pathologies, as the structure and organization of the brain differs dramatically in rodents and humans. Without a clearer understanding of how TBI changes the young, still-developing brain, and what pathological events happen in the weeks, months and years following injury, we are left to speculate about the underlying biological bases of such changes. Head impact data collection and risk assessment in youth sports might be improved through use of sensor technologies that record linear and rotational forces. Such commercially available devices, if validated, could determine levels of cumulative head impact forces during games and across seasons of play, and the findings could be linked to neuroimaging data and functional outcome assessments. Combined with 'hit-count' metrics, sensor data may improve knowledge of short-term and long-term neuropsychological outcomes of repetitive subconcussive impacts. Our knowledge of CTE might be improved by understanding baseline rates in the general population, in injured athletes, among uninjured athletes matched by sport and playing positions, and in 'control' athletes in low-risk sports. Improved knowledge of risk exposures could lead to prevention efforts, including practice and competition rule changes. A decades-long, prospective, longitudinal study, following youth athletes through their sporting careers and beyond, would provide more-definitive knowledge of cumulative head impacts and risks of long-term neuropsychological dysfunction and dementia. Such a study is underway in NCAA alumni, who were first studied in 2003 and were re-assessed in 2013.29, 205 Studies in other populations, especially if NIH-funded, would probably begin with a 5-year study that could be renewed in further 5-year increments. Public–private partnerships are likely to be required to secure enough funding to involve multiple study centres. The NCAA has provided partial sponsorship for the 10-year re-assessment of over 100 athletes, but further funding from the NIH, the US Department of Defense (DoD), and private philanthropic sources will be required to extend the range of assessment from neuropsychology, through MRI, to molecular imaging for amyloid, tau and/or inflammation. Ideally, the longitudinal study design should combine epidemiological and interventional trial methodologies and utilize multiple control groups, including non-contact athletes and uninjured impact sport athletes. A longitudinal study would also shed light on the role of cognitive reserve. A precedent for such studies has been established by the late-life dementia research community, using NIH funds and public–private partnerships involving pharmaceutical companies and foundations. For such studies to be successful, additional surveillance systems and data repositories must first be established. Efforts would be accelerated if athletes participating in impact sports had universal access to athletic trainers, who could act as reliable data reporters while promoting safety and providing basic care. In addition, any longitudinal studies must include postmortem analyses to better understand the influence of childhood and young-adult concussions on the development of neurodegenerative pathology and dementia in later life. 'Return-to-play' guidelines are currently hampered by a lack of rigorous epidemiological evidence, and could be greatly improved by long-term safety data from longitudinal studies.206 Longitudinal research could also include studies to determine whether those athletes who fail to follow guidelines experience any negative health effects, such as lingering symptoms or altered risk of incurring a second concussion. The infrastructure for a long-term prospective study might be created through the formation of a research consortium modelled after the Alzheimer's Disease Neuroimaging Initiative (ADNI). ADNI has set standards for data collection, dissemination agreements, testing methodologies, and biomarker collection and analysis. A version of ADNI currently underway with participation of the DoD (ADNI-DoD) is focused on blast-related TBI research in military populations.207 In May 2014, in addition to the NCAA Concussion Study, the NCAA and the DoD announced the launch of the largest prospective sports-related concussion study to date, which will monitor approximately 37,000 NCAA athletes over 3 years. One can envision how this study's infrastructure may eventually be extended to study younger athletes over an extended longitudinal range. Many gaps remain in our knowledge of the biology of TBI, which limit our ability to develop effective drugs. These gaps must be filled if we are to tackle the underlying disease pathology and move beyond treating the symptoms. However, much can be accomplished while research into fundamental TBI biology continues. Drug repurposing involves testing of existing FDA-approved drugs for new indications, and can reduce expense and shorten the path for drug approval. Current repurposing trials include methylphenidate for pain and mental fatigue,208 the dopamine receptor agonist bromocriptine for working memory,209 and the antidepressant sertraline for mood and anxiety, the most frequent neuropsychological complications that influence long-term outcomes after concussion.210 Larger randomized clinical trials should be conducted before these drugs can be introduced into clinical practice for these new indications. In addition, the recent failure of the PROTECT phase III trial of progesterone to improve outcomes after acute TBI211 may serve as a reminder of the need for more research to better understand the fundamental biology underlying TBI. Although many drug repurposing efforts are designed primarily to address concussion symptoms, the drugs may also influence injury pathology and progression. Research on established drugs can also lead to new drug discovery efforts and, potentially, new preventive or management therapeutics. New drugs are urgently needed for TBI and concussions that do not resolve. Drug discovery efforts in the areas of neuroprotection and anti-inflammation are especially relevant because of their potential cross-applicability to neurodegenerative diseases such as AD. Similarly, drugs currently in development for other neurodegenerative diseases might be repositioned for testing in patients with TBI or nonresolving concussion symptoms. As is often the case in medical research, recent advances in concussion research raise as many questions as they answer. Evidence exists for long-term neuropsychological dysfunction and later-life dementia after concussions or repetitive subconcussive head impacts, and more work is needed to better understand the implications and outcomes of youth participation in impact sports. As outlined in this Expert Consensus Document, there is a path forward, but achieving the goals outlined here will require public and private sector cooperation. While recommendations can be improved with increased knowledge, the available evidence can still inform individual decision-making when considering youth sport participation, as well as practice policies and competition rules. With an ageing population and a looming epidemic of dementia, we must learn more about potential early-life risk factors, including sports-related concussion. The choices made by parents, coaches, school boards and children will be better informed when the critical gaps in scientific knowledge of concussion are filled. Download references|与运动相关的脑震荡和重复性亚震荡暴露越来越被认为是儿科人群的潜在危险，但是对于这些事件的短期和长期后果，包括潜在的认知障碍和晚年痴呆的风险，仍然知之甚少。这份专家共识文件是由全球安全儿童、阿尔茨海默氏症药物发现基金会和安德鲁斯矫形外科和运动医学研究所召集的为期一天的会议的结果。目标是强调在脑震荡科学、痴呆症、遗传学、诊断和预后生物标志物、神经影像学、运动损伤监测和信息共享等领域的知识差距和亟需研究的领域。针对这些领域，我们提出了明确和可实现的途径，以提高对青少年体育相关脑震荡的理解、治疗和预防。2009年，美国年龄 < 19岁的个体中记录了约250,000例非致命性创伤性脑损伤(TBI)。1疾病控制和预防中心估计，5-18岁的年轻人维持着所有运动相关脑震荡的65% 。2尽管最近在诊断性脑成像方面取得了进展，并且在我们对脑震荡物理学的理解方面，长期的认知结果仍然知之甚少。由于脑震荡的身体、认知和情感后果引起了公众的广泛关注，我们对如何预防、诊断和治疗这种伤害的不完整知识危及我们儿童的总体健康，特别是他们的大脑健康。这份专家共识文件是儿科和成人创伤性脑损伤、阿兹海默病(AD)研究、遗传学、流行病学、生物伦理学和运动医学领域专家为期一天的会议的结果(专栏1) ，该会议于2013年11月由全球安全儿童、阿尔茨海默氏症药物发现基金会和安德鲁斯矫形外科和运动医学研究所召集。我们的主要目标是强调我们在儿童和青少年脑震荡知识方面的重大差距。我们强调需要进行研究的领域，如开发诊断和预测性生物标志物，阐明遗传风险因素，以及预测短期和长期结果。在我们的结论中，我们提出了提高我们对与运动相关的儿童脑震荡的长期后果的理解的途径。术语“脑震荡”经常与术语“轻度 TBI”(mTBI)交替使用，考虑到脑震荡后可能的脑损伤程度和慢性神经心理功能障碍的潜在可能性，这是一种潜在的误导性做法。然而，我们应该强调的是，大多数脑震荡不会产生后遗症。美国康复医学会将 mTBI 定义为格拉斯哥昏迷量表3评分为13-15分，意识丧失 < 30分钟，创伤后遗忘持续时间 < 24小时。脑震荡描述了损伤表型的异质混合物，取决于许多因素，包括头部撞击的大小，位置和方向。尽管缺乏宏观结构发现，脑震荡损伤涉及由线性和旋转剪切力破坏轴突和膜功能(弥漫性轴突损伤，5离子通量和谷氨酸兴奋毒性)引起的原发性神经元损伤，随后是继发性病理生理效应，包括线粒体氧化应激，脑血流中断，血脑屏障(BBB)完整性受损，突触功能障碍和神经炎症。持续的神经心理学脑震荡后症状(脑震盪症候群)包括情绪障碍(例如抑郁症) ，难以集中和记忆问题(方框2)。年轻运动员的脖子和躯干比老年人的脖子和躯干更弱，因此，造成脑损伤所需的力量更少。发育中的大脑也可能特别容易受到由头部创伤的剪切力引起的轴突损伤，这在美国青年足球中可以超过100g 的线性加速力。然而，青年运动中持续的平均力量通常会小于较高水平的运动。正确的突触发育对认知和行为健康至关重要。神经发生、竞争性突触消除(“修剪”)、髓鞘形成、轴突和树突树枝化等过程在产前发育的整个生命周期中持续进行。额叶和颞叶是最后成熟的区域，人类在20岁出头的时候经历了这些区域的修剪[16] ，因此这些仍在发育的区域的损伤可能对大脑产生病理生理效应，增加了以后生活中出现神经心理问题的可能性。轴突髓鞘形成在青春期持续到20岁出头，易受损伤的影响。职业拳击手大脑健康研究的早期结果表明，第一次接触拳击比赛的年龄越早，尾状核体积损失越大，额叶轴突损伤越严重。这项研究对拳击手和追踪研究综合格斗拳击手进行了5年的研究，他们都经历过重复性的脑震荡和脑震荡。23,24年轻的大脑也有一些有助于恢复的特征。已经显示，这个年龄组的神经可塑性增加有助于局灶性损伤后更好的结果[25]。此外，发育中的动物对重复 TBI 的葡萄糖代谢障碍的窗口比成年动物更短[26]。总的来说，发育中的大脑在 TBI 后显示出脆弱性和恢复力。这些相互交织的因素可能解释了脑震荡和重复 mTBI 对年轻人和成年人大脑影响的差异。应高度重视对脑震荡风险采取保守的方法，并加大努力调查这些发育差异。大多数人ーー无论老少ーー从脑震荡中完全恢复过来。在儿童中，可能影响康复的因素包括年龄和脑震荡史。27,28在一项研究中，大约90% 的年轻成年男运动员在21天内经历了症状恢复。然而，在一项针对11-22岁患者(包括所有脑震荡原因，而不仅仅是运动相关)的急诊科研究中，15% 的样本在受伤后90天仍然表现出脑震荡后症状，包括头痛，头晕，“精神模糊”和抑郁。一些研究表明，美国高中橄榄球运动员从脑震荡中恢复的速度比大学运动员和职业运动员要慢。尽管最近一项包括青春期前年龄组(11-12岁)的研究表明，脑震荡后恢复持续时间可能与年龄没有线性关系，但与高中以下青少年的直接比较尚未发表[30] ，因为这个样本中的青少年恢复时间比青春期前的儿童更长。这些发现加在一起，意味着男性青春期年龄组的恢复时间较长的独特风险。对年幼儿童和女性的进一步研究将大大提高我们评估和减轻整个儿科和青少年年龄段风险的能力。在新的脑震荡发生前1年内遭受一次或多次脑震荡的青少年报告出现更长时间的症状，30表明可能存在“脆弱性窗口”，并将先前受伤的青少年置于更高的长期恢复风险中。11-18岁的青少年在急诊室出现脑震荡后发生脑震盪症候群的可能性比5-10岁的儿童高出近80% ，同样，伴有头痛的儿童和青少年出现脑震盪症候群的风险增加了一倍。在 mtBI 后在急诊室接受治疗的儿童中，6岁以上的儿童在受伤后3个月报告持续症状的发生率高于6岁以下的儿童。当然，获得 < 6岁儿童脑震荡症状的准确信息的能力可能受到缺乏症状自我意识和有效沟通这些症状的必要语言技能的限制。此外，从这些报告中不可能直接比较损伤的严重程度; 事实上，各种损伤的身体异质性，加上个体从脑震荡中恢复的先天能力，使得这种比较具有高度挑战性。一些专业研究中心正在使用“智能头盔”来标准化头部撞击产生的体力和角加速度，目前正在研究这些头盔用于测量和预测可能导致脑震荡的影响。36,37从脑震荡中恢复的年轻人可能会经历重大挑战，包括社会和学术发展的改变，38,39,40在一般智力测试中得分较低，以及学校表现下降(以年级平均分衡量)。39较低的父母教育水平和儿童学业成绩都与较差的脑震荡恢复相关。人格特质也起到了一定的作用，例如，伤前焦虑是运动性脑震荡后长时间恢复的一个危险因素。42年轻的男女运动员都有脑震荡的危险，但是女孩的脑震荡发生率高于男孩，特别是在高中和大学的足球、篮球、棒球或垒球比赛中。28,43,44,45解释这些差异的因素仍然不确定，但可能包括保护装备的质量，脑震荡症状的识别和报告，以及颈部长度和颈部肌肉力量。46男女之间在恢复轨迹方面的差异也知之甚少。然而，最近的一项研究表明，女性黄体酮水平影响脑震荡后的恢复。47青春期激素变化导致偏头痛，也可能导致脑震荡后恢复的性别差异。在青春期后，女性偏头痛的发病率是男性的四倍[48,49] ，一些证据表明，偏头痛患者在脑震荡后恢复较慢[50,51]。有必要进一步研究脑震荡风险和恢复的性别差异。一般来说，成人脑震荡比儿童和青少年脑震荡更容易理解。有几点值得注意。首先，脑震荡有多种非协调的定义。其次，脑震荡诊断是一门不完善的艺术。最后，在缺乏快速和廉价的客观诊断措施的情况下，脑震荡仍然是一种临床诊断，受到变异性的影响，包括不同亚专业和个体医生、神经心理学家和运动训练员的诊断阈值不同，以及教练、家长和年轻运动员报告不足。如果没有经过验证的诊断，脑震荡将仍然是一个模糊和报告不足的实体，发病率估计的准确性将继续受到不确切标准的差别应用的影响。重复性次级脑震荡可导致大脑结构和功能的改变。52弥散张量成像(DTI)检测到的白质异常在职业足球运动员中已有报道，即使没有任何明显的脑震荡史。与游泳运动员相比，男性职业足球运动员表现出 DTI 信号改变，提示几个大脑区域的白质完整性降低，这可能表明轴突髓鞘形成的丧失，类似于 mTBI 患者的改变。53名大学冰球运动员在一个赛季中表现出类似的白质变化。54,55,56,57此外，美国大学生橄榄球运动员重复性亚震荡性头部撞击已经以剂量依赖性方式与 BBB 完整性缺陷，白质完整性潜在丧失和认知功能障碍有关。58这些研究结果可能反映了持续遭受重复性次生脑震荡撞击的青少年的某种程度的风险，尽管很少有专门针对这一主题的研究。一个跟踪头部影响的指标ーー即“命中次数”ーー已经提出，59可以作为确定累积风险敞口的一个因素。这种方法的一个挑战是准确定义“命中”的参数，但改进的生物传感器在这方面显示出一些希望。与棒球中的“投球次数”类似，这个概念最近也被提出用于拳击运动员。24目前没有证据表明青少年重复性脑震荡冲击与晚年痴呆之间的因果关系，如果未来的研究将头部冲击与随后的神经心理功能障碍相关联，这些指标可能被证明是无价的。在成年人中，包括脑震荡在内的脑外伤可能会增加个体发生神经退行性疾病的风险，包括 AD 和 CTE (CTE) ，这是一种仅与重复性头部创伤相关的疾病[65,66]。尽管 mTBI 和 PD 风险之间的关系仍然不确定，但 TBI 也可能增加发生帕金森氏症的风险[67]。在儿科人群，特别是年轻运动员中，单次或重复性脑震荡对晚年神经退行性疾病和痴呆风险的影响是未知的。CTE 在20世纪20年代后期首次被症状性描述为拳击运动员的“拳击醉”痴呆，69后来被描述为“痴呆拳击”[70] ，并在1973年首次被病理学描述[71]。自2005年在一名前职业美式足球运动员身上发现 CTE 以来,这种病症已经引起了公众的广泛关注，目前已经在前冰球、棒球、橄榄球和足球运动员、73名摔跤运动员、74名退伍军人的大脑中发现。75,76业余和职业运动员慢性创伤性脑病的患病率和发病率仍然是未知的，这增加了讨论其流行病学和运动员的人口风险的困难。虽然慢性创伤性脑病主要被认为是一种神经退行性疾病，有时是由大学或专业接触性运动的职业生涯造成的，但在高中运动员中也有慢性创伤性脑病的报道。这一发现表明，慢性创伤性脑病的发展并不需要长期的运动生涯，青年运动员代表着高危人群。新出现的证据表明，临床的慢性创伤性脑病症状可以分为认知和情绪行为两种常见表现[78,79]。主观记忆症状如顺行性遗忘症是常见的，包括焦虑或抑郁在内的情绪障碍也是常见的[79] ，并且执行功能降低，这可能导致去抑制和决策技能受损[80]。这些临床症状定义了疾病的严重程度[81]。慢性创伤性脑病的神经退行性病理生理学是复杂的，对神经系统后遗症的了解很少。在严重的情况下，大脑皮层和内侧颞叶似乎受到最深刻的影响，81,82与病理学拥有属性由磷酸化 tau79组成的神经原纤维缠结，在某些情况下，TAR DNA 结合蛋白43病理学。CTE 也与明显的萎缩有关，特别是在额叶皮层和内侧颞叶，以及在乳头体，丘脑和下丘脑。79确诊的 CTE 临床诊断仍以尸检为基础。鉴于慢性脑震荡中描述的重复病变是否引起临床表型的不确定性，以及大多数专业和大学运动员不发展慢性脑震荡的事实，了解早期暴露于脑震荡是否与其他形式的神经退行性疾病和认知功能障碍(包括慢性神经认知障碍(CNI))相关至关重要。CTE 和 CNI 之间存在重要的临床区别，其中一些使得直接比较困难。CTE 是一种新出现的临床和病理状况，涉及多个领域的神经和认知功能的进行性恶化，主要在尸检中诊断。相反，CNI 表型并不一定是进行性的，而是拥有属性功能从组平均值或基线功能下降到创伤性脑损伤之前的水平。CNI 可以通过神经心理测试进行临床诊断。CNI 与头部创伤之间的因果关系尚未得到证实，但在专业运动员中一直发现剂量依赖性风险。此外，在业余运动员中进行的几乎一半的研究发现 CNI 的风险升高。年轻人群中是否存在类似的风险关联仍有待确定。一个假设是 CNI 代表了慢性创伤性脑病的前驱症状，但并非不可避免，类似于轻微认知障碍和 AD 之间的关系。另外，CNI 可能代表静态损伤而不退化。我们目前对 CNI 和 CTE 的基本生物学基础缺乏了解，这强调了进一步研究的必要性。对这两种情况的生物学知识的增加以及运动员(特别是青年运动员) CNI 的早期检测可能会推动干预措施以阻止进一步认知障碍的发展，并且还可能有助于验证推定的生物标志物。通过 tau 成像评估 CNI 可能有助于确定进展为 CTE 的可能性。脑震荡遗传学领域，特别是在儿科人群中，仍然处于起步阶段。尽管重复的头部撞击似乎对于 CTE 的发展是必要的，但是包括遗传学在内的其他因素可能具有重要作用，因为大多数脑震荡运动员不发展 CTE.87 CTE 的遗传危险因素可能与影响脑震荡易感性和恢复的因素重叠，AD 的遗传危险因素为这些因素的身份提供了重要的线索。E型载脂蛋白质的 ε4等位基因(APOEε4)是迄今为止发现的 AD 最重要的遗传危险因素，它严重影响中枢神经系统的损伤反应，特别是从大脑中清除淀粉样蛋白 -β (Aβ)。APOE 的三个等位基因赋予不同程度的 AD 风险: APOEε2降低风险，APOEε3是最常见的等位基因，代表与其他变体进行比较的基线风险，APOEε4增加风险。90,91研究表明 APOEε4与性别之间存在相互作用，因此 APOEε4相关的 AD 风险在女性中比在男性中更为突出。92,93 APOE 基因型与 TBI 协同作用增加 AD 的风险[94] ，尽管其与 CTE 作为重复 mTBI 的结果的假设风险相关性需要更多的研究。关于 APOE 同种型对儿童 TBI 结果的影响尚未达成共识，但来自成年人的数据表明 APOEε4对脑震荡结果有负面影响。一些研究表明，拥有至少一个 APOEε4等位基因与美国职业橄榄球运动员，96名拳击运动员95和其他成年人97,98,99,100的脑震荡后认知较差和持续的神经心理障碍有关，尽管其他研究没有发现这种关联。101,102一些证据表明 APOE 基因及其启动子的多态性是大学生运动员脑震荡危险的促成因素。另一项研究没有确定 APOEε4在脑震荡风险中的作用[105] ，尽管这个等位基因可能增加中年或晚年 mTBI 后痴呆的风险。106由于样本量小，方法不同，很难从这些相互矛盾的研究中得出结论。在儿童中，对于 APOEε4与脑震荡后神经心理学结果之间的关系知之甚少，而且 APOEε4测试在儿科 TBI 研究中并不常规。2012年，Kurowski 回顾了少数现有的研究，并结合了使用格拉斯哥结果量表的三项研究的结果[107,108,109]。在合并样本(252名儿童)中，6-12个月后不良临床结果的风险在 APOEε4携带者中高于非携带者(19% 比9%)。然而，这些研究包括了广泛的异质性损伤儿童的发育范围，并没有考虑到年龄和基因型之间可能的相互作用。此外，APOE 与性别之间的相互作用尚未在脑震荡的背景下进行研究。改进的前瞻性研究有助于澄清这些联系。将遗传学纳入儿科脑震荡研究充满了复杂的挑战，包括获得父母同意和儿童的知情同意，临床研究参与者的感知耻辱，获得的遗传知识的可行性以及关于可保性(特别是长期护理保险)的潜在担忧。对了解 APOEε4 + 状态的成年人的研究表明，许多人愿意改变生活方式，包括增加运动和改善药物管理[111] ，以及增加购买健康和长期护理保险[112,113]。关于新的遗传知识和相应的疾病风险的教育是必不可少的，正如个人对获得的知识的影响的个人感觉与痴呆风险增加的实际后果之间的实质性不一致所证明的那样.114 APOE 遗传知识对儿童，其家庭和参与影响性体育的决策过程的影响尚不清楚。APOE 基因型对该年龄组脑震荡风险和恢复的影响也需要进一步阐明。如果未来的研究发现，对于任何特定水平的影响，具有 APOEε4 + 状态的儿童比其 APOEε4同龄人具有更大的脑震荡或恢复不良的风险，则应考虑在参加影响性运动之前对学龄运动员进行基因检测。要充分理解基因影响的细微差别，就需要对高中和年轻运动员进行仔细研究。未来对青少年脑震荡结果(包括认知结果和痴呆风险)的研究应尽可能包括 APOE 基因分型。新的 APOE 研究应标准化研究方法和报告措施，包括收集“共同数据元素”，以确保有效的比较研究。110,115 APOE 基因型不一定是脑震荡恢复的不可改变的危险因素: 正在开发的 AD 治疗包括改变 ApoE4蛋白和 Aβ 之间相互作用的药物，这也可能适用于儿科脑震荡。编码脑源性神经营养因子的基因中的 Val66Met 多态性与 mtBI 后更好的结果有关，但与局灶性穿透性脑损伤后更差的结果有关。参与多巴胺能信号传导的基因多态性也可能有助于解释广泛的 TBI 结果。120此外，α-synuclein 基因启动子区的 Rep1多态性可能增加头部损伤后帕金森病的风险。为了提高我们对脑震荡风险和管理的理解，应该进行大型的前瞻性基于人群的全基因组关联研究(GWAS)和全基因组测序研究，以确定其他遗传变异(可能是低频率或低外显率) ，这些变异可以改变长期恢复，认知结果差或痴呆的风险。122这样的研究将需要大规模的数据共享，并且必须解决道德、隐私以及对可保性和可雇佣性的潜在影响等问题。尽管在确定可能应用于成人创伤性脑损伤治疗的可能的脑嵴液(CSF)和血液生物标志物方面取得了进展，但成人或儿科人群都没有经过临床验证的生物标志物。与成人脑震荡相比，儿童脑震荡的临床变异性更大; 因此，生物标志物在改善儿童脑震荡诊断方面具有特殊的潜力。值得注意的是，大多数 TBI 生物标志物已经在中度至重度 TBI 的背景下进行了研究，这使我们在 mTBI 生物标志物的知识方面存在明显的差距，特别是在儿童中。生物标志物的发展对 AD 治疗的进步至关重要。基于脑脊液的生物标志物已经被用于识别高危患者，并改善流行病学研究和临床试验的设计。123新的 PET 放射性配体，如淀粉样蛋白标记剂(其中三种现在是 FDA 批准的) ，可以用于诊断和改善基于神经病理学的患者临床试验分层。一些 tau 成像剂也在人体试验中，它们在包括 CTE 在内的 tau 病中的应用正在迅速建立。与基于液体的生物标志物一样，目前还没有足够敏感或特异的神经影像生物标志物来诊断成人或儿童的脑震荡或 CTE。目前 FDA 尚未批准任何创伤性脑损伤的诊断或治疗药物，而脑震荡生物标志物的验证可以加速这类药物的开发。然而，必须努力确保临床生物标志物检测的成本效益和广泛可用性。此外，考虑到与腰椎穿刺相关的风险，对脑震荡青少年脑脊液取样用于生物标志物研究的伦理问题应该得到解决。在成人体液为基础的生物标志物研究中有希望的发现必须在儿科人群中探索。过去数十年，推定脑震荡的生物标志物在科学文献中零星出现，其中最突出的是星形胶质细胞活化的非特异性标志物 S100钙结合蛋白 B (S100B)。血清中 S100B 的存在可能提示血脑屏障完整性的丧失。在成年拳击手比赛后观察到血清和脑脊液 S100B 水平升高，并且与头部撞击的数量和严重程度呈正相关。在脑震荡的职业冰球运动员中也观察到血清 S100B 水平升高，126在脑震荡后1小时测量的水平预测症状恢复时间。然而，S100B 的水平也提高后，控制发挥，没有发生脑震荡，表明这一标志物是不伤害特异性。事实上，没有头部损伤的成年创伤患者血清 S100B 水平升高。127,128,129其他研究表明，脑震荡后最初的 S100B 水平对于恢复不能很好地预测。与所有生物标志物一样，S100B 在儿童 TBI 管理中的作用甚至更不清楚[131] ，一些人认为这种标志物在儿科人群中几乎没有诊断或预后效用。132在一项关于≤15岁 TBI 患儿的研究中，5岁以下或9岁以上儿童的血清 S100B 水平高于5-9岁儿童。因此，S100B 可能不足以区分有症状和无症状的脑震荡儿童[133] ，S100B 在诊断和预后预后方面的效用是值得怀疑的。134,135,136神经元特异性烯醇化酶(NSE)是神经元损伤的标志物，但其作为血清或脑脊液生物标志物的用途仍不确定。拳击手头部撞击后观察到血清 NSE 水平升高[133,134,135,136,137] ，但在没有发生脑震荡的比赛后，冰球运动员也观察到 NSE 水平升高。血清 NSE 水平无法预测脑震荡后的恢复时间，可能与儿童损伤严重程度无关。133在≤15岁的儿童中，血清 NSE 水平与年龄呈负相关。一旦释放到血液中，NSE 具有缓慢的消除动力学，使得难以根据 NSE 水平区分原发性和继发性神经元损伤。神经丝轻链和胶质纤维酸性蛋白(GFAP)分别是 CSF 神经元特异性和胶质特异性损伤标志物，并且在成年拳击手打斗后 CSF 均升高。125,137,140在儿科脑震荡的情况下，对任何一种标志物都知之甚少，但对儿童和年轻成年人的初步研究表明，脑震荡后72小时内的血清 GFAP 水平与损伤后1个月的症状负担相关。神经元特异性蛋白 UCH-L1(泛素羧基末端水解酶同工酶 L1)首先通过参与 PD 与神经退行性病理学相关[142] ，其在血清中的存在后来被确定为严重 TBI 的生物标志物。血清 UCH-L1水平可能对脑震荡有诊断价值[146] ，但最近的证据表明血清水平升高与脑震荡次数之间缺乏相关性。UCH-L1在儿科人群中的临床应用值得进一步研究。也许最有希望的进展成人液基 TBI 生物标志物涉及 tau 蛋白。血清或脑脊液 tau 蛋白水平被认为表明轴突损伤，因为 tau 蛋白通常存在于轴突中，稳定微管。在 AD 患者中，脑脊液中切割的 tau 蛋白水解水平可能与认知功能相关。拳击手在比赛后脑脊液和血液中的 Tau 水平升高，脑脊液 Tau 水平与头部撞击的质量和数量相关。125,150最近的证据表明，脑震荡后冰球运动员血液中的 tau 水平升高，可能有助于预测恢复时间。然而，问题依然存在，一些研究报道血清切割 tau 对预测脑震盪症候群或长期结果的价值很小或没有价值。130,151 tau 作为儿童生物标志物的潜力尚不清楚，至今没有进行研究。事实上，血清 tau 作为一种生物标志物的可靠性尚未被确定为任何适应症。这种可能性是没有单一的生物标志物将足以诊断儿童脑震荡或预测结果。此外，很少有研究调查遗传组成和推定的生物标志物之间的相互作用。随着我们对生物标志物与损伤严重程度及其相互关系的理解的增加，生物标志物小组的发展，可能包括炎症和氧化标志物，152应该被考虑。未来的研究应试图进一步确定这些关系，建立生物标志物小组的临床价值，考虑到商业成本和实际可行性。代谢组学、脂质组学和蛋白质组学的最新进展ーー特别是寻找 AD 的代谢组学和脂质组学标志物ーー可能为今后研究脑震荡和脑震荡下损伤的生物标志物提供参考。最近的一些研究提出了与 MCI 和 AD 相关的代谢物和脂质谱的改变.153,154,155,156来自动物模型的数据表明，脂质和代谢物变化伴随着急性和慢性脑震荡后期，并且可能有助于预测恢复轨迹，157,158但是这些发现尚未在人类中得到验证。将生物标志物的搜索范围从血液和脑脊液扩展到唾液和尿液159，可能会提高快速和非侵入性测量的能力，特别是从儿童身上。从儿童抽取脑脊液样本，特别是在需要快速评估的情况下，在很大程度上是不切实际的。Mondello 等人提出了一套评估 TBI 生物标志物的有用标准，这些标准应该允许更精简的开发和验证.137任何经过验证的生物标志物小组必然是更大的多模式诊断套件的组成部分，其中可能包括结构和功能成像以及神经心理学测试。在设计未来的生物标志物研究时，应考虑 FDA 批准的可能性，以加快批准临床使用。虽然脑震荡仍然是一种临床诊断，但神经影像学技术正在提高我们对成人脑结构和功能后果的认识。儿科人群的神经影像学可能受到几个因素的限制，例如，脑震荡后纵向变化的测量由于动态的、未成熟的大脑的背景而变得复杂。没有成像技术被证实为脑震荡的诊断工具，成像结果与临床可测量的认知或行为功能之间的相关性是可变的。目前正在研究容积成像、 DTI 和功能磁共振成像(fMRI)等工具，特别是动脉自旋标记。通过 DTI 测量的分数各向异性(FA)可以推断白质束的结构完整性，TBI 后白质束通常被破坏。FA 变化的临床意义仍然存在争议，因为在脑震荡研究中观察到 FA 增加和减少[162,163,164,165,166]。这些差异可能部分是由于所检查的脑区域的相当大的空间异质性[167]以及损伤后间隔的差异。FA 可能仍然具有预后价值，有证据表明变化的方向和幅度与临床结果相关; 然而，这个想法等待在儿科和成人人群中验证。FA 可能缺乏必要的敏感性来充分评估脑损伤后白质束完整性的变化，扩散率的测量可能更合适。169 DTI 领域将大大受益于规范数据集的开发，以衡量观察到的变化。年轻运动员的赛前、赛后和赛季研究可以采用连续 DTI 成像技术为特定个体建立规范的数据，但数据汇总后的效用尚不清楚。儿科标准数据的缺乏严重限制了包括 DTI 在内的神经影像技术的临床应用。儿童脑震荡后的“回归基线”神经影像学研究也是必要的，因为它们可以极大地改善恢复的预测。尽管自动化提高了重复性，但 DTI 测量仍然对硬件和软件特异性，采集参数和分析软件敏感，这限制了重复性，标准化和中心之间以及跨研究之间的比较。标准化 DTI 成像中心的努力正在进行中。170 MRI 在绘制大脑的“连接体”(结构和功能神经连接网络及其各自的焦点节点的集合)以及研究脑震荡如何影响这些网络方面特别成功。局灶性或弥漫性 TBI 可以破坏大脑的功能连接，导致多个网络的功能障碍，包括默认模式和显着网络，这与记忆，情绪和情绪有关[171]。网络功能障碍对恢复的影响可能比病变部位更强[171,172,173] ，但对大脑发育和认知功能的长期影响尚不清楚[26,174]。脑震荡后儿童网络连接功能障碍的进一步研究对于改善损伤预后和管理至关重要。用于 PET 成像的放射性示踪剂有可能推进脑震荡和 CTE 的诊断和治疗，但目前它们在儿科人群中的应用纯粹是研究性的。三种 FDA 批准的放射性标记成像剂目前可用于检测疑似 AD 患者的脑淀粉样蛋白。175在成年人中，一些脑震荡病例与急性 Aβ 病理有关。PET 扫描可以使儿科患者监测急性脑震荡后淀粉样蛋白的存在和持续性，并确定扫描阳性和阴性是否预测不同的结果.176,177在儿科人群中具有潜在用途的其他 PET 成像剂包括结合由 tau 组成的神经原纤维缠结的新示踪剂。用18F-T807,18F-T808和18F-THK5105进行的早期成像结果证明对于确认包括 AD 在内的各种临床情况下存在共病是有用的。178,179,180在最近的一项 AD 研究中，tau 示踪信号的大小与疾病的分期和认知障碍的严重程度呈正相关。第三种 tau PET 示踪剂11C-PBB3已经在健康个体和 AD 患者中进行了测试，并且可能能够检测 tau 的非 AD 构象。181此外，最近的一份报告首次描述了疑似与运动相关的慢性创伤性脑病(CTE)在活人中的重病影像学表现。鉴于脑震荡，重复性亚震荡损伤和 CTE 中慢性 tau 病理学的程度，tau 示踪剂可用作诊断和预后生物标志物(例如，区分 CNI 和 CTE)。目前正在对 CTE 成人进行这些示踪剂的研究，但它们在儿科人群中的应用将取决于未来的研究，以确定 TBI 或脑震荡后年轻患者是否存在 tau 病理学。小胶质细胞胆固醇转运蛋白的 PET 示踪剂可能有助于成像与创伤性脑损伤相关的神经炎症。182正在开发的新型 PET 配体可以成像脑小胶质细胞，对神经退行性疾病具有潜在的应用价值，也可能证明对脑震荡和慢性创伤性脑病的治疗有用。在脑震荡和 TBI 的儿科人群中探索这些 PET 配体将是有益的，但是在开始进行涉及该年龄组放射性示踪剂的研究之前必须进行风险-效益分析。任何 PET 成像剂的最终效用将取决于其作为多模式生物标志物和神经影像技术小组的一部分的诊断和预后价值。非侵入性技术如经颅磁力刺激(tMS)也发现了创伤性脑损伤和脑震荡后突触可塑性的变化，特别是在无症状的个体中。对20多岁有脑震荡史的年轻运动员进行的几项小型 TMS 研究表明，运动皮层中 γ-氨基丁酸和/或谷氨酸神经传导的不平衡与突触长时程增强作用和抑郁症的缺陷有关。184,185,187,188经颅磁刺激还显示，脑震荡相关的突触可塑性损伤可以损害运动学习的各个方面，这些缺陷在个体最后一次脑震荡几十年后仍然可以检测到。另一个检测与脑震荡相关的神经化学功能障碍的关键非侵入性工具是质子磁共振谱(MRS)。专门针对运动相关脑震荡后使用光谱学的报告表明，与神经化学改变一致的各种异常。在年轻(高中)运动员中，MRS 在赛季后与赛季前评估中检测到谷氨酸和谷氨酰胺水平增加，即使在赛季期间没有经历临床显着脑震荡的运动员中也是如此。这些发现表明，即使是次震荡性头部撞击也可能导致谷氨酸途径的激活，意味着细胞损伤或神经元死亡，尽管没有症状。在上述研究中，一部分参与者的肌酐和肌醇水平(位于星形胶质细胞中的有机渗透液192,193)也发生了显著变化。在一项使用 MRS 的罕见追踪研究中，194名持续单次运动相关脑震荡的个体在受伤后3天在大脑中表现出显着降低的 N- 乙酰天冬氨酸(NAA，神经元和轴突健康，完整性和功能的标志物195)水平。损伤后15天水平升高，损伤后30天恢复到对照值。相比之下，在第一次脑震荡后10-13天再次受到脑震荡的参与者表现出 NAA 水平的长时间下降，即使在受伤后45天也没有恢复正常。这些结果表明，在短时间内反复受伤增加了延长或不完全恢复的可能性。除了 MRS 检测到的急性和亚急性改变之外，其他关于脑震荡长期影响的研究已经揭示了在其他健康的前运动员中，内侧颞叶中肌醇(与胶质增殖相关)增加和胆碱(与膜转换相关195)水平降低在测试之前持续最后一次脑震荡超过三十年。196最近的另一项研究使用一种叫做相关光谱学(COSY)的先进的 MRS 方法，检测了一组有症状的退役国家橄榄球联盟球员，这种方法可以测量额外的代谢物。作者发现胆碱和谷氨酸-谷氨酰胺水平升高(分别表明弥漫性轴突损伤和兴奋性毒性) ，与之前的 mtBI MRS 研究一致，以及额外的大脑代谢物表明神经炎症的变化。这些新陈代谢的变化可能提供了损伤机制的洞察力，如兴奋性毒性和/或炎症，这可能是所报道的结构变化的基础。总的来说，现有的数据支持使用 MRS 作为一种研究工具，以确定改变的神经生理学和监测恢复成年运动员，即使在解决后脑震荡症状。目前，MRS 检测到的生化改变可以增强我们对潜在病理生理学的理解，但尚不能提供具体的诊断信息。需要更大的横断面，前瞻性和纵向研究来确定 MRS 在运动相关脑震荡领域内的敏感性和预后价值.190由于未成熟大脑中 MRS 的解释需要某些发育方面的考虑，因此将来在儿童中的工作将需要适当的比较样本。具有更高光谱分辨率的 MRS 技术，包括 COSY，可能提供额外的生化特异性。空间分辨率的其他进展，如3D 化学位移成像，也可以通过允许调查整个大脑的代谢改变而不是在特定的感兴趣的区域，提供更大的特异性。最后，MRS 可以在测量治疗效果方面发挥作用，例如经颅直流电刺激198和 TMS.199。体育相关伤害测量，报告，跟踪和数据共享的机制和监测基础设施不足以满足目前的需求和目标。脑震荡的研究和临床工作受到缺乏运动和运动水平的脑震荡数据的阻碍。2014年美国医学研究所的一份报告只确定了三个国家运动伤害监测系统: 国家电子伤害监测系统ーー所有伤害项目(NEISS-AIP)、全美大学体育协会伤害监测系统(NCAA ISS)和高中伤害在线报告系统(rIOTM)。1这些系统可以补充临床数据(例如，来自急诊科、住院病人和体育诊所) ，但这些数据偏向于更严重的伤害和社会经济地位更高的病人。事实上，农村地区或社会经济地位较低的社区的学校往往很难获得运动医疗专业人员和设施。一些新出现的项目可能会改善监督。区域性的努力，如运动训练员临床结果研究教育(CORE-AT)和全国性的努力，如全国运动训练员协会全国运动治疗，伤害和结果网络(NATA NATIONTM)试图将伤害跟踪与高中和大学水平的治疗和结果数据结合起来。然而，这些系统中没有一个专门针对年轻运动员、那些参加非学校赞助体育项目的运动员或那些在没有运动教练的学校的运动员。运动损伤数据库也很少考虑人口统计因素，包括社会经济地位、种族或民族以及医疗保健覆盖率。目前，还没有有效的机制来连贯和廉价地将各种监测数据集联系起来，或者跨越体育、跟踪系统或年龄连续体跟踪个别运动员。现在相当需要一个系统来追踪个人运动员的运动生涯和其他方面。应该对每个人进行数十年的跟踪，以确定 TBI 的负担是否、何时以及如何演变为 CTE，并评估与脑震荡相关的所有可能的负面健康结果。这种系统还可以更准确地描述脑震荡病史和风险因素，并可以捕捉短期和长期的结果，包括身体和心理健康、学业和职业成功、生活质量和社会联系以及不断变化的社会经济地位。这种努力受到各种问题的挑战，包括缺乏任何级别的脑震荡强制性报告。强制性脑震荡报告、为监测工作提供资金以及为数据记者(例如教练和运动员培训员)提供培训将极大地改善流行病学研究。然而，如果没有经过验证的、对脑震荡的共识定义，以及通用数据库和全球唯一标识符(GUID)系统的开发，强制性报告将无法提供有意义的结果。然后可以将标准化监测工作的数据集联系起来，从而改善研究和临床护理的数据共享。将监测数据与组织和液体样本生物库的标准化收集、储存和管理基础设施耦合起来，可以大大改善损伤和结果研究。200这些努力可以通过公私伙伴关系的资金来催化，并通过制定现实的短期和长期目标来实现，以创建一个多年计划。然而，至少在美国，这些努力目前受到对健康保险便利和责任法案(HIPAA)规定的误解和对运动员保密的普遍关注的阻碍。运动员更广泛地使用计算机神经认知测试(CNT)可以改善脑震荡的监测，以及诊断和管理。然而，在 CNT 成为常规手术之前，必须克服几个重要的挑战。这些挑战包括缺乏标准化的管理协议，不同计算机硬件引起的技术错误的可能性，评估的认知功能类型的限制，以及缺乏合格的测试管理员和数据解释员.201尽管存在这些缺陷，但是，CNT 已经被大约40% 的美国高中雇用运动教练员.202虽然不是所有学校都负担得起，但是 CNT 可以加强地面数据收集，帮助风险暴露估计和脑震荡后恢复跟踪，以及提高向运动损伤监测网络报告的数据质量。CNT 也可能有助于评估和跟踪脑震荡后认知改善或下降，并可能有助于预测结果.203,204在学校环境中收集的 CNT 数据是否将达到由临床研究小组进行的 CNT 所达到的验证和重复性标准仍有待观察。重要的是，CNT 需要标准化和指导方针，以确定“返回运动”和“返回学习”的运动员在一个领域表现出恢复，但在其他领域仍然有症状。在临床和青少年运动员脑震荡监测和管理方面，需要对 CNT 的应用进行更多的研究。在一些关键领域，不完整的知识阻碍了儿科脑震荡领域有意义的进展。在分子和细胞水平上，迫切需要重点研究脑震荡和重复性亚震荡损伤后的轴突损伤，以阐明轴突运输和修复的变化，并更好地定义瞬时 Aβ 积累作为下游和/或未来病理学的潜在驱动因素的作用。脑震荡研究人员可能需要确定更合适的动物模型来研究分子病理学，包括 tau 蛋白及其对脑震荡后和慢性创伤脑炎病理学的贡献，因为啮齿动物和人类的大脑结构和组织大不相同。如果不能更清楚地了解创伤性脑损伤如何改变年轻、仍在发育中的大脑，以及在损伤后的数周、数月和数年内会发生什么样的病理事件，我们就只能推测这种改变的潜在生物学基础。通过使用记录线性和旋转力的传感器技术，可以改进青年体育运动中头部影响数据的收集和风险评估。这种商业上可用的设备，如果经过验证，可以确定在比赛期间和整个比赛季节中头部累积冲击力的水平，并且研究结果可以与神经影像学数据和功能结果评估联系起来。结合“击中计数”指标，传感器数据可以提高对重复性次生震荡影响的短期和长期神经心理学结果的认识。我们对慢性创伤性脑病的认识可以通过了解一般人群、受伤运动员、运动和运动位置匹配的未受伤运动员以及低风险运动中的“控制”运动员的基线率来改善。提高对风险暴露的认识可导致预防努力，包括改变做法和竞争规则。一项长达数十年的前瞻性追踪研究，追踪青年运动员的运动生涯及以后的发展，将提供有关累积性头部撞击以及长期神经心理功能障碍和痴呆风险的更确切知识。这样的研究正在 NCAA 校友中进行，他们于2003年首次接受研究，并于2013年重新评估。其他人群的研究，特别是如果 NIH 资助的话，可能会从5年的研究开始，可以进一步延长5年的增量。可能需要建立公私伙伴关系，以获得足够的资金，使多个研究中心参与进来。NCAA 已经为100多名运动员的10年重新评估提供了部分赞助，但需要来自 NIH，美国国防部(DoD)和私人慈善来源的进一步资助，以扩大评估范围，从神经心理学，通过 MRI，淀粉样蛋白，tau 和/或炎症的分子成像。理想情况下，追踪研究设计应结合流行病学和介入试验方法，并利用多个对照组，包括非接触运动员和未受伤的撞击运动员。追踪研究还将阐明认知储备的作用。老年痴呆症研究团体利用国家卫生研究院的资金以及涉及制药公司和基金会的公私伙伴关系，开创了这类研究的先例。为了使这类研究取得成功，必须首先建立更多的监测系统和数据库。如果参加影响力体育运动的运动员能够普遍获得运动员训练员的帮助，这些训练员能够在促进安全和提供基本护理的同时充当可靠的数据报告员，那么将加快努力。此外，任何纵向研究都必须包括死后分析，以便更好地了解儿童和青少年脑震荡对今后生活中神经退行性病理和痴呆发展的影响。由于缺乏严格的流行病学证据，“重返赛场”的指导方针目前受到阻碍，纵向研究的长期安全数据可能会大大改善这一点。纵向研究还可以包括确定那些未能遵循指导方针的运动员是否会经历任何负面健康影响的研究，例如持续的症状或改变发生第二次脑震荡的风险。长期前瞻性研究的基础设施可以通过建立一个以阿尔茨海默氏病神经影像学倡议(ADNI)为模型的研究联盟来创建。ADNI 为数据收集、传播协议、测试方法和生物标志物收集和分析制定了标准。目前正在国防部参与的一个版本的 ADNI (ADNI-DoD)专注于军事人群中与爆炸相关的 TBI 研究。2072014年5月，除了 NCAA 脑震荡研究，NCAA 和国防部宣布启动迄今为止最大的前瞻性运动相关脑震荡研究，该研究将在3年内监测大约37,000名 NCAA 运动员。我们可以想象，这项研究的基础设施可能最终扩展到研究年轻运动员在一个延长的纵向范围。我们对创伤性脑损伤的生物学知识仍然存在许多差距，这限制了我们开发有效药物的能力。如果我们要解决潜在的疾病病理，并超越治疗症状，就必须填补这些空白。然而，当基础创伤性脑损伤生物学的研究继续进行时，许多工作可以完成。药物再利用包括测试现有 FDA 批准的新适应症药物，可以减少费用和缩短药物批准的路径。目前的再利用试验包括哌醋甲酯治疗疼痛和精神疲劳，多巴胺受体激动剂溴隐亭治疗工作记忆，舍曲林治疗情绪和焦虑，这是最常见的影响脑震荡后长期结果的神经心理并发症。此外，黄体酮的 PROTECT III 期临床试验最近未能改善急性 TBI211后的结局，这可能提醒人们需要更多的研究来更好地理解 TBI 的基础生物学。虽然许多药物重新利用的努力主要是为了解决脑震荡症状，药物也可能影响损伤病理学和进展。对现有药物的研究也可能导致新的药物发现努力，并可能导致新的预防或管理治疗。急需新的药物治疗创伤性脑损伤和无法消除的脑震荡。在神经保护和抗炎领域的药物发现努力是特别相关的，因为它们潜在的交叉适用于神经退行性疾病，如 AD。同样，目前正在开发的治疗其他神经退行性疾病的药物可能会被重新定位，用于 TBI 或无脑震荡症状患者的检测。正如医学研究中经常出现的情况一样，脑震荡研究的最新进展提出的问题和回答的问题一样多。有证据表明脑震荡或重复性次生脑震荡后长期神经心理功能障碍和晚年痴呆，需要更多的工作来更好地理解青年参与影响性运动的含义和结果。正如本专家共识文件所概述的那样，有一条前进的道路，但实现这里概述的目标将需要公共和私营部门的合作。虽然可以通过增加知识来改进建议，但现有证据仍然可以在考虑青年参与体育运动以及实践政策和竞赛规则时为个人决策提供信息。随着人口老龄化和痴呆症的流行，我们必须更多地了解潜在的早期生活风险因素，包括与运动有关的脑震荡。家长、教练、学校董事会和孩子们做出的选择将在脑震荡科学知识的关键差距得到填补时得到更好的信息。下载参考资料|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+Design+of+Performant+Recommender+Systems+using+Large-scale+Linear+Programming-based+Global+Inference)|0|
|[Rank-heterogeneous Preference Models for School Choice](https://doi.org/10.1145/3580305.3599484)|Amel Awadelkarim, Arjun Seshadri, Itai Ashlagi, Irene Lo, Johan Ugander|Stanford University; Amazon|School choice mechanism designers use discrete choice models to understand and predict families' preferences. The most widely-used choice model, the multinomial logit (MNL), is linear in school and/or household attributes. While the model is simple and interpretable, it assumes the ranked preference lists arise from a choice process that is uniform throughout the ranking, from top to bottom. In this work, we introduce two strategies for rank-heterogeneous choice modeling tailored for school choice. First, we adapt a context-dependent random utility model (CDM), considering down-rank choices as occurring in the context of earlier up-rank choices. Second, we consider stratifying the choice modeling by rank, regularizing rank-adjacent models towards one another when appropriate. Using data on household preferences from the San Francisco Unified School District (SFUSD) across multiple years, we show that the contextual models considerably improve our out-of-sample evaluation metrics across all rank positions over the non-contextual models in the literature. Meanwhile, stratifying the model by rank can yield more accurate first-choice predictions while down-rank predictions are relatively unimproved. These models provide performance upgrades that school choice researchers can adopt to improve predictions and counterfactual analyses.|学校选择机制的设计者使用离散选择模型来理解和预测家庭的偏好。最广泛使用的选择模型，多项式 logit (MNL) ，在学校和/或家庭属性中是线性的。虽然这个模型是简单和可解释的，但是它假设排名的偏好列表来自于一个从上到下在整个排名过程中是统一的选择过程。本文介绍了两种适用于学校选择的秩异质选择模型的建模策略。首先，我们采用了一个上下文相关的随机效用模型(CDM) ，考虑了在早期上层选择的情况下发生的下层选择。其次，我们考虑根据等级对选择模型进行分层，在适当的时候将相邻等级的模型相互调整。使用来自旧金山联合校区多年的家庭偏好数据，我们发现相对于文献中的非上下文模型，上下文模型大大提高了我们在所有排名位置的外部评估指标。同时，按等级对模型进行分层可以得到更准确的第一选择预测，而低等级预测相对来说没有改进。这些模型提供了学校选择研究人员可以用来改进预测和反事实分析的绩效提升。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rank-heterogeneous+Preference+Models+for+School+Choice)|0|
|[Connecting the Dots - Density-Connectivity Distance unifies DBSCAN, k-Center and Spectral Clustering](https://doi.org/10.1145/3580305.3599283)|Anna Beer, Andrew Draganov, Ellen Hohma, Philipp Jahn, Christian M. M. Frey, Ira Assent||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Connecting+the+Dots+-+Density-Connectivity+Distance+unifies+DBSCAN,+k-Center+and+Spectral+Clustering)|0|
|[Shilling Black-box Review-based Recommender Systems through Fake Review Generation](https://doi.org/10.1145/3580305.3599502)|HungYun Chiang, YiSyuan Chen, YunZhu Song, HongHan Shuai, Jason S. Chang|National Tsing Hua University; National Yang Ming Chiao Tung University|Review-Based Recommender Systems (RBRS) have attracted increasing research interest due to their ability to alleviate well-known cold-start problems. RBRS utilizes reviews to construct the user and items representations. However, in this paper, we argue that such a reliance on reviews may instead expose systems to the risk of being shilled. To explore this possibility, in this paper, we propose the first generation-based model for shilling attacks against RBRSs. Specifically, we learn a fake review generator through reinforcement learning, which maliciously promotes items by forcing prediction shifts after adding generated reviews to the system. By introducing the auxiliary rewards to increase text fluency and diversity with the aid of pre-trained language models and aspect predictors, the generated reviews can be effective for shilling with high fidelity. Experimental results demonstrate that the proposed framework can successfully attack three different kinds of RBRSs on the Amazon corpus with three domains and Yelp corpus. Furthermore, human studies also show that the generated reviews are fluent and informative. Finally, equipped with Attack Review Generators (ARGs), RBRSs with adversarial training are much more robust to malicious reviews.|基于评论的推荐系统(RBRS)由于其缓解众所周知的冷启动问题的能力而引起了越来越多的研究兴趣。RBRS 利用评论来构建用户和项目表示。然而，在本文中，我们认为，这种对审查的依赖反而可能使系统面临被托儿的风险。为了探索这种可能性，本文提出了第一代基于先令攻击的 RBRS 模型。具体来说，我们通过强化学习学习一个虚假的评论生成器，它在向系统添加生成的评论之后，通过强制预测变化来恶意推销项目。通过引入辅助奖励，以提高文本流畅性和多样性的帮助下，预先训练的语言模型和方面预测，生成的评论可以有效的先令与高保真度。实验结果表明，该框架能够成功地利用三个域和 Yelp 语料库对亚马逊语料库中的三种不同类型的 RBRS 进行攻击。此外，人类研究也表明，生成的评论是流畅和信息。最后，配备了攻击评论生成器(ARGs) ，具有对抗性训练的 RBRS 对恶意评论更加有力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shilling+Black-box+Review-based+Recommender+Systems+through+Fake+Review+Generation)|0|
|[Below the Surface: Summarizing Event Sequences with Generalized Sequential Patterns](https://doi.org/10.1145/3580305.3599264)|Joscha Cüppers, Jilles Vreeken||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Below+the+Surface:+Summarizing+Event+Sequences+with+Generalized+Sequential+Patterns)|0|
|[Generalized Matrix Local Low Rank Representation by Random Projection and Submatrix Propagation](https://doi.org/10.1145/3580305.3599361)|Pengtao Dang, Haiqi Zhu, Tingbo Guo, Changlin Wan, Tong Zhao, Paul Salama, Yijie Wang, Sha Cao, Chi Zhang|; Amazon; Indiana University, School of Medicine; Indiana University, Bloomington; Genentech; Purdue University; Indiana University|Detecting distinct submatrices of low rank property is a highly desirable matrix representation learning technique for the ease of data interpretation, called the matrix local low rank representation (MLLRR). Based on different mathematical assumptions of the local pattern, the MLLRR problem could be categorized into two sub-problems, namely local constant variation (LCV) and local linear low rank (LLR). Existing solutions on MLLRR only focused on the LCV problem, which misses a substantial amount of true and interesting patterns. In this work, we develop a novel matrix computational framework called RPSP (Random Probing based submatrix Propagation) that provides an effective solution for both of the LCV and LLR problems. RPSP detects local low rank patterns that grow from small submatrices of low rank property, which are determined by a random projection approach. RPSP is supported by theories of random projection. Experiments on synthetic data demonstrate that RPSP outperforms all state-of-the-art methods, with the capacity to robustly and correctly identify the low rank matrices under both LCV and LLR settings. On real-world datasets, RPSP also demonstrates its effectiveness in identifying interpretable local low rank matrices.|矩阵局部低秩表示(MLLRR)是一种非常理想的矩阵表示学习技术，它可以检测出具有低秩性质的不同子矩阵。根据对局部模式的不同数学假设，MLLRR 问题可以分为局部常变(LCV)和局部线性低秩(LLR)两个子问题。MLLRR 上的现有解决方案只关注 LCV 问题，而这个问题忽略了大量真实而有趣的模式。在这项工作中，我们开发了一个新的矩阵计算框架称为 RPSP (随机探测为基础的子矩阵传播) ，提供了一个有效的解决方案，这两个 LCV 和 LLR 问题。RPSP 检测由低秩性质的小子矩阵生成的局部低秩模式，这些小子矩阵由随机投影方法确定。RPSP 得到了随机投影理论的支持。对合成数据的实验表明，RPSP 算法优于所有的最新方法，在 LCV 和 LLR 设置下都具有鲁棒性和正确识别低秩矩阵的能力。在实际数据集上，RPSP 也证明了其识别可解释的局部低秩矩阵的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generalized+Matrix+Local+Low+Rank+Representation+by+Random+Projection+and+Submatrix+Propagation)|0|
|[TWIN: Personalized Clinical Trial Digital Twin Generation](https://doi.org/10.1145/3580305.3599534)|Trisha Das, Zifeng Wang, Jimeng Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TWIN:+Personalized+Clinical+Trial+Digital+Twin+Generation)|0|
|[Accelerating Dynamic Network Embedding with Billions of Parameter Updates to Milliseconds](https://doi.org/10.1145/3580305.3599250)|Haoran Deng, Yang Yang, Jiahe Li, Haoyang Cai, Shiliang Pu, Weihao Jiang|Carnegie Mellon University; Hikvision Research Institute; Zhejiang University|Network embedding, a graph representation learning method illustrating network topology by mapping nodes into lower-dimension vectors, is challenging to accommodate the ever-changing dynamic graphs in practice. Existing research is mainly based on node-by-node embedding modifications, which falls into the dilemma of efficient calculation and accuracy. Observing that the embedding dimensions are usually much smaller than the number of nodes, we break this dilemma with a novel dynamic network embedding paradigm that rotates and scales the axes of embedding space instead of a node-by-node update. Specifically, we propose the Dynamic Adjacency Matrix Factorization (DAMF) algorithm, which achieves an efficient and accurate dynamic network embedding by rotating and scaling the coordinate system where the network embedding resides with no more than the number of edge modifications changes of node embeddings. Moreover, a dynamic Personalized PageRank is applied to the obtained network embeddings to enhance node embeddings and capture higher-order neighbor information dynamically. Experiments of node classification, link prediction, and graph reconstruction on different-sized dynamic graphs suggest that DAMF advances dynamic network embedding. Further, we unprecedentedly expand dynamic network embedding experiments to billion-edge graphs, where DAMF updates billion-level parameters in less than 10ms.|网络嵌入是一种通过将节点映射为低维向量来表示网络拓扑的图形表示学习方法，在实际应用中很难适应不断变化的动态图形。现有的研究主要是基于逐个节点的嵌入修改，这种方法陷入了计算效率和精度的两难境地。针对嵌入维数通常远小于节点数的问题，提出了一种新的动态网络嵌入方法，该方法不需要逐个节点更新，而是通过对嵌入空间的轴线进行旋转和缩放来解决这一问题。具体来说，我们提出了动态邻接矩阵分解(dAMF)算法，该算法通过旋转和缩放网络嵌入所在的坐标系，在不超过节点嵌入的边修改变化量的情况下，实现了一个高效、准确的动态网络嵌入。此外，将动态个性化 PageRank 应用于所获得的网络嵌入，以增强节点的嵌入，并动态捕获高阶邻居信息。对不同大小的动态图进行节点分类、链路预测和图重构的实验表明，DAMF 推进了动态网络嵌入。进一步，我们前所未有地将动态网络嵌入实验扩展到十亿边图，其中 DAMF 在不到10ms 的时间内更新十亿级参数。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Accelerating+Dynamic+Network+Embedding+with+Billions+of+Parameter+Updates+to+Milliseconds)|0|
|[MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text Classification](https://doi.org/10.1145/3580305.3599430)|Hongyuan Dong, Weinan Zhang, Wanxiang Che|Harbin Institute of Technology|Prompting methods have shown impressive performance in a variety of text mining tasks and applications, especially few-shot ones. Despite the promising prospects, the performance of prompting model largely depends on the design of prompt template and verbalizer. In this work, we propose MetricPrompt, which eases verbalizer design difficulty by reformulating few-shot text classification task into text pair relevance estimation task. MetricPrompt adopts prompting model as the relevance metric, further bridging the gap between Pre-trained Language Model's (PLM) pre-training objective and text classification task, making possible PLM's smooth adaption. Taking a training sample and a query one simultaneously, MetricPrompt captures cross-sample relevance information for accurate relevance estimation. We conduct experiments on three widely used text classification datasets across four few-shot settings. Results show that MetricPrompt outperforms manual verbalizer and other automatic verbalizer design methods across all few-shot settings, achieving new state-of-the-art (SOTA) performance.|提示方法已经在各种文本挖掘任务和应用程序中显示出了令人印象深刻的性能，特别是那些很少使用的方法。尽管激励模式前景广阔，但其性能在很大程度上取决于激励模板和语言表达器的设计。在这项工作中，我们提出了 MetricPrompt，它通过将少镜头文本分类任务重构为文本对相关性估计任务，从而减轻了语言表达器的设计难度。MetricPrompt 采用提示模型作为相关度量，进一步缩小了预训练语言模型(Pre-training Language Model，PLM)的预训练目标与文本分类任务之间的差距，使得 PLM 的顺利适应成为可能。同时采用训练样本和查询样本，MetricPrompt 捕获跨样本的相关性信息以进行准确的相关性估计。我们在三个广泛使用的文本分类数据集上通过四个少镜头设置进行实验。结果表明，MetricPrompt 在所有短镜头设置中都优于手动语音表达器和其他自动语音表达器设计方法，实现了新的最新(SOTA)性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetricPrompt:+Prompting+Model+as+a+Relevance+Metric+for+Few-shot+Text+Classification)|0|
|[Delving into Global Dialogue Structures: Structure Planning Augmented Response Selection for Multi-turn Conversations](https://doi.org/10.1145/3580305.3599304)|Tingchen Fu, Xueliang Zhao, Rui Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Delving+into+Global+Dialogue+Structures:+Structure+Planning+Augmented+Response+Selection+for+Multi-turn+Conversations)|0|
|[Partial-label Learning with Mixed Closed-set and Open-set Out-of-candidate Examples](https://doi.org/10.1145/3580305.3599460)|Shuo He, Lei Feng, Guowu Yang|University of Electronic Science and Technology of China; Nanyang Technological University|Partial-label learning (PLL) relies on a key assumption that the true label of each training example must be in the candidate label set. This restrictive assumption may be violated in complex real-world scenarios, and thus the true label of some collected examples could be unexpectedly outside the assigned candidate label set. In this paper, we term the examples whose true label is outside the candidate label set OOC (out-of-candidate) examples, and pioneer a new PLL study to learn with OOC examples. We consider two types of OOC examples in reality, i.e., the closed-set/open-set OOC examples whose true label is inside/outside the known label space. To solve this new PLL problem, we first calculate the wooden cross-entropy loss from candidate and non-candidate labels respectively, and dynamically differentiate the two types of OOC examples based on specially designed criteria. Then, for closed-set OOC examples, we conduct reversed label disambiguation in the non-candidate label set; for open-set OOC examples, we leverage them for training by utilizing an effective regularization strategy that dynamically assigns random candidate labels from the candidate label set. In this way, the two types of OOC examples can be differentiated and further leveraged for model training. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art PLL methods.|部分标签学习(PLL)依赖于一个关键的假设，即每个训练样本的真实标签必须在候选标签集中。在复杂的现实场景中，这种限制性假设可能会被违反，因此，一些收集的示例的真实标签可能意外地位于分配的候选标签集之外。在本文中，我们将真实标签在候选标签集外的例子称为候选标签集外的例子，并且开创了一种新的 PLL 研究方法来学习候选标签集外的例子。我们在实际中考虑两种类型的 OOC 示例，即闭集/开集 OOC 示例，它们的真实标签位于已知标签空间的内部或外部。为了解决这个新的锁相环问题，我们首先分别计算候选标签和非候选标签的木质交叉熵损失，并根据特定的准则动态区分两种类型的 OOC 实例。然后，对于闭集 OOC 例子，我们在非候选标签集中进行反向标签消歧; 对于开集 OOC 例子，我们利用它们进行训练，利用一种有效的正则化策略，从候选标签集中动态分配随机候选标签。通过这种方式，两种类型的 OOC 示例可以区分并进一步用于模型培训。大量的实验表明，我们提出的方法优于最先进的锁相环方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Partial-label+Learning+with+Mixed+Closed-set+and+Open-set+Out-of-candidate+Examples)|0|
|[COMET: Learning Cardinality Constrained Mixture of Experts with Trees and Local Search](https://doi.org/10.1145/3580305.3599278)|Shibal Ibrahim, Wenyu Chen, Hussein Hazimeh, Natalia Ponomareva, Zhe Zhao, Rahul Mazumder|Google DeepMind; Massachusetts Institute of Technology; Google Research|The sparse Mixture-of-Experts (Sparse-MoE) framework efficiently scales up model capacity in various domains, such as natural language processing and vision. Sparse-MoEs select a subset of the "experts" (thus, only a portion of the overall network) for each input sample using a sparse, trainable gate. Existing sparse gates are prone to convergence and performance issues when training with first-order optimization methods. In this paper, we introduce two improvements to current MoE approaches. First, we propose a new sparse gate: COMET, which relies on a novel tree-based mechanism. COMET is differentiable, can exploit sparsity to speed up computation, and outperforms state-of-the-art gates. Second, due to the challenging combinatorial nature of sparse expert selection, first-order methods are typically prone to low-quality solutions. To deal with this challenge, we propose a novel, permutation-based local search method that can complement first-order methods in training any sparse gate, e.g., Hash routing, Top-k, DSelect-k, and COMET. We show that local search can help networks escape bad initializations or solutions. We performed large-scale experiments on various domains, including recommender systems, vision, and natural language processing. On standard vision and recommender systems benchmarks, COMET+ (COMET with local search) achieves up to 13% improvement in ROC AUC over popular gates, e.g., Hash routing and Top-k, and up to 9% over prior differentiable gates e.g., DSelect-k. When Top-k and Hash gates are combined with local search, we see up to $100\times$ reduction in the budget needed for hyperparameter tuning. Moreover, for language modeling, our approach improves over the state-of-the-art MoEBERT model for distilling BERT on 5/7 GLUE benchmarks as well as SQuAD dataset.|稀疏混合专家(Sparse-MoE)框架有效地扩展了各种领域的模型容量，例如自然语言处理和视觉。稀疏-MoEs 使用稀疏的、可训练的门为每个输入样本选择一个“专家”子集(因此，只是整个网络的一部分)。现有的稀疏门在用一阶优化方法进行训练时容易出现收敛和性能问题。在本文中，我们介绍了两个改进的目前的教育方法。首先，我们提出了一种新的稀疏门: COMET，它依赖于一种新的基于树的机制。COMET 是可微的，可以利用稀疏性来加速计算，并且性能优于最先进的门。其次，由于稀疏专家选择具有挑战性的组合性质，一阶方法通常倾向于低质量的解决方案。为了应对这一挑战，我们提出了一种新颖的基于置换的局部搜索方法，可以补充一阶方法训练任何稀疏门，例如，散列路由，Top-k，DSelect-k 和 COMET。我们展示了本地搜索可以帮助网络逃避糟糕的初始化或解决方案。我们在不同的领域进行了大规模的实验，包括推荐系统、视觉和自然语言处理。在标准愿景和推荐系统基准上，COMET + (本地搜索的 COMET)在 ROC AUC 比流行的门(如散列路由和 Top-k)提高了13% ，比以前的可微分门(如 DSelect-k)提高了9% 。当 Top-k 和 Hash 门与本地搜索相结合时，我们看到超参数调优所需的预算减少了100倍。此外，对于语言建模，我们的方法改进了最先进的 MoEBERT 模型，用于提取5/7 GLUE 基准测试和 SQuAD 数据集上的 BERT。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=COMET:+Learning+Cardinality+Constrained+Mixture+of+Experts+with+Trees+and+Local+Search)|0|
|[Exploiting Relation-aware Attribute Representation Learning in Knowledge Graph Embedding for Numerical Reasoning](https://doi.org/10.1145/3580305.3599338)|Gayeong Kim, Sookyung Kim, Ko Keun Kim, Suchan Park, Heesoo Jung, Hogun Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Relation-aware+Attribute+Representation+Learning+in+Knowledge+Graph+Embedding+for+Numerical+Reasoning)|0|
|[Efficient Distributed Approximate k-Nearest Neighbor Graph Construction by Multiway Random Division Forest](https://doi.org/10.1145/3580305.3599327)|SangHong Kim, HaMyung Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Distributed+Approximate+k-Nearest+Neighbor+Graph+Construction+by+Multiway+Random+Division+Forest)|0|
|[MM-DAG: Multi-task DAG Learning for Multi-modal Data - with Application for Traffic Congestion Analysis](https://doi.org/10.1145/3580305.3599436)|Tian Lan, Ziyue Li, Zhishuai Li, Lei Bai, Man Li, Fugee Tsung, Wolfgang Ketter, Rui Zhao, Chen Zhang|SenseTime Research; University of Cologne; The Hong Kong University of Science and Technology; Shanghai AI Laboratory; Tsinghua University; The Hong Kong University of Science and Technology (Guangzhou)|This paper proposes to learn Multi-task, Multi-modal Direct Acyclic Graphs (MM-DAGs), which are commonly observed in complex systems, e.g., traffic, manufacturing, and weather systems, whose variables are multi-modal with scalars, vectors, and functions. This paper takes the traffic congestion analysis as a concrete case, where a traffic intersection is usually regarded as a DAG. In a road network of multiple intersections, different intersections can only have some overlapping and distinct variables observed. For example, a signalized intersection has traffic light-related variables, whereas unsignalized ones do not. This encourages the multi-task design: with each DAG as a task, the MM-DAG tries to learn the multiple DAGs jointly so that their consensus and consistency are maximized. To this end, we innovatively propose a multi-modal regression for linear causal relationship description of different variables. Then we develop a novel Causality Difference (CD) measure and its differentiable approximator. Compared with existing SOTA measures, CD can penalize the causal structural difference among DAGs with distinct nodes and can better consider the uncertainty of causal orders. We rigidly prove our design's topological interpretation and consistency properties. We conduct thorough simulations and one case study to show the effectiveness of our MM-DAG. The code is available under https://github.com/Lantian72/MM-DAG|本文提出学习多任务、多模态直接无环图(MM-DAGs) ，这是在交通、制造、天气等复杂系统中常见的图形，其变量是多模态的，包括标量、向量和函数。本文以交通堵塞分析作为一个具体案例，其中交通十字路口通常被视为一个 DAG。在一个多交叉口的道路网络中，不同的交叉口只能观察到一些重叠的、不同的变量。例如，信号交叉口有与交通灯相关的变量，而无信号交叉口没有。这鼓励了多任务设计: 将每个 DAG 作为一个任务，MM-DAG 试图联合学习多个 DAG，以便最大化它们的一致性和一致性。为此，我们创新性地提出了一种多模态回归方法来描述不同变量之间的线性因果关系。然后我们发展了一个新的因果差分(CD)测度及其可微逼近器。与现有的 SOTA 方法相比，CD 方法能够更好地考虑因果顺序的不确定性，并且能够惩罚具有不同节点的 DAGs 之间的因果结构差异。我们严格证明了我们的设计的拓扑解释和一致性性质。我们进行了彻底的模拟和一个案例研究，以显示我们的 MM-DAG 的有效性。代码可在 https://github.com/lantian72/mm-dag 下查阅|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MM-DAG:+Multi-task+DAG+Learning+for+Multi-modal+Data+-+with+Application+for+Traffic+Congestion+Analysis)|0|
|[Who Should Be Given Incentives? Counterfactual Optimal Treatment Regimes Learning for Recommendation](https://doi.org/10.1145/3580305.3599550)|Haoxuan Li, Chunyuan Zheng, Peng Wu, Kun Kuang, Yue Liu, Peng Cui||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Who+Should+Be+Given+Incentives?+Counterfactual+Optimal+Treatment+Regimes+Learning+for+Recommendation)|0|
|[UCEpic: Unifying Aspect Planning and Lexical Constraints for Generating Explanations in Recommendation](https://doi.org/10.1145/3580305.3599535)|Jiacheng Li, Zhankui He, Jingbo Shang, Julian J. McAuley||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UCEpic:+Unifying+Aspect+Planning+and+Lexical+Constraints+for+Generating+Explanations+in+Recommendation)|0|
|[Learning-Based Ad Auction Design with Externalities: The Framework and A Matching-Based Approach](https://doi.org/10.1145/3580305.3599403)|Ningyuan Li, Yunxuan Ma, Yang Zhao, Zhijian Duan, Yurong Chen, Zhilin Zhang, Jian Xu, Bo Zheng, Xiaotie Deng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning-Based+Ad+Auction+Design+with+Externalities:+The+Framework+and+A+Matching-Based+Approach)|0|
|[Communication Efficient Distributed Newton Method with Fast Convergence Rates](https://doi.org/10.1145/3580305.3599280)|Chengchang Liu, Lesi Chen, Luo Luo, John C. S. Lui|Fudan University; Chinese University of Hong Kong; The Chinese University of Hong Kong|We propose a communication and computation efficient second-order method for distributed optimization. For each iteration, our method only requires $\mathcal{O}(d)$ communication complexity, where $d$ is the problem dimension. We also provide theoretical analysis to show the proposed method has the similar convergence rate as the classical second-order optimization algorithms. Concretely, our method can find~$\big(\epsilon, \sqrt{dL\epsilon}\,\big)$-second-order stationary points for nonconvex problem by $\mathcal{O}\big(\sqrt{dL}\,\epsilon^{-3/2}\big)$ iterations, where $L$ is the Lipschitz constant of Hessian. Moreover, it enjoys a local superlinear convergence under the strongly-convex assumption. Experiments on both convex and nonconvex problems show that our proposed method performs significantly better than baselines.|提出了一种分布式优化的通信和计算有效的二阶方法。对于每个迭代，我们的方法只需要 $mathcal { O }(d) $通信复杂性，其中 $d $是问题维度。理论分析表明，该方法与经典的二阶优化算法具有相似的收敛速度。具体地说，我们的方法可以通过数学上的{ O } big (sqrt { dL } ，epsilon ^ {-3/2} big)迭代找到非凸问题的 ~ $big (epsilon，sqrt { dL epsilon } ，big) $- 二阶驻点，其中 $L $是 Hessian 的 Lipschitz 常数。在强凸假设下，该算法具有局部超线性收敛性。对凸问题和非凸问题的实验表明，该方法的性能明显优于基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Communication+Efficient+Distributed+Newton+Method+with+Fast+Convergence+Rates)|0|
|[Meta Multi-agent Exercise Recommendation: A Game Application Perspective](https://doi.org/10.1145/3580305.3599429)|Fei Liu, Xuegang Hu, Shuochen Liu, Chenyang Bu, Le Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meta+Multi-agent+Exercise+Recommendation:+A+Game+Application+Perspective)|0|
|[Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation](https://doi.org/10.1145/3580305.3599292)|JinDuk Park, Siqing Li, Xin Cao, WonYong Shin|Yonsei University; The University of New South Wales|The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborative signal in MC ratings. Next, to strengthen the capability of criteria preference awareness, CPA-LGC incorporates newly characterized embeddings, including user-specific criteria-preference embeddings and item-specific criterion embeddings, into our graph convolution model. Through comprehensive evaluations using four real-world datasets, we demonstrate (a) the superiority over benchmark MC recommendation methods and benchmark recommendation methods using GNNs with tremendous gains, (b) the effectiveness of core components in CPA-LGC, and (c) the computational efficiency.|多准则推荐系统在电子商贸领域广泛应用，充分利用多准则评级信息。令人惊讶的是，尽管图神经网络(GNN)由于其在学习图表示方面的高度表达能力而被广泛应用于开发各种推荐系统，但是如何利用 GNN 设计 MC 推荐系统仍然是一个未知数。有鉴于此，我们首次尝试设计一个 GNN 辅助的 MC 推荐系统。具体而言，我们不直接采用现有的基于 GNN 的推荐方法，而是设计了一种新的标准偏好感知光图卷积 CPA-LGC 方法，该方法能够精确地捕获用户的标准偏好以及复杂高阶连接中的协作信号。为此，我们首先构建一个 MC 扩展图，将用户-项目 MC 评分转换为一个扩展的二分图，以便潜在地学习 MC 评分中的协作信号。接下来，为了加强标准偏好意识的能力，CPA-LGC 将新的特征嵌入，包括用户特定的标准偏好嵌入和项目特定的标准嵌入，纳入我们的图卷积模型。通过使用四个实际数据集的综合评估，我们证明了(a)使用 GNN 的基准 MC 推荐方法和基准推荐方法的优越性，(b) CPA-LGC 中核心组件的有效性，以及(c)计算效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Criteria+Tell+You+More+than+Ratings:+Criteria+Preference-Aware+Light+Graph+Convolution+for+Effective+Multi-Criteria+Recommendation)|0|
|[Locality Sensitive Hashing for Optimizing Subgraph Query Processing in Parallel Computing Systems](https://doi.org/10.1145/3580305.3599419)|Peng Peng, Shengyi Ji, Zhen Tian, Hongbo Jiang, Weiguo Zheng, Xuecang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Locality+Sensitive+Hashing+for+Optimizing+Subgraph+Query+Processing+in+Parallel+Computing+Systems)|0|
|[Deep Pipeline Embeddings for AutoML](https://doi.org/10.1145/3580305.3599303)|Sebastian PinedaArango, Josif Grabocka|University of Freiburg|Automated Machine Learning (AutoML) is a promising direction for democratizing AI by automatically deploying Machine Learning systems with minimal human expertise. The core technical challenge behind AutoML is optimizing the pipelines of Machine Learning systems (e.g. the choice of preprocessing, augmentations, models, optimizers, etc.). Existing Pipeline Optimization techniques fail to explore deep interactions between pipeline stages/components. As a remedy, this paper proposes a novel neural architecture that captures the deep interaction between the components of a Machine Learning pipeline. We propose embedding pipelines into a latent representation through a novel per-component encoder mechanism. To search for optimal pipelines, such pipeline embeddings are used within deep-kernel Gaussian Process surrogates inside a Bayesian Optimization setup. Furthermore, we meta-learn the parameters of the pipeline embedding network using existing evaluations of pipelines on diverse collections of related datasets (a.k.a. meta-datasets). Through extensive experiments on three large-scale meta-datasets, we demonstrate that pipeline embeddings yield state-of-the-art results in Pipeline Optimization.|自动机器学习(AutoML)是通过自动部署具有最少人类专业知识的机器学习系统来实现人工智能大众化的一个有前途的方向。AutoML 背后的核心技术挑战是优化机器学习系统的管道(例如，预处理、扩展、模型、优化器等的选择)。现有的流水线优化技术无法探索流水线阶段/组件之间的深层交互。作为补救措施，本文提出了一种新颖的神经网络结构，该结构能够捕捉机器学习流水线各组件之间的深层交互。我们提出了一种新的每组件编码机制，将管道嵌入到潜在表示中。为了寻找最佳管道，这种管道嵌入在贝叶斯优化设置内的深核高斯过程代理中使用。此外，我们使用现有的对不同相关数据集(也称为元数据集)上的管道的评估来元学习管道嵌入网络的参数。通过在三个大规模元数据集上的大量实验，我们证明了流水线嵌入在流水线优化中产生了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Pipeline+Embeddings+for+AutoML)|0|
|[FedAPEN: Personalized Cross-silo Federated Learning with Adaptability to Statistical Heterogeneity](https://doi.org/10.1145/3580305.3599344)|Zhen Qin, Shuiguang Deng, Mingyu Zhao, Xueqiang Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedAPEN:+Personalized+Cross-silo+Federated+Learning+with+Adaptability+to+Statistical+Heterogeneity)|0|
|[All in One: Multi-Task Prompting for Graph Neural Networks](https://doi.org/10.1145/3580305.3599256)|Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan|Tongji University; The Hong Kong University of Science and Technology (Guangzhou); Southeast University; The Chinese University of Hong Kong|Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.|近年来，“预训练和微调”已经成为许多图形任务的标准工作流，因为它需要一般的图形知识来解决每个应用程序缺乏图形注释的问题。然而，具有节点级、边级和图级的图形任务种类繁多，使得预训练的借口往往与这些多任务不相容。这种差距甚至可能导致特定应用程序的“负转移”，从而导致较差的结果。自然语言处理中的快速学习在利用先验知识完成各种自然语言处理任务方面表现出了显著的效果，受此启发，我们研究了图形的提示主题，以填补预先训练的模型和各种图形任务之间的空白。本文提出了一种新的图模型多任务提示方法。具体来说，我们首先将图形提示符和语言提示符的格式与提示符标记、标记结构和插入模式统一起来。通过这种方式，可以将自然语言处理中的提示思想无缝地引入到图区域中。然后，为了进一步缩小各种图形任务与最先进的预训练策略之间的差距，我们进一步研究了各种图形应用的任务空间，并将下游问题重新表述为图形级任务。在此基础上，引入元学习，有效地学习图形的多任务提示的初始化，使得提示框架对于不同的任务具有更高的可靠性和通用性。我们进行了广泛的实验，实验结果证明了我们方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=All+in+One:+Multi-Task+Prompting+for+Graph+Neural+Networks)|0|
|[GMOCAT: A Graph-Enhanced Multi-Objective Method for Computerized Adaptive Testing](https://doi.org/10.1145/3580305.3599367)|Hangyu Wang, Ting Long, Liang Yin, Weinan Zhang, Wei Xia, Qichen Hong, Dingyin Xia, Ruiming Tang, Yong Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GMOCAT:+A+Graph-Enhanced+Multi-Objective+Method+for+Computerized+Adaptive+Testing)|0|
|[Theoretical Convergence Guaranteed Resource-Adaptive Federated Learning with Mixed Heterogeneity](https://doi.org/10.1145/3580305.3599521)|Yangyang Wang, Xiao Zhang, Mingyi Li, Tian Lan, Huashan Chen, Hui Xiong, Xiuzhen Cheng, Dongxiao Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Theoretical+Convergence+Guaranteed+Resource-Adaptive+Federated+Learning+with+Mixed+Heterogeneity)|0|
|[Efficient Bi-Level Optimization for Recommendation Denoising](https://doi.org/10.1145/3580305.3599324)|Zongwei Wang, Min Gao, Wentao Li, Junliang Yu, Linxin Guo, Hongzhi Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Bi-Level+Optimization+for+Recommendation+Denoising)|0|
|[Meta Graph Learning for Long-tail Recommendation](https://doi.org/10.1145/3580305.3599428)|Chunyu Wei, Jian Liang, Di Liu, Zehui Dai, Mang Li, Fei Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meta+Graph+Learning+for+Long-tail+Recommendation)|0|
|[Personalized Federated Learning with Parameter Propagation](https://doi.org/10.1145/3580305.3599464)|Jun Wu, Wenxuan Bao, Elizabeth A. Ainsworth, Jingrui He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Federated+Learning+with+Parameter+Propagation)|0|
|[Serverless Federated AUPRC Optimization for Multi-Party Collaborative Imbalanced Data Mining](https://doi.org/10.1145/3580305.3599499)|Xidong Wu, Zhengmian Hu, Jian Pei, Heng Huang|University of Pittsburgh; Duke University|Multi-party collaborative training, such as distributed learning and federated learning, is used to address the big data challenges. However, traditional multi-party collaborative training algorithms were mainly designed for balanced data mining tasks and are intended to optimize accuracy (\emph{e.g.}, cross-entropy). The data distribution in many real-world applications is skewed and classifiers, which are trained to improve accuracy, perform poorly when applied to imbalanced data tasks since models could be significantly biased toward the primary class. Therefore, the Area Under Precision-Recall Curve (AUPRC) was introduced as an effective metric. Although single-machine AUPRC maximization methods have been designed, multi-party collaborative algorithm has never been studied. The change from the single-machine to the multi-party setting poses critical challenges.   To address the above challenge, we study the serverless multi-party collaborative AUPRC maximization problem since serverless multi-party collaborative training can cut down the communications cost by avoiding the server node bottleneck, and reformulate it as a conditional stochastic optimization problem in a serverless multi-party collaborative learning setting and propose a new ServerLess biAsed sTochastic gradiEnt (SLATE) algorithm to directly optimize the AUPRC. After that, we use the variance reduction technique and propose ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction (SLATE-M) algorithm to improve the convergence rate, which matches the best theoretical convergence result reached by the single-machine online method. To the best of our knowledge, this is the first work to solve the multi-party collaborative AUPRC maximization problem.|多方协作培训，如分布式学习和联合学习，被用来解决大数据的挑战。然而，传统的多方协同训练算法主要是针对平衡的数据挖掘任务而设计的，其目的是优化精度(例如: 交叉熵)。许多实际应用中的数据分布是倾斜的，分类器经过训练以提高准确性，但在应用于不平衡的数据任务时表现不佳，因为模型可能明显偏向于主类。因此，引入精确回忆曲线下面积(AUPRC)作为一个有效的度量指标。虽然单机 AUPRC 最大化方法已经设计出来，但是多方协作算法还没有得到研究。从单一机器设置到多方设置的变化提出了关键的挑战。为了解决上述问题，我们研究了无服务器多方协作的 AUPRC 最大化问题，因为无服务器多方协作培训可以通过避免服务器节点瓶颈来降低通信成本，并将其重新表述为无服务器多方协作环境中的条件随机最佳化问题，提出了一种新的无服务器偏置 sTo侯机梯度(slATE)算法来直接优化 AUPRC，该算法可以用于解决无服务器多方协作的合作学习。在此基础上，利用方差减少技术，提出了基于动量方差减少(SLATE-M)的 ServerLess 偏向随机梯度算法，提高了算法的收敛速度，达到了单机在线算法的最佳理论收敛效果。据我们所知，这是第一个解决多方协作 AUPRC 最大化问题的工作。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Serverless+Federated+AUPRC+Optimization+for+Multi-Party+Collaborative+Imbalanced+Data+Mining)|0|
|[MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer](https://doi.org/10.1145/3580305.3599438)|Yazheng Yang, Zhou Zhao, Qi Liu|The University of Hong Kong; Zhejiang University|Unsupervised text style transfer task aims to rewrite a text into target style while preserving its main content. Traditional methods rely on the use of a fixed-sized vector to regulate text style, which is difficult to accurately convey the style strength for each individual token. In fact, each token of a text contains different style intensity and makes different contribution to the overall style. Our proposed method addresses this issue by assigning individual style vector to each token in a text, allowing for fine-grained control and manipulation of the style strength. Additionally, an adversarial training framework integrated with teacher-student learning is introduced to enhance training stability and reduce the complexity of high-dimensional optimization. The results of our experiments demonstrate the efficacy of our method in terms of clearly improved style transfer accuracy and content preservation in both two-style transfer and multi-style transfer settings.|无监督文本样式转换任务的目标是在保留文本主要内容的同时将文本重写成目标样式。传统的方法依赖于使用固定大小的向量来调整文本样式，这很难准确地表达每个单独标记的样式强度。事实上，文本的每一个标记都包含着不同的风格强度，并对整体风格做出不同的贡献。我们提出的方法通过为文本中的每个标记分配单独的样式向量来解决这个问题，从而允许对样式强度进行细粒度控制和操作。此外，为了提高训练的稳定性，降低高维优化的复杂性，提出了一种结合师生学习的对抗性训练框架。实验结果表明，该方法在两种类型和多种类型的转移设置下，均能明显提高文体转移的准确性和内容保存率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MSSRNet:+Manipulating+Sequential+Style+Representation+for+Unsupervised+Text+Style+Transfer)|0|
|[Knowledge Graph Self-Supervised Rationalization for Recommendation](https://doi.org/10.1145/3580305.3599400)|Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang|Tencent; The University of Hong Kong|In this paper, we introduce a new self-supervised rationalization method, called KGRec, for knowledge-aware recommender systems. To effectively identify informative knowledge connections, we propose an attentive knowledge rationalization mechanism that generates rational scores for knowledge triplets. With these scores, KGRec integrates generative and contrastive self-supervised tasks for recommendation through rational masking. To highlight rationales in the knowledge graph, we design a novel generative task in the form of masking-reconstructing. By masking important knowledge with high rational scores, KGRec is trained to rebuild and highlight useful knowledge connections that serve as rationales. To further rationalize the effect of collaborative interactions on knowledge graph learning, we introduce a contrastive learning task that aligns signals from knowledge and user-item interaction views. To ensure noise-resistant contrasting, potential noisy edges in both graphs judged by the rational scores are masked. Extensive experiments on three real-world datasets demonstrate that KGRec outperforms state-of-the-art methods. We also provide the implementation codes for our approach at https://github.com/HKUDS/KGRec.|本文针对知识感知推荐系统，提出了一种新的自监督合理化方法 KGRec。为了有效地识别信息知识连接，我们提出了一种注意的知识合理化机制，为知识三元组生成合理的分数。根据这些分数，KGRec 通过合理的掩蔽将生成性和对比性自我监督任务集成到推荐系统中。为了突出知识图中的基本原理，我们设计了一个新的生成任务，即掩蔽-重构。通过用高理性分数掩盖重要的知识，KGRec 被训练重建和突出作为基本原理的有用的知识联系。为了进一步合理化协作交互对知识图学习的影响，我们引入了一个对比学习任务，该任务从知识和用户项目交互视图中调整信号。为了确保抗噪声的对比，在两个图的潜在噪声边缘判断有理分数被掩盖。在三个真实世界数据集上的大量实验表明，KGRec 的性能优于最先进的方法。我们亦会在 https://github.com/hkuds/kgrec 为我们的方法提供实施守则。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Self-Supervised+Rationalization+for+Recommendation)|0|
|[FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy](https://doi.org/10.1145/3580305.3599345)|Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, Haibing Guan|Queen’s University Belfast; Shanghai Jiao Tong University; Louisiana State University|Recently, personalized federated learning (pFL) has attracted increasing attention in privacy protection, collaborative learning, and tackling statistical heterogeneity among clients, e.g., hospitals, mobile smartphones, etc. Most existing pFL methods focus on exploiting the global information and personalized information in the client-level model parameters while neglecting that data is the source of these two kinds of information. To address this, we propose the Federated Conditional Policy (FedCP) method, which generates a conditional policy for each sample to separate the global information and personalized information in its features and then processes them by a global head and a personalized head, respectively. FedCP is more fine-grained to consider personalization in a sample-specific manner than existing pFL methods. Extensive experiments in computer vision and natural language processing domains show that FedCP outperforms eleven state-of-the-art methods by up to 6.69%. Furthermore, FedCP maintains its superiority when some clients accidentally drop out, which frequently happens in mobile settings. Our code is public at https://github.com/TsingZ0/FedCP.|近年来，个性化联邦学习(pFL)在保护个人隐私、合作学习以及处理客户之间的统计异质性等方面受到越来越多的关注，例如医院、移动智能手机等。现有的 pFL 方法大多侧重于利用客户端模型参数中的全局信息和个性化信息，而忽视了数据是这两类信息的来源。为了解决这个问题，我们提出了联邦条件策略(FedCP)方法，该方法为每个样本生成一个条件策略来分离其特征中的全局信息和个性化信息，然后分别通过一个全局头和一个个性化头来处理它们。与现有的 pFL 方法相比，FedCP 更加细粒度地以特定于样本的方式考虑个性化。在计算机视觉和自然语言处理领域的大量实验表明，FedCP 比11种最先进的方法的性能提高了6.69% 。此外，当一些客户端意外退出时，FedCP 仍然保持其优势，这种情况在移动设置中经常发生。我们的代码在 https://github.com/tsingz0/fedcp 是公开的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedCP:+Separating+Feature+Information+for+Personalized+Federated+Learning+via+Conditional+Policy)|0|
|[CFGL-LCR: A Counterfactual Graph Learning Framework for Legal Case Retrieval](https://doi.org/10.1145/3580305.3599273)|Kun Zhang, Chong Chen, Yuanzhuo Wang, Qi Tian, Long Bai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CFGL-LCR:+A+Counterfactual+Graph+Learning+Framework+for+Legal+Case+Retrieval)|0|
|[DM-PFL: Hitchhiking Generic Federated Learning for Efficient Shift-Robust Personalization](https://doi.org/10.1145/3580305.3599311)|Wenhao Zhang, Zimu Zhou, Yansheng Wang, Yongxin Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DM-PFL:+Hitchhiking+Generic+Federated+Learning+for+Efficient+Shift-Robust+Personalization)|0|
|[Efficient Approximation Algorithms for Spanning Centrality](https://doi.org/10.1145/3580305.3599323)|Shiqi Zhang, Renchi Yang, Jing Tang, Xiaokui Xiao, Bo Tang|Hong Kong Baptist University; Southern University of Science and Technology; National University of Singapore; The Hong Kong University of Science and Technology (Guangzhou)|Given a graph $\mathcal{G}$, the spanning centrality (SC) of an edge $e$ measures the importance of $e$ for $\mathcal{G}$ to be connected. In practice, SC has seen extensive applications in computational biology, electrical networks, and combinatorial optimization. However, it is highly challenging to compute the SC of all edges (AESC) on large graphs. Existing techniques fail to deal with such graphs, as they either suffer from expensive matrix operations or require sampling numerous long random walks. To circumvent these issues, this paper proposes TGT and its enhanced version TGT+, two algorithms for AESC computation that offers rigorous theoretical approximation guarantees. In particular, TGT remedies the deficiencies of previous solutions by conducting deterministic graph traversals with carefully-crafted truncated lengths. TGT+ further advances TGT in terms of both empirical efficiency and asymptotic performance while retaining result quality, based on the combination of TGT with random walks and several additional heuristic optimizations. We experimentally evaluate TGT+ against recent competitors for AESC using a variety of real datasets. The experimental outcomes authenticate that TGT+ outperforms the state of the arts often by over one order of magnitude speedup without degrading the accuracy.|给定一个图 $mathal { G } $，边 $e $的生成中心性(SC)度量 $e $对于要连接的 $mathal { G } $的重要性。在实践中，SC 已经在计算生物学、电力网络和组合优化等领域得到了广泛的应用。然而，计算大图上所有边的 SC (AESC)是一个非常具有挑战性的问题。现有的技术无法处理这样的图，因为它们要么需要进行昂贵的矩阵运算，要么需要对大量的长随机游动进行采样。为了解决这些问题，本文提出了 TGT 及其改进版本 TGT + ，这两种 AESC 计算算法提供了严格的理论近似保证。特别是，TGT 通过使用精心设计的截断长度进行确定性图遍历，弥补了以前解决方案的缺陷。TGT + 基于 TGT 与随机游动的结合以及几个附加的启发式优化，在保持结果质量的同时，进一步提高了 TGT 的经验有效性和渐近性能。我们使用各种实际数据集对 AESC 的最近竞争对手进行了 TGT + 的实验评估。实验结果表明，在不降低准确性的情况下，TGT + 的性能通常比现有技术水平高出一个数量级。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Approximation+Algorithms+for+Spanning+Centrality)|0|
|[Improving Search Clarification with Structured Information Extracted from Search Results](https://doi.org/10.1145/3580305.3599389)|Ziliang Zhao, Zhicheng Dou, Yu Guo, Zhao Cao, Xiaohua Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Search+Clarification+with+Structured+Information+Extracted+from+Search+Results)|0|
|[Dense Representation Learning and Retrieval for Tabular Data Prediction](https://doi.org/10.1145/3580305.3599305)|Lei Zheng, Ning Li, Xianyu Chen, Quan Gan, Weinan Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dense+Representation+Learning+and+Retrieval+for+Tabular+Data+Prediction)|0|
|[A Sublinear Time Algorithm for Opinion Optimization in Directed Social Networks via Edge Recommendation](https://doi.org/10.1145/3580305.3599247)|Xiaotian Zhou, Liwang Zhu, Wei Li, Zhongzhi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Sublinear+Time+Algorithm+for+Opinion+Optimization+in+Directed+Social+Networks+via+Edge+Recommendation)|0|
|[Path-Specific Counterfactual Fairness for Recommender Systems](https://doi.org/10.1145/3580305.3599462)|Yaochen Zhu, Jing Ma, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li|LinkedIn Inc.; University of Virginia|Recommender systems (RSs) have become an indispensable part of online platforms. With the growing concerns of algorithmic fairness, RSs are not only expected to deliver high-quality personalized content, but are also demanded not to discriminate against users based on their demographic information. However, existing RSs could capture undesirable correlations between sensitive features and observed user behaviors, leading to biased recommendations. Most fair RSs tackle this problem by completely blocking the influences of sensitive features on recommendations. But since sensitive features may also affect user interests in a fair manner (e.g., race on culture-based preferences), indiscriminately eliminating all the influences of sensitive features inevitably degenerate the recommendations quality and necessary diversities. To address this challenge, we propose a path-specific fair RS (PSF-RS) for recommendations. Specifically, we summarize all fair and unfair correlations between sensitive features and observed ratings into two latent proxy mediators, where the concept of path-specific bias (PS-Bias) is defined based on path-specific counterfactual inference. Inspired by Pearl's minimal change principle, we address the PS-Bias by minimally transforming the biased factual world into a hypothetically fair world, where a fair RS model can be learned accordingly by solving a constrained optimization problem. For the technical part, we propose a feasible implementation of PSF-RS, i.e., PSF-VAE, with weakly-supervised variational inference, which robustly infers the latent mediators such that unfairness can be mitigated while necessary recommendation diversities can be maximally preserved simultaneously. Experiments conducted on semi-simulated and real-world datasets demonstrate the effectiveness of PSF-RS.|推荐系统已经成为在线平台不可或缺的一部分。随着对算法公平性的日益关注，RSS 不仅被期望提供高质量的个性化内容，而且被要求不因用户的人口统计信息而歧视用户。然而，现有的 RSS 可能捕获敏感特性和观察到的用户行为之间不希望看到的相关性，从而导致有偏见的推荐。大多数公平的 RSS 通过完全屏蔽敏感特性对推荐的影响来解决这个问题。但是，由于敏感特性也可能以公平的方式影响用户的兴趣(例如，基于文化的偏好的种族) ，不加区分地消除敏感特性的所有影响必然会降低推荐的质量和必要的多样性。为了应对这一挑战，我们提出了一个路径特定公平 RS (PSF-RS)的建议。具体而言，我们将敏感特征和观察评分之间的所有公平和不公平的相关性总结为两个潜在的代理中介，其中路径特异性偏倚(PS-Bias)的概念是基于路径特异性反事实推断定义的。受珀尔的最小改变原则的启发，我们通过最小化地将有偏见的现实世界转化为一个假设的公平世界，在这个假设的公平世界中，可以通过解决一个受限制的最佳化问题来相应地学习一个公平的遥感模型，从而解决偏差问题。在技术部分，我们提出了一种可行的 PSF-RS 实现方法，即 PSF-VAE，该方法利用弱监督变分推理强有力地推导出潜在的中介因子，从而在最大限度地保留必要的推荐多样性的同时减少不公平性。在半模拟和真实数据集上进行的实验证明了 PSF-RS 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Path-Specific+Counterfactual+Fairness+for+Recommender+Systems)|0|
|[Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach](https://doi.org/10.1145/3580305.3599788)|Zhangming Chan, Yu Zhang, Shuguang Han, Yong Bai, XiangRong Sheng, Siyuan Lou, Jiacen Hu, Baolin Liu, Yuning Jiang, Jian Xu, Bo Zheng|University of Science and Technology Beijing; Alibaba Group; Nanjing University|Conversion rate (CVR) prediction is one of the core components in online recommender systems, and various approaches have been proposed to obtain accurate and well-calibrated CVR estimation. However, we observe that a well-trained CVR prediction model often performs sub-optimally during sales promotions. This can be largely ascribed to the problem of the data distribution shift, in which the conventional methods no longer work. To this end, we seek to develop alternative modeling techniques for CVR prediction. Observing similar purchase patterns across different promotions, we propose reusing the historical promotion data to capture the promotional conversion patterns. Herein, we propose a novel \textbf{H}istorical \textbf{D}ata \textbf{R}euse (\textbf{HDR}) approach that first retrieves historically similar promotion data and then fine-tunes the CVR prediction model with the acquired data for better adaptation to the promotion mode. HDR consists of three components: an automated data retrieval module that seeks similar data from historical promotions, a distribution shift correction module that re-weights the retrieved data for better aligning with the target promotion, and a TransBlock module that quickly fine-tunes the original model for better adaptation to the promotion mode. Experiments conducted with real-world data demonstrate the effectiveness of HDR, as it improves both ranking and calibration metrics to a large extent. HDR has also been deployed on the display advertising system in Alibaba, bringing a lift of $9\%$ RPM and $16\%$ CVR during Double 11 Sales in 2022.|转化率(CVR)预测是在线推荐系统的核心组成部分之一，为了获得准确、标定良好的 CVR 估计，人们提出了各种方法。然而，我们观察到，训练有素的 CVR 预测模型在促销期间往往表现不佳。这在很大程度上归因于数据分布偏移的问题，在这个问题中，传统的方法不再起作用。为此，我们寻求发展可替代的 CVR 预测建模技术。通过观察不同促销活动中相似的购买模式，我们建议重用历史促销数据来捕获促销转换模式。在这里，我们提出了一种新的 textbf { H }历史 textbf { D } ata textbf { R } euse (textbf { HDR })方法，首先检索历史上相似的促销数据，然后用所获得的数据对 CVR 预测模型进行微调，以更好地适应促销模式。人类发展报告由三个组成部分组成: 一个自动数据检索模块，从历史促销活动中寻找类似数据; 一个分配转移校正模块，重新加权检索的数据，以便更好地与目标促销活动保持一致; 一个 TransBlock 模块，快速微调原始模型，以便更好地适应促销模式。利用实际数据进行的实验证明了 HDR 的有效性，因为它在很大程度上改善了排序和校准指标。HDR 也已经部署在阿里巴巴的显示广告系统上，在2022年双11销售期间，带来了9% 的每分钟转速和16% 的 CVR。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Capturing+Conversion+Rate+Fluctuation+during+Sales+Promotions:+A+Novel+Historical+Data+Reuse+Approach)|0|
|[SAMD: An Industrial Framework for Heterogeneous Multi-Scenario Recommendation](https://doi.org/10.1145/3580305.3599955)|Zhaoxin Huan, Ang Li, Xiaolu Zhang, Xu Min, Jieyu Yang, Yong He, Jun Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SAMD:+An+Industrial+Framework+for+Heterogeneous+Multi-Scenario+Recommendation)|0|
|[Learning Discrete Document Representations in Web Search](https://doi.org/10.1145/3580305.3599854)|Rong Huang, Danfeng Zhang, Weixue Lu, Han Li, Meng Wang, Daiting Shi, Jun Fan, Zhicong Cheng, Simiu Gu, Dawei Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Discrete+Document+Representations+in+Web+Search)|0|
|[AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness](https://doi.org/10.1145/3580305.3599770)|Liyao Jiang, Chenglin Li, Haolan Chen, Xiaodong Gao, Xinwang Zhong, Yang Qiu, Shani Ye, Di Niu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdSEE:+Investigating+the+Impact+of+Image+Style+Editing+on+Advertisement+Attractiveness)|0|
|[Adaptive Graph Contrastive Learning for Recommendation](https://doi.org/10.1145/3580305.3599768)|Yangqin Jiang, Chao Huang, Lianghao Huang|University of Hong Kong|Recently, graph neural networks (GNNs) have been successfully applied to recommender systems as an effective collaborative filtering (CF) approach. The key idea of GNN-based recommender system is to recursively perform the message passing along the user-item interaction edge for refining the encoded embeddings, relying on sufficient and high-quality training data. Since user behavior data in practical recommendation scenarios is often noisy and exhibits skewed distribution, some recommendation approaches, e.g., SGL and SimGCL, leverage self-supervised learning to improve user representations against the above issues. Despite their effectiveness, however, they conduct self-supervised learning through creating contrastvie views, depending on the exploration of data augmentations with the problem of tedious trial-and-error selection of augmentation methods. In this paper, we propose a novel Adaptive Graph Contrastive Learning (AdaptiveGCL) framework which conducts graph contrastive learning with two adaptive contrastive view generators to better empower CF paradigm. Specifically, we use two trainable view generators, which are a graph generative model and a graph denoising model respectively, to create contrastive views. Two generators are able to create adaptive contrastive views, addressing the problem of model collapse and achieving adaptive contrastive learning. With two adaptive contrasive views, more additionally high-quality training signals will be introduced into the CF paradigm and help to alleviate the data sparsity and noise issues. Extensive experiments on three benchmark datasets demonstrate the superiority of our model over various state-of-the-art recommendation methods. Further visual analysis intuitively explains why our AdaptiveGCL outperforms existing contrastive learning approaches based on selected data augmentation methods.|最近，图形神经网络(GNN)已成功应用于推荐系统，作为一种有效的协同过滤(CF)方法。基于 GNN 的推荐系统的关键思想是依靠充分和高质量的训练数据，递归地执行沿用户项目交互边缘传递的消息，以完善编码的嵌入。由于实际推荐场景中的用户行为数据通常是有噪音的，并且呈现出偏态分布，因此一些推荐方法，如 SGL 和 SimGCL，利用自监督学习来改善用户对上述问题的表示。然而，尽管他们的有效性，他们进行自我监督学习通过创建对比观点，依赖于探索数据增强与繁琐的试错选择增强方法的问题。本文提出了一种新的自适应图形对比学习(AdaptiveGCL)框架，该框架使用两个自适应对比视图生成器进行图形对比学习，以更好地支持 CF 范式。具体来说，我们使用两个可训练的视图生成器，分别是一个图形生成模型和一个图形去噪模型，来创建对比视图。两个生成器能够创建自适应对比视图，解决模型崩溃问题，实现自适应对比学习。通过两个自适应对立视图，在 CF 范式中引入更多高质量的训练信号，有助于缓解数据稀疏和噪声问题。在三个基准数据集上的大量实验证明了我们的模型优于各种最先进的推荐方法。进一步的可视化分析直观地解释了为什么我们的 AdaptiveGCL 优于基于所选数据增强方法的现有对比学习方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Graph+Contrastive+Learning+for+Recommendation)|0|
|[PGLBox: Multi-GPU Graph Learning Framework for Web-Scale Recommendation](https://doi.org/10.1145/3580305.3599885)|Xuewu Jiao, Weibin Li, Xinxuan Wu, Wei Hu, Miao Li, Jiang Bian, Siming Dai, Xinsheng Luo, Mingqing Hu, Zhengjie Huang, Danlei Feng, Junchao Yang, Shikun Feng, Haoyi Xiong, Dianhai Yu, Shuanglong Li, Jingzhou He, Yanjun Ma, Lin Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PGLBox:+Multi-GPU+Graph+Learning+Framework+for+Web-Scale+Recommendation)|0|
|[IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research](https://doi.org/10.1145/3580305.3599843)|Arpandeep Khatua, Vikram Sharma Mailthody, Bhagyashree Taleka, Tengfei Ma, Xiang Song, WenMei Hwu|AWS AI; NVIDIA; UIUC; IBM Research|Graph neural networks (GNNs) have shown high potential for a variety of real-world, challenging applications, but one of the major obstacles in GNN research is the lack of large-scale flexible datasets. Most existing public datasets for GNNs are relatively small, which limits the ability of GNNs to generalize to unseen data. The few existing large-scale graph datasets provide very limited labeled data. This makes it difficult to determine if the GNN model's low accuracy for unseen data is inherently due to insufficient training data or if the model failed to generalize. Additionally, datasets used to train GNNs need to offer flexibility to enable a thorough study of the impact of various factors while training GNN models.   In this work, we introduce the Illinois Graph Benchmark (IGB), a research dataset tool that the developers can use to train, scrutinize and systematically evaluate GNN models with high fidelity. IGB includes both homogeneous and heterogeneous graphs of enormous sizes, with more than 40% of their nodes labeled. Compared to the largest graph datasets publicly available, the IGB provides over 162X more labeled data for deep learning practitioners and developers to create and evaluate models with higher accuracy. The IGB dataset is designed to be flexible, enabling the study of various GNN architectures, embedding generation techniques, and analyzing system performance issues. IGB is open-sourced, supports DGL and PyG frameworks, and comes with releases of the raw text that we believe foster emerging language models and GNN research projects. An early public version of IGB is available at https://github.com/IllinoisGraphBenchmark/IGB-Datasets.|图形神经网络(GNN)在现实世界中具有很大的应用潜力，但是缺乏大规模的灵活数据集是 GNN 研究的主要障碍之一。大多数现有的 GNN 公共数据集相对较小，这限制了 GNN 推广到未见数据的能力。少数现有的大规模图形数据集提供非常有限的标记数据。这使得很难确定 GNN 模型对于不可见数据的低精度是否本质上是由于训练数据不足或者模型没有推广。此外，用于训练 GNN 的数据集需要提供灵活性，以便在训练 GNN 模型时能够对各种因素的影响进行彻底的研究。在这项工作中，我们介绍了伊利诺伊图基准(IGB) ，一个研究数据集工具，开发人员可以用来训练，审查和系统地评估 GNN 模型的高保真度。IGB 包括大型的同质和异质图，其中超过40% 的节点被标记。与公开发布的最大的图形数据集相比，IGB 为深度学习从业者和开发者提供了超过162倍的标记数据，以创建和评估更高精度的模型。IGB 数据集的设计是灵活的，能够研究各种 GNN 体系结构、嵌入生成技术和分析系统性能问题。IGB 是开源的，支持 DGL 和 PyG 框架，并且附带了原始文本的发布，我们相信这些原始文本可以促进新兴语言模型和 GNN 研究项目的发展。IGB 的早期公开版本可在 https://github.com/illinoisgraphbenchmark/IGB-datasets 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IGB:+Addressing+The+Gaps+In+Labeling,+Features,+Heterogeneity,+and+Size+of+Public+Graph+Datasets+for+Deep+Learning+Research)|0|
|[AdaTT: Adaptive Task-to-Task Fusion Network for Multitask Learning in Recommendations](https://doi.org/10.1145/3580305.3599769)|Danwei Li, Zhengyu Zhang, Siyang Yuan, Mingze Gao, Weilin Zhang, Chaofei Yang, Xi Liu, Jiyan Yang|Meta AI; Meta Platforms, Inc.|Multi-task learning (MTL) aims at enhancing the performance and efficiency of machine learning models by training them on multiple tasks simultaneously. However, MTL research faces two challenges: 1) modeling the relationships between tasks to effectively share knowledge between them, and 2) jointly learning task-specific and shared knowledge. In this paper, we present a novel model Adaptive Task-to-Task Fusion Network (AdaTT) to address both challenges. AdaTT is a deep fusion network built with task specific and optional shared fusion units at multiple levels. By leveraging a residual mechanism and gating mechanism for task-to-task fusion, these units adaptively learn shared knowledge and task specific knowledge. To evaluate the performance of AdaTT, we conduct experiments on a public benchmark and an industrial recommendation dataset using various task groups. Results demonstrate AdaTT can significantly outperform existing state-of-the-art baselines.|多任务学习(MTL)旨在通过同时对多任务进行训练来提高机器学习模型的性能和效率。然而，MTL 研究面临着两个挑战: 1)建立任务之间的关系以有效地分享它们之间的知识，2)联合学习任务特定的和共享的知识。在本文中，我们提出了一个新的模型自适应任务到任务融合网络(AdaTT) ，以解决这两个挑战。AdaTT 是一个深度融合网络，由多个级别的任务特定的和可选的共享融合单元构成。通过利用剩余机制和门控机制进行任务-任务融合，这些单元自适应地学习共享知识和任务特定知识。为了评估 AdaTT 的性能，我们使用不同的任务组在一个公共基准和一个工业推荐数据集上进行了实验。结果表明，AdaTT 可以显著优于现有的最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdaTT:+Adaptive+Task-to-Task+Fusion+Network+for+Multitask+Learning+in+Recommendations)|0|
|[Stationary Algorithmic Balancing For Dynamic Email Re-Ranking Problem](https://doi.org/10.1145/3580305.3599909)|Jiayi Liu, Jennifer Neville|Purdue University|Email platforms need to generate personalized rankings of emails that satisfy user preferences, which may vary over time. We approach this as a recommendation problem based on three criteria: closeness (how relevant the sender and topic are to the user), timeliness (how recent the email is), and conciseness (how brief the email is). We propose MOSR (Multi-Objective Stationary Recommender), a novel online algorithm that uses an adaptive control model to dynamically balance these criteria and adapt to preference changes. We evaluate MOSR on the Enron Email Dataset, a large collection of real emails, and compare it with other baselines. The results show that MOSR achieves better performance, especially under non-stationary preferences, where users value different criteria more or less over time. We also test MOSR's robustness on a smaller down-sampled dataset that exhibits high variance in email characteristics, and show that it maintains stable rankings across different samples. Our work offers novel insights into how to design email re-ranking systems that account for multiple objectives impacting user satisfaction.|电子邮件平台需要生成个性化的电子邮件排名，以满足用户的喜好，这可能随着时间的推移而变化。我们基于三个标准来处理这个推荐问题: 亲密性(发送者和主题与用户的相关程度)、及时性(邮件发送时间有多近)和简洁性(邮件有多简短)。我们提出了一种新的在线算法——多目标平稳推荐(MOSR) ，它使用自适应控制模型来动态平衡这些标准，并适应偏好的变化。我们评估 MOSR 的安然电子邮件数据集，一大批真实的电子邮件，并比较它与其他基线。结果表明，在非平稳偏好条件下，特别是在用户随着时间的推移或多或少评价不同标准的情况下，MOSR 可以获得更好的性能。我们还测试了 MOSR 的稳健性较小的下采样数据集，表现出高的电子邮件特征方差，并表明它保持稳定的排名在不同的样本。我们的工作提供了新颖的见解，如何设计电子邮件重新排序系统的帐户多个目标影响用户的满意度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stationary+Algorithmic+Balancing+For+Dynamic+Email+Re-Ranking+Problem)|0|
|[Tree based Progressive Regression Model for Watch-Time Prediction in Short-video Recommendation](https://doi.org/10.1145/3580305.3599919)|Xiao Lin, Xiaokai Chen, Linfeng Song, Jingwei Liu, Biao Li, Peng Jiang|Kuaishou Technology|An accurate prediction of watch time has been of vital importance to enhance user engagement in video recommender systems. To achieve this, there are four properties that a watch time prediction framework should satisfy: first, despite its continuous value, watch time is also an ordinal variable and the relative ordering between its values reflects the differences in user preferences. Therefore the ordinal relations should be reflected in watch time predictions. Second, the conditional dependence between the video-watching behaviors should be captured in the model. For instance, one has to watch half of the video before he/she finishes watching the whole video. Third, modeling watch time with a point estimation ignores the fact that models might give results with high uncertainty and this could cause bad cases in recommender systems. Therefore the framework should be aware of prediction uncertainty. Forth, the real-life recommender systems suffer from severe bias amplifications thus an estimation without bias amplification is expected. Therefore we propose TPM for watch time prediction. Specifically, the ordinal ranks of watch time are introduced into TPM and the problem is decomposed into a series of conditional dependent classification tasks which are organized into a tree structure. The expectation of watch time can be generated by traversing the tree and the variance of watch time predictions is explicitly introduced into the objective function as a measurement for uncertainty. Moreover, we illustrate that backdoor adjustment can be seamlessly incorporated into TPM, which alleviates bias amplifications. Extensive offline evaluations have been conducted in public datasets and TPM have been deployed in a real-world video app Kuaishou with over 300 million DAUs. The results indicate that TPM outperforms state-of-the-art approaches and indeed improves video consumption significantly.|准确预测观看时间对于提高用户在视频推荐系统中的参与度至关重要。为了实现这一点，手表时间预测框架应该满足四个特性: 第一，尽管手表时间是连续的，但它也是一个有序变量，其值之间的相对排序反映了用户偏好的差异。因此，序数关系应反映在手表时间预测中。其次，在模型中要捕捉视频观看行为之间的条件依赖关系。例如，一个人必须看完一半的视频才能看完整个视频。第三，使用点估计对手表时间进行建模忽略了这样一个事实，即模型可能会给出高度不确定性的结果，这可能会导致推荐系统出现问题。因此，框架应该意识到预测的不确定性。第四，现实生活中的推荐系统遭受严重的偏差放大，因此估计没有偏差放大的预期。因此，我们提出 TPM 来预测手表时间。在 TPM 中引入了观察时间序列，并将问题分解为一系列条件相关的分类任务，这些任务被组织成一个树形结构。通过遍历该树可以产生观察时间的期望值，并且在目标函数中明确地引入观察时间预测的方差作为不确定性的度量。此外，我们说明后门调整可以无缝地纳入 TPM，从而减轻偏差放大。在公共数据集中已经进行了广泛的离线评估，TPM 已经部署在一个现实世界的视频应用快手中，有超过3亿 DAU。结果表明，TPM 优于最先进的方法，确实显著提高了视频消费。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tree+based+Progressive+Regression+Model+for+Watch-Time+Prediction+in+Short-video+Recommendation)|0|
|[HUGE: Huge Unsupervised Graph Embeddings with TPUs](https://doi.org/10.1145/3580305.3599840)|Brandon A. Mayer, Anton Tsitsulin, Hendrik Fichtenberger, Jonathan Halcrow, Bryan Perozzi|Google Research|Graphs are a representation of structured data that captures the relationships between sets of objects. With the ubiquity of available network data, there is increasing industrial and academic need to quickly analyze graphs with billions of nodes and trillions of edges. A common first step for network understanding is Graph Embedding, the process of creating a continuous representation of nodes in a graph. A continuous representation is often more amenable, especially at scale, for solving downstream machine learning tasks such as classification, link prediction, and clustering. A high-performance graph embedding architecture leveraging Tensor Processing Units (TPUs) with configurable amounts of high-bandwidth memory is presented that simplifies the graph embedding problem and can scale to graphs with billions of nodes and trillions of edges. We verify the embedding space quality on real and synthetic large-scale datasets.|图表是结构化数据的表示，它捕捉对象集之间的关系。随着可用网络数据的普及，工业界和学术界越来越需要快速分析具有数十亿个节点和数万亿条边的图形。网络理解的一个常见的第一步是图形嵌入，即在图形中创建连续的节点表示的过程。连续表示通常更适合于解决下游机器学习任务，如分类、链接预测和聚类，尤其是在规模上。提出了一种利用张量处理单元(TPU)和可配置的高带宽存储器构成的高性能图嵌入体系结构，简化了图嵌入问题，并且可以扩展到具有数十亿个节点和数万亿条边的图。在实际和合成的大规模数据集上验证了嵌入空间的质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HUGE:+Huge+Unsupervised+Graph+Embeddings+with+TPUs)|0|
|[Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks](https://doi.org/10.1145/3580305.3599898)|Zeyu Qin, Liuyi Yao, Daoyuan Chen, Yaliang Li, Bolin Ding, Minhao Cheng|Hong Kong University of Science and Technology; Alibaba Group|In this work, besides improving prediction accuracy, we study whether personalization could bring robustness benefits to backdoor attacks. We conduct the first study of backdoor attacks in the pFL framework, testing 4 widely used backdoor attacks against 6 pFL methods on benchmark datasets FEMNIST and CIFAR-10, a total of 600 experiments. The study shows that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In contrast, pFL methods with full model-sharing do not show robustness. To analyze the reasons for varying robustness performances, we provide comprehensive ablation studies on different pFL methods. Based on our findings, we further propose a lightweight defense method, Simple-Tuning, which empirically improves defense performance against backdoor attacks. We believe that our work could provide both guidance for pFL application in terms of its robustness and offer valuable insights to design more robust FL methods in the future.|在这项工作中，除了提高预测的准确性，我们研究个性化是否可以带来健壮性的好处后门攻击。我们在 pFL 框架中进行了后门攻击的第一次研究，在基准数据集 FEMNIST 和 CIFAR-10上测试了4个广泛使用的后门攻击与6个 pFL 方法，共计600个实验。研究表明，部分模型共享的 pFL 方法可以显著提高对后门攻击的鲁棒性。相比之下，完全模型共享的 pFL 方法不具有鲁棒性。为了分析鲁棒性能变化的原因，我们对不同的 pFL 方法进行了全面的消融研究。在此基础上，我们进一步提出了一种轻量级的防御方法——简单调整(Simple-Tuning) ，该方法可以实验性地提高对后门攻击的防御性能。我们相信，我们的工作可以为 pFL 的应用提供指导，在其健壮性方面，并提供有价值的见解，以设计更健壮的 FL 方法在未来。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Personalized+Federated+Learning:+Robustness+Against+Backdoor+Attacks)|0|
|[Joint Optimization of Ranking and Calibration with Contextualized Hybrid Model](https://doi.org/10.1145/3580305.3599851)|XiangRong Sheng, Jingyue Gao, Yueyao Cheng, Siran Yang, Shuguang Han, Hongbo Deng, Yuning Jiang, Jian Xu, Bo Zheng|Alibaba Group|Despite the development of ranking optimization techniques, the pointwise model remains the dominating approach for click-through rate (CTR) prediction. It can be attributed to the calibration ability of the pointwise model since the prediction can be viewed as the click probability. In practice, a CTR prediction model is also commonly assessed with the ranking ability, for which prediction models based on ranking losses (e.g., pairwise or listwise loss) usually achieve better performances than the pointwise loss. Previous studies have experimented with a direct combination of the two losses to obtain the benefit from both losses and observed an improved performance. However, previous studies break the meaning of output logit as the click-through rate, which may lead to sub-optimal solutions. To address this issue, we propose an approach that can Jointly optimize the Ranking and Calibration abilities (JRC for short). JRC improves the ranking ability by contrasting the logit value for the sample with different labels and constrains the predicted probability to be a function of the logit subtraction. We further show that JRC consolidates the interpretation of logits, where the logits model the joint distribution. With such an interpretation, we prove that JRC approximately optimizes the contextualized hybrid discriminative-generative objective. Experiments on public and industrial datasets and online A/B testing show that our approach improves both ranking and calibration abilities. Since May 2022, JRC has been deployed on the display advertising platform of Alibaba and has obtained significant performance improvements.|尽管排序优化技术不断发展，逐点模型仍然是点进率预测的主要方法。这可以归因于点态模型的校准能力，因为预测可以被视为点击概率。在实践中，CTR 预测模型通常也是用排序能力来评估的，其中基于排序损失的预测模型(例如，成对损失或列表损失)通常比逐点损失的预测模型获得更好的性能。以前的研究已经试验了两种损失的直接组合，以获得两种损失的收益，并观察到改善的性能。然而，以前的研究打破了 logit 作为点进率的意义，这可能导致次优解。为了解决这个问题，我们提出了一种方法，可以联合优化排名和校准能力(简称 JRC)。JRC 通过对比不同标签样本的 logit 值来提高排序能力，并将预测概率约束为 logit 减法的函数。我们进一步表明，JRC 巩固了 logit 的解释，其中 logit 模型的联合分布。通过这样的解释，我们证明了 JRC 近似地优化了上下文混合判别生成目标。在公共和工业数据集上的实验和在线 A/B 测试表明，该方法提高了排序和校准能力。自2022年5月起，JRC 已被部署在阿里巴巴的展示广告平台上，并取得显著的性能改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Joint+Optimization+of+Ranking+and+Calibration+with+Contextualized+Hybrid+Model)|0|
|[Workplace Recommendation with Temporal Network Objectives](https://doi.org/10.1145/3580305.3599932)|Kiran Tomlinson, Jennifer Neville, Longqi Yang, Mengting Wan, Cao Lu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Workplace+Recommendation+with+Temporal+Network+Objectives)|0|
|[Experimentation Platforms Meet Reinforcement Learning: Bayesian Sequential Decision-Making for Continuous Monitoring](https://doi.org/10.1145/3580305.3599818)|Runzhe Wan, Yu Liu, James McQueen, Doug Hains, Rui Song|Amazon|With the growing needs of online A/B testing to support the innovation in industry, the opportunity cost of running an experiment becomes non-negligible. Therefore, there is an increasing demand for an efficient continuous monitoring service that allows early stopping when appropriate. Classic statistical methods focus on hypothesis testing and are mostly developed for traditional high-stake problems such as clinical trials, while experiments at online service companies typically have very different features and focuses. Motivated by the real needs, in this paper, we introduce a novel framework that we developed in Amazon to maximize customer experience and control opportunity cost. We formulate the problem as a Bayesian optimal sequential decision making problem that has a unified utility function. We discuss extensively practical design choices and considerations. We further introduce how to solve the optimal decision rule via Reinforcement Learning and scale the solution. We show the effectiveness of this novel approach compared with existing methods via a large-scale meta-analysis on experiments in Amazon.|随着支持行业创新的在线 A/B 测试需求的不断增长，运行一个实验的机会成本变得不可忽视。因此，人们越来越需要一种有效的连续监测服务，以便能够在适当的时候提早停止。经典的统计方法侧重于假设检验，主要针对传统的高风险问题，如临床试验，而在线服务公司的实验通常具有非常不同的特点和重点。本文从实际需求出发，介绍了我们在亚马逊开发的一个新的框架，以最大限度地提高客户体验和控制机会成本。将该问题表示为一个具有统一效用函数的贝叶斯最优序贯决策问题。我们广泛讨论实用的设计选择和考虑因素。我们进一步介绍了如何通过强化学习和规模求解最优决策规则。我们通过对亚马逊上的实验进行大规模的荟萃分析，证明了这种新方法与现有方法相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Experimentation+Platforms+Meet+Reinforcement+Learning:+Bayesian+Sequential+Decision-Making+for+Continuous+Monitoring)|0|
|[BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction](https://doi.org/10.1145/3580305.3599780)|Dong Wang, Kavé Salamatian, Yunqing Xia, Weiwei Deng, Qi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BERT4CTR:+An+Efficient+Framework+to+Combine+Pre-trained+Language+Model+with+Non-textual+Features+for+CTR+Prediction)|0|
|[Macular: A Multi-Task Adversarial Framework for Cross-Lingual Natural Language Understanding](https://doi.org/10.1145/3580305.3599864)|Haoyu Wang, Yaqing Wang, Feijie Wu, Hongfei Xue, Jing Gao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Macular:+A+Multi-Task+Adversarial+Framework+for+Cross-Lingual+Natural+Language+Understanding)|0|
|[ECGGAN: A Framework for Effective and Interpretable Electrocardiogram Anomaly Detection](https://doi.org/10.1145/3580305.3599812)|Huazhang Wang, Zhaojing Luo, James W. L. Yip, Chuyang Ye, Meihui Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ECGGAN:+A+Framework+for+Effective+and+Interpretable+Electrocardiogram+Anomaly+Detection)|0|
|[Fresh Content Needs More Attention: Multi-funnel Fresh Content Recommendation](https://doi.org/10.1145/3580305.3599826)|Jianling Wang, Haokai Lu, Sai Zhang, Bart N. Locanthi, Haoting Wang, Dylan Greaves, Benjamin Lipshitz, Sriraj Badam, Ed H. Chi, Cristos J. Goodrow, SuLin Wu, Lexi Baugher, Minmin Chen|Google|Recommendation system serves as a conduit connecting users to an incredibly large, diverse and ever growing collection of contents. In practice, missing information on fresh (and tail) contents needs to be filled in order for them to be exposed and discovered by their audience. We here share our success stories in building a dedicated fresh content recommendation stack on a large commercial platform. To nominate fresh contents, we built a multi-funnel nomination system that combines (i) a two-tower model with strong generalization power for coverage, and (ii) a sequence model with near real-time update on user feedback for relevance. The multi-funnel setup effectively balances between coverage and relevance. An in-depth study uncovers the relationship between user activity level and their proximity toward fresh contents, which further motivates a contextual multi-funnel setup. Nominated fresh candidates are then scored and ranked by systems considering prediction uncertainty to further bootstrap content with less exposure. We evaluate the benefits of the dedicated fresh content recommendation stack, and the multi-funnel nomination system in particular, through user corpus co-diverted live experiments. We conduct multiple rounds of live experiments on a commercial platform serving billion of users demonstrating efficacy of our proposed methods.|推荐系统作为一个管道，将用户连接到一个极其庞大、多样化和不断增长的内容集合。在实践中，需要填补关于新鲜(和尾部)内容的缺失信息，以便它们被观众暴露和发现。我们在这里分享我们在一个大型商业平台上建立一个专门的新内容推荐堆栈的成功故事。为了提名新的内容，我们建立了一个多漏斗提名系统，该系统结合了(i)一个具有很强覆盖泛化能力的双塔模型和(ii)一个具有近实时更新用户反馈相关性的序列模型。多漏斗设置有效地平衡了覆盖率和相关性。深入的研究揭示了用户活动水平与其接近新鲜内容之间的关系，进一步激发了上下文多漏斗设置。提名的新鲜候选人，然后得分和排名的系统考虑预测不确定性，以进一步引导内容，较少的曝光。我们通过用户语料库共转向的现场实验，评估了专用新鲜内容推荐堆栈，特别是多漏斗提名系统的优点。我们在一个为数十亿用户服务的商业平台上进行多轮实验，证明我们提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fresh+Content+Needs+More+Attention:+Multi-funnel+Fresh+Content+Recommendation)|0|
|[Contrastive Learning of Stress-specific Word Embedding for Social Media based Stress Detection](https://doi.org/10.1145/3580305.3599795)|Xin Wang, Huijun Zhang, Lei Cao, Kaisheng Zeng, Qi Li, Ningyun Li, Ling Feng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Learning+of+Stress-specific+Word+Embedding+for+Social+Media+based+Stress+Detection)|0|
|[RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads](https://doi.org/10.1145/3580305.3599900)|Penghui Wei, Yongqiang Chen, Shaoguo Liu, Liang Wang, Bo Zheng|Alibaba Group|To increase brand awareness, many advertisers conclude contracts with advertising platforms to purchase traffic and then deliver advertisements to target audiences. In a whole delivery period, advertisers usually desire a certain impression count for the ads, and they also expect that the delivery performance is as good as possible (e.g., obtaining high click-through rate). Advertising platforms employ pacing algorithms to satisfy the demands via adjusting the selection probabilities to traffic requests in real-time. However, the delivery procedure is also affected by the strategies from publishers, which cannot be controlled by advertising platforms. Preloading is a widely used strategy for many types of ads (e.g., video ads) to make sure that the response time for displaying after a traffic request is legitimate, which results in delayed impression phenomenon. Traditional pacing algorithms cannot handle the preloading nature well because they rely on immediate feedback signals, and may fail to guarantee the demands from advertisers.   In this paper, we focus on a new research problem of impression pacing for preloaded ads, and propose a Reinforcement Learning To Pace framework RLTP. It learns a pacing agent that sequentially produces selection probabilities in the whole delivery period. To jointly optimize the two objectives of impression count and delivery performance, RLTP employs tailored reward estimator to satisfy the guaranteed impression count, penalize the over-delivery and maximize the traffic value. Experiments on large-scale industrial datasets verify that RLTP outperforms baseline pacing algorithms by a large margin. We have deployed the RLTP framework online to our advertising platform, and results show that it achieves significant uplift to core metrics including delivery completion rate and click-through rate.|为了提高品牌知名度，许多广告商与广告平台签订合同，购买流量，然后向目标受众投放广告。在整个投放期间，广告商通常希望广告能给人留下一定的印象，而且他们也希望投放的效果尽可能好(例如，获得较高的点进率)。广告平台采用节奏算法，通过实时调整流量请求的选择概率来满足需求。然而，传递过程也受到出版商策略的影响，而出版商策略又不受广告平台的控制。预加载是一种广泛使用的策略，许多类型的广告(如视频广告) ，以确保响应时间显示后的流量请求是合法的，这导致了延迟印象现象。传统的节奏算法不能很好地处理预载性质，因为它们依赖于即时反馈信号，可能无法保证来自广告商的需求。在这篇文章中，我们关注一个新的研究问题——预装广告的印象节奏，并提出了一个强化学习到节奏的框架 RLTP。它学习一种起搏剂，该起搏剂在整个交付期间依次产生选择概率。为了共同优化印象计数和传递性能这两个目标，RLTP 使用定制的报酬估计器来满足保证的印象计数，惩罚超额传递和最大化流量价值。在大规模工业数据集上的实验证明，RLTP 算法的性能优于基线起搏算法。我们已经在我们的广告平台上部署了 RLTP 框架，结果显示它实现了包括交付完成率和点进率在内的核心指标的显著提升。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RLTP:+Reinforcement+Learning+to+Pace+for+Delayed+Impression+Modeling+in+Preloaded+Ads)|0|
|[Multi-channel Integrated Recommendation with Exposure Constraints](https://doi.org/10.1145/3580305.3599868)|Yue Xu, Qijie Shen, Jianwen Yin, Zengde Deng, Dimin Wang, Hao Chen, Lixiang Lai, Tao Zhuang, Junfeng Ge|The Hong Kong Polytechnic University.; Cainiao Network.; Alibaba Group.|Integrated recommendation, which aims at jointly recommending heterogeneous items from different channels in a main feed, has been widely applied to various online platforms. Though attractive, integrated recommendation requires the ranking methods to migrate from conventional user-item models to the new user-channel-item paradigm in order to better capture users' preferences on both item and channel levels. Moreover, practical feed recommendation systems usually impose exposure constraints on different channels to ensure user experience. This leads to greater difficulty in the joint ranking of heterogeneous items. In this paper, we investigate the integrated recommendation task with exposure constraints in practical recommender systems. Our contribution is forth-fold. First, we formulate this task as a binary online linear programming problem and propose a two-layer framework named Multi-channel Integrated Recommendation with Exposure Constraints (MIREC) to obtain the optimal solution. Second, we propose an efficient online allocation algorithm to determine the optimal exposure assignment of different channels from a global view of all user requests over the entire time horizon. We prove that this algorithm reaches the optimal point under a regret bound of $ \mathcal{O}(\sqrt{T}) $ with linear complexity. Third, we propose a series of collaborative models to determine the optimal layout of heterogeneous items at each user request. The joint modeling of user interests, cross-channel correlation, and page context in our models aligns more with the browsing nature of feed products than existing models. Finally, we conduct extensive experiments on both offline datasets and online A/B tests to verify the effectiveness of MIREC. The proposed framework has now been implemented on the homepage of Taobao to serve the main traffic.|综合推荐是指在一个主要的推送平台上联合推荐来自不同渠道的异构项目，已广泛应用于各种在线平台。虽然综合推荐具有吸引力，但是它需要排名方法从传统的用户项目模型迁移到新的用户渠道项目范式，以便更好地捕捉用户在项目和渠道级别上的偏好。此外，实际的饲料推荐系统通常对不同的渠道施加暴露约束，以确保用户体验。这导致了异构项目联合排序的更大困难。本文研究了实际推荐系统中具有曝光约束的集成推荐任务。我们的贡献是四倍。首先，我们将这个任务表述为一个二进制在线线性规划问题，并提出一个名为多通道暴露约束综合推荐(MIREC)的两层架构来获得最优解。其次，我们提出了一个有效的在线分配算法，从全局的角度来确定不同信道在整个时间范围内的最佳曝光分配。证明了该算法在线性复杂度为 $数学{ O }(sqrt { T }) $的遗憾界下达到最优点。第三，我们提出了一系列的协作模型，以确定在每个用户请求的异构项目的最佳布局。在我们的模型中，用户兴趣、跨通道相关性和页面上下文的联合建模比现有模型更符合饲料产品的浏览特性。最后，我们对离线数据集和在线 A/B 测试进行了广泛的实验，以验证 MIREC 的有效性。这个建议框架现已在淘宝网的主页上实施，以服务于主要流量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-channel+Integrated+Recommendation+with+Exposure+Constraints)|0|
|[Interactive Generalized Additive Model and Its Applications in Electric Load Forecasting](https://doi.org/10.1145/3580305.3599848)|Linxiao Yang, Rui Ren, Xinyue Gu, Liang Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interactive+Generalized+Additive+Model+and+Its+Applications+in+Electric+Load+Forecasting)|0|
|[UA-FedRec: Untargeted Attack on Federated News Recommendation](https://doi.org/10.1145/3580305.3599923)|Jingwei Yi, Fangzhao Wu, Bin Zhu, Jing Yao, Zhulin Tao, Guangzhong Sun, Xing Xie|; University of Science and Technology of China; Microsoft Research Asia|News recommendation is critical for personalized news distribution. Federated news recommendation enables collaborative model learning from many clients without sharing their raw data. It is promising for privacy-preserving news recommendation. However, the security of federated news recommendation is still unclear. In this paper, we study this problem by proposing an untargeted attack called UA-FedRec. By exploiting the prior knowledge of news recommendation and federated learning, UA-FedRec can effectively degrade the model performance with a small percentage of malicious clients. First, the effectiveness of news recommendation highly depends on user modeling and news modeling. We design a news similarity perturbation method to make representations of similar news farther and those of dissimilar news closer to interrupt news modeling, and propose a user model perturbation method to make malicious user updates in opposite directions of benign updates to interrupt user modeling. Second, updates from different clients are typically aggregated by weighted-averaging based on their sample sizes. We propose a quantity perturbation method to enlarge sample sizes of malicious clients in a reasonable range to amplify the impact of malicious updates. Extensive experiments on two real-world datasets show that UA-FedRec can effectively degrade the accuracy of existing federated news recommendation methods, even when defense is applied. Our study reveals a critical security issue in existing federated news recommendation systems and calls for research efforts to address the issue.|新闻推荐对个性化新闻发布至关重要。联合新闻推荐使得许多客户能够在不共享原始数据的情况下进行协作模型学习。它对于保护隐私的新闻推荐来说是很有前途的。然而，联邦新闻推荐的安全性仍不清楚。在本文中，我们通过提出一种称为 UA-FedRec 的非目标攻击来研究这个问题。通过利用新闻推荐和联邦学习的先验知识，UA-FedRec 能够有效地降低小比例恶意客户端的模型性能。首先，新闻推荐的有效性很大程度上取决于用户建模和新闻建模。设计了一种新闻相似性摄动方法，使相似新闻和不同新闻的表示更接近于中断新闻建模，提出了一种用户模型摄动方法，使恶意用户在良性更新的相反方向更新，以中断用户建模。其次，来自不同客户端的更新通常根据样本大小进行加权平均。我们提出了一种数量扰动方法，在合理的范围内扩大恶意客户端的样本量，以放大恶意更新的影响。在两个实际数据集上的大量实验表明，UA-FedRec 能够有效地降低现有联邦新闻推荐方法的准确性，即使在采用防御策略的情况下也是如此。我们的研究揭示了现有联邦新闻推荐系统中的一个关键安全问题，并呼吁研究人员努力解决这个问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UA-FedRec:+Untargeted+Attack+on+Federated+News+Recommendation)|0|
|[Group-based Fraud Detection Network on e-Commerce Platforms](https://doi.org/10.1145/3580305.3599836)|Jianke Yu, Hanchen Wang, Xiaoyang Wang, Zhao Li, Lu Qin, Wenjie Zhang, Jian Liao, Ying Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Group-based+Fraud+Detection+Network+on+e-Commerce+Platforms)|0|
|[Commonsense Knowledge Graph towards Super APP and Its Applications in Alipay](https://doi.org/10.1145/3580305.3599791)|Xiaoling Zang, Binbin Hu, Jun Chu, Zhiqiang Zhang, Guannan Zhang, Jun Zhou, Wenliang Zhong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Commonsense+Knowledge+Graph+towards+Super+APP+and+Its+Applications+in+Alipay)|0|
|[Revisiting Neural Retrieval on Accelerators](https://doi.org/10.1145/3580305.3599897)|Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, Xing Liu|Meta Platforms, Inc.|Retrieval finds a small number of relevant candidates from a large corpus for information retrieval and recommendation applications. A key component of retrieval is to model (user, item) similarity, which is commonly represented as the dot product of two learned embeddings. This formulation permits efficient inference, commonly known as Maximum Inner Product Search (MIPS). Despite its popularity, dot products cannot capture complex user-item interactions, which are multifaceted and likely high rank. We hence examine non-dot-product retrieval settings on accelerators, and propose \textit{mixture of logits} (MoL), which models (user, item) similarity as an adaptive composition of elementary similarity functions. This new formulation is expressive, capable of modeling high rank (user, item) interactions, and further generalizes to the long tail. When combined with a hierarchical retrieval strategy, \textit{h-indexer}, we are able to scale up MoL to 100M corpus on a single GPU with latency comparable to MIPS baselines. On public datasets, our approach leads to uplifts of up to 77.3\% in hit rate (HR). Experiments on a large recommendation surface at Meta showed strong metric gains and reduced popularity bias, validating the proposed approach's performance and improved generalization.|Retrieval 从一个大型语料库中为信息检索和推荐应用程序找到少量相关的候选人。检索的一个关键组成部分是模型(用户，项目)的相似性，这是通常表示为点积的两个学习嵌入。这个公式允许有效的推理，通常称为最大内积搜索(MIPS)。尽管广受欢迎，点产品不能捕捉复杂的用户项目交互，这是多方面的，可能排名很高。因此，我们研究了加速器上的非点积检索设置，并提出了 text { mix of logits }(MoL) ，它将(用户，项目)相似度建模为基本相似度函数的自适应组合。这个新的公式是有表现力的，能够建模高级别(用户，项目)的交互，并进一步推广到长尾。当结合分层检索策略 texttit { h-indexer }时，我们能够在单个 GPU 上扩展 MoL 到100M 语料库，延迟与 MIPS 基线相当。在公共数据集上，我们的方法导致命中率(HR)提高高达77.3% 。在 Meta 的一个大型推荐面上进行的实验表明，该方法具有很强的度量增益和较小的普及偏差，验证了该方法的性能和改进的泛化能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Neural+Retrieval+on+Accelerators)|0|
|[Constrained Social Community Recommendation](https://doi.org/10.1145/3580305.3599793)|Xingyi Zhang, Shuliang Xu, Wenqing Lin, Sibo Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Constrained+Social+Community+Recommendation)|0|
|[Modeling Dual Period-Varying Preferences for Takeaway Recommendation](https://doi.org/10.1145/3580305.3599866)|Yuting Zhang, Yiqing Wu, Ran Le, Yongchun Zhu, Fuzhen Zhuang, Ruidong Han, Xiang Li, Wei Lin, Zhulin An, Yongjun Xu|Meituan; Institute of Computing Technology, Chinese Academy of Sciences; Unaffiliated; Institute of Artificial Intelligence, Beihang University|Takeaway recommender systems, which aim to accurately provide stores that offer foods meeting users' interests, have served billions of users in our daily life. Different from traditional recommendation, takeaway recommendation faces two main challenges: (1) Dual Interaction-Aware Preference Modeling. Traditional recommendation commonly focuses on users' single preferences for items while takeaway recommendation needs to comprehensively consider users' dual preferences for stores and foods. (2) Period-Varying Preference Modeling. Conventional recommendation generally models continuous changes in users' preferences from a session-level or day-level perspective. However, in practical takeaway systems, users' preferences vary significantly during the morning, noon, night, and late night periods of the day. To address these challenges, we propose a Dual Period-Varying Preference modeling (DPVP) for takeaway recommendation. Specifically, we design a dual interaction-aware module, aiming to capture users' dual preferences based on their interactions with stores and foods. Moreover, to model various preferences in different time periods of the day, we propose a time-based decomposition module as well as a time-aware gating mechanism. Extensive offline and online experiments demonstrate that our model outperforms state-of-the-art methods on real-world datasets and it is capable of modeling the dual period-varying preferences. Moreover, our model has been deployed online on Meituan Takeaway platform, leading to an average improvement in GMV (Gross Merchandise Value) of 0.70%.|外卖推荐系统，旨在准确地提供商店，提供符合用户兴趣的食品，已服务于数十亿用户在我们的日常生活。与传统的推荐不同，外卖推荐面临着两个主要挑战: (1)双交互感知偏好建模。传统的推荐方式通常侧重于用户对商品的单一偏好，而外卖推荐方式则需要全面考虑用户对商店和食品的双重偏好。(变周期偏好模型。传统的推荐通常从会话级或日级的角度模拟用户偏好的持续变化。然而，在实际的外卖系统中，用户的偏好在白天的早上、中午、晚上和深夜各不相同。为了应对这些挑战，我们提出了一个外卖推荐的双周期变化偏好模型(DPVP)。具体来说，我们设计了一个双交互感知模块，旨在根据用户与商店和食物的交互来捕捉他们的双重偏好。此外，为了模拟一天中不同时段的不同偏好，我们提出了一个基于时间的分解模块以及一个时间感知的门控机制。大量的离线和在线实验表明，我们的模型优于现实世界数据集的最先进的方法，它能够建模的双周期变化的偏好。此外，我们的模型已经在美团外卖平台上进行了在线部署，导致平均商品总值(GMV)提高了0.70% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Dual+Period-Varying+Preferences+for+Takeaway+Recommendation)|0|
|[JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving](https://doi.org/10.1145/3580305.3599850)|Xin Zhao, Kun Zhou, Beichen Zhang, Zheng Gong, Zhipeng Chen, Yuanhang Zhou, JiRong Wen, Jing Sha, Shijin Wang, Cong Liu, Guoping Hu|iFLYTEK Research; School of Information, Renmin University of China; Gaoling School of Artificial Intelligence, Renmin University of China; iFLYTEK AI Research (Central China; iFLYTEK Research, State Key Laboratory of Cognitive Intelligence|Although pre-trained language models~(PLMs) have recently advanced the research progress in mathematical reasoning, they are not specially designed as a capable multi-task solver, suffering from high cost for multi-task deployment (\eg a model copy for a task) and inferior performance on complex mathematical problems in practical applications. To address these issues, in this paper, we propose \textbf{JiuZhang~2.0}, a unified Chinese PLM specially for multi-task mathematical problem solving. Our idea is to maintain a moderate-sized model and employ the \emph{cross-task knowledge sharing} to improve the model capacity in a multi-task setting. Specially, we construct a Mixture-of-Experts~(MoE) architecture for modeling mathematical text, so as to capture the common mathematical knowledge across tasks. For optimizing the MoE architecture, we design \emph{multi-task continual pre-training} and \emph{multi-task fine-tuning} strategies for multi-task adaptation. These training strategies can effectively decompose the knowledge from the task data and establish the cross-task sharing via expert networks. In order to further improve the general capacity of solving different complex tasks, we leverage large language models~(LLMs) as complementary models to iteratively refine the generated solution by our PLM, via in-context learning. Extensive experiments have demonstrated the effectiveness of our model.|尽管预先训练的语言模型 ~ (PLM)最近已经推动了数学推理的研究进展，但是它们并没有被特别设计成一个有能力的多任务解决者，因为多任务部署的高成本(例如一个任务的模型拷贝)和在实际应用中复杂数学问题的低表现。为了解决这些问题，本文提出了一个专门用于多任务数学问题求解的统一中文 PLM textbf {旧掌 ~ 2.0}。我们的想法是维持一个中等规模的模型，并采用跨任务知识共享的方法来提高模型在多任务环境下的能力。特别地，我们构建了一个专家混合模型，用于数学文本建模，以便跨任务获取常见的数学知识。为了优化教学体系结构，我们设计了多任务连续预训练和多任务微调策略来实现多任务自适应。这些训练策略可以有效地分解任务数据中的知识，并通过专家网络建立跨任务共享。为了进一步提高解决不同复杂任务的能力，我们利用大语言模型 ~ (LLM)作为补充模型，通过上下文学习的方法，迭代地完善 PLM 生成的解决方案。大量的实验证明了我们模型的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=JiuZhang+2.0:+A+Unified+Chinese+Pre-trained+Language+Model+for+Multi-task+Mathematical+Problem+Solving)|0|
|[ReLoop2: Building Self-Adaptive Recommendation Models via Responsive Error Compensation Loop](https://doi.org/10.1145/3580305.3599785)|Jieming Zhu, Guohao Cai, Junjie Huang, Zhenhua Dong, Ruiming Tang, Weinan Zhang|Shanghai Jiao Tong University; Huawei Noah’s Ark Lab|Industrial recommender systems face the challenge of operating in non-stationary environments, where data distribution shifts arise from evolving user behaviors over time. To tackle this challenge, a common approach is to periodically re-train or incrementally update deployed deep models with newly observed data, resulting in a continual training process. However, the conventional learning paradigm of neural networks relies on iterative gradient-based updates with a small learning rate, making it slow for large recommendation models to adapt. In this paper, we introduce ReLoop2, a self-correcting learning loop that facilitates fast model adaptation in online recommender systems through responsive error compensation. Inspired by the slow-fast complementary learning system observed in human brains, we propose an error memory module that directly stores error samples from incoming data streams. These stored samples are subsequently leveraged to compensate for model prediction errors during testing, particularly under distribution shifts. The error memory module is designed with fast access capabilities and undergoes continual refreshing with newly observed data samples during the model serving phase to support fast model adaptation. We evaluate the effectiveness of ReLoop2 on three open benchmark datasets as well as a real-world production dataset. The results demonstrate the potential of ReLoop2 in enhancing the responsiveness and adaptiveness of recommender systems operating in non-stationary environments.|工业推荐系统面临着在非平稳环境下运行的挑战，数据分布随着时间的推移而发生变化。为了应对这一挑战，一种常见的方法是定期用新观测数据重新训练或增量更新已部署的深度模型，从而形成持续的训练过程。然而，传统的神经网络学习范式依赖于迭代的基于梯度的更新，学习速度很小，使得大型推荐模型的适应速度变慢。本文介绍了 ReLoop2，一种通过响应误差补偿实现在线推荐系统中模型快速自适应的自校正学习循环。受到在人脑中观察到的慢-快互补学习系统的启发，我们提出了一个错误记忆模块，它直接存储来自输入数据流的错误样本。这些存储的样本随后被用来补偿测试期间的模型预测错误，特别是在分布变化的情况下。错误存储模块设计具有快速访问能力，并在模型服务阶段不断刷新新观察到的数据样本，以支持快速模型适应。我们评估了 ReLoop2在三个开放基准数据集和一个真实生产数据集上的有效性。结果表明，ReLoop2在提高非平稳环境中运行的推荐系统的响应能力和适应能力方面具有潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReLoop2:+Building+Self-Adaptive+Recommendation+Models+via+Responsive+Error+Compensation+Loop)|0|
|[Trustworthy Recommender Systems: Foundations and Frontiers](https://doi.org/10.1145/3580305.3599575)|Wenqi Fan, Xiangyu Zhao, Lin Wang, Xiao Chen, Jingtong Gao, Qidong Liu, Shijie Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trustworthy+Recommender+Systems:+Foundations+and+Frontiers)|0|
|[Mining Electronic Health Records for Real-World Evidence](https://doi.org/10.1145/3580305.3599566)|Chengxi Zang, Weishen Pan, Fei Wang|Department of Clinical Pharmacy and Toxicology, Leiden University Medical Center, Leiden, the Netherlands.|Real-world evidence can close the inferential gap between marketing authorization studies and clinical practice. However, the current standard for real-world data extraction from electronic health records (EHRs) for treatment evaluation is manual review (MR), which is time-consuming and laborious. Clinical Data Collector (CDC) is a novel natural language processing and text mining software tool for both structured and unstructured EHR data and only shows relevant EHR sections improving efficiency. We investigated CDC as a real-world data (RWD) collection method, through application of CDC queries for patient inclusion and information extraction on a cohort of patients with metastatic renal cell carcinoma (RCC) receiving systemic drug treatment. Baseline patient characteristics, disease characteristics, and treatment outcomes were extracted and these were compared with MR for validation. One hundred patients receiving 175 treatments were included using CDC, which corresponded to 99% with MR. Calculated median overall survival was 21.7 months (95% confidence interval (CI) 18.7-24.8) vs. 21.7 months (95% CI 18.6-24.8) and progression-free survival 8.9 months (95% CI 5.4-12.4) vs. 7.6 months (95% CI 5.7-9.4) for CDC vs. MR, respectively. Highest F1-score was found for cancer-related variables (88.1-100), followed by comorbidities (71.5-90.4) and adverse drug events (53.3-74.5), with most diverse scores on international metastatic RCC database criteria (51.4-100). Mean data collection time was 12 minutes (CDC) vs. 86 minutes (MR). In conclusion, CDC is a promising tool for retrieving RWD from EHRs because the correct patient population can be identified as well as relevant outcome data, such as overall survival and progression-free survival.|真实世界的证据可以缩小上市许可研究和临床实践之间的推断差距。然而，目前从电子健康记录(EHRs)中提取真实数据用于治疗评估的标准是人工审查(MR) ，这是一项费时费力的工作。临床数据采集器(CDC)是一种新型的自然语言处理和文本挖掘软件工具，用于结构化和非结构化 EHR 数据，只显示相关的 EHR 部分提高效率。我们研究了 CDC 作为一种现实世界数据(RWD)收集方法，通过应用 CDC 查询对患者进行纳入，并对接受全身药物治疗的转移性信息抽取(rCC)患者队列进行肾细胞癌分析。提取基线患者特征、疾病特征和治疗结果，并与 MR 进行比较验证。接受175次治疗的100名患者使用 CDC，其中99% 为 MR。计算的中位总生存期分别为21.7个月(95% 置信区间(CI)18.7-24.8)和21.7个月(95% CI 18.6-24.8)和无进展生存期分别为 CDC 和 MR 的8.9个月(95% CI 5.4-12.4)和7.6个月(95% CI 5.7-9.4)。发现癌症相关变量(88.1-100)的 F1评分最高，其次是合并症(71.5-90.4)和不良药物事件(53.3-74.5) ，国际转移性 RCC 数据库标准(51.4-100)。平均数据收集时间为12分钟(CDC)比86分钟(MR)。总之，CDC 是从 EHR 中检索 RWD 的有希望的工具，因为可以确定正确的患者人群以及相关的结果数据，如总生存期和无进展生存期。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+Electronic+Health+Records+for+Real-World+Evidence)|0|
|[EvalRS 2023: Well-Rounded Recommender Systems for Real-World Deployments](https://doi.org/10.1145/3580305.3599222)|Federico Bianchi, Patrick John Chia, Jacopo Tagliabue, Ciro Greco, Gabriel de Souza P. Moreira, Davide Eynard, Fahd Husain, Claudio Pomo||EvalRS aims to bring together practitioners from industry and academia to foster a debate on rounded evaluation of recommender systems, with a focus on real-world impact across a multitude of deployment scenarios. Recommender systems are often evaluated only through accuracy metrics, which fall short of fully characterizing their generalization capabilities and miss important aspects, such as fairness, bias, usefulness, informativeness. This workshop builds on the success of last year's workshop at CIKM, but with a broader scope and an interactive format.|EvalRS 旨在汇集来自行业和学术界的从业人员，促进关于全面评估推荐系统的辩论，重点是在多种部署情景下的现实世界影响。推荐系统往往只能通过精度指标进行评估，这些指标不能充分表征推荐系统的泛化能力，而且忽略了公平性、偏差性、有用性、信息性等重要方面。这个研讨会建立在去年 CIKM 研讨会的成功基础之上，但是范围更广，而且采用了交互式的形式。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EvalRS+2023:+Well-Rounded+Recommender+Systems+for+Real-World+Deployments)|0|
|[A Multi-stage Framework for Online Bonus Allocation Based on Constrained User Intent Detection](https://doi.org/10.1145/3580305.3599764)|Chao Wang, Xiaowei Shi, Shuai Xu, Zhe Wang, Zhiqiang Fan, Yan Feng, An You, Yu Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multi-stage+Framework+for+Online+Bonus+Allocation+Based+on+Constrained+User+Intent+Detection)|0|
|[LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias](https://doi.org/10.1145/3580305.3599402)|Mario Almagro, Emilio J. Almazán, Diego Ortego, David Jiménez||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LEA:+Improving+Sentence+Similarity+Robustness+to+Typos+Using+Lexical+Attention+Bias)|0|
|[IPOC: An Adaptive Interval Prediction Model based on Online Chasing and Conformal Inference for Large-Scale Systems](https://doi.org/10.1145/3580305.3599396)|Jiadong Chen, Yang Luo, Xiuqi Huang, Fuxin Jiang, Yangguang Shi, Tieying Zhang, Xiaofeng Gao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IPOC:+An+Adaptive+Interval+Prediction+Model+based+on+Online+Chasing+and+Conformal+Inference+for+Large-Scale+Systems)|0|
|[SketchPolymer: Estimate Per-item Tail Quantile Using One Sketch](https://doi.org/10.1145/3580305.3599505)|Jiarui Guo, Yisen Hong, Yuhan Wu, Yunfei Liu, Tong Yang, Bin Cui||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SketchPolymer:+Estimate+Per-item+Tail+Quantile+Using+One+Sketch)|0|
|[Unbiased Locally Private Estimator for Polynomials of Laplacian Variables](https://doi.org/10.1145/3580305.3599537)|Quentin Hillebrand, Vorapong Suppakitpaisarn, Tetsuo Shibuya||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Locally+Private+Estimator+for+Polynomials+of+Laplacian+Variables)|0|
|[Semantic Dissimilarity Guided Locality Preserving Projections for Partial Label Dimensionality Reduction](https://doi.org/10.1145/3580305.3599496)|Yuheng Jia, Jiahao Jiang, Yongheng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic+Dissimilarity+Guided+Locality+Preserving+Projections+for+Partial+Label+Dimensionality+Reduction)|0|
|[B2-Sampling: Fusing Balanced and Biased Sampling for Graph Contrastive Learning](https://doi.org/10.1145/3580305.3599262)|Mengyue Liu, Yun Lin, Jun Liu, Bohao Liu, Qinghua Zheng, Jin Song Dong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=B2-Sampling:+Fusing+Balanced+and+Biased+Sampling+for+Graph+Contrastive+Learning)|0|
|[DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication](https://doi.org/10.1145/3580305.3599314)|Igor Nunes, Mike Heddes, Pere Vergés, Danny Abraham, Alexander V. Veidenbaum, Alex Nicolau, Tony Givargis|University of California, Irvine|Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.|集合相似性度量是数据挖掘任务的一个核心方面。例如，为了删除 Web 搜索中的重复结果，通常的方法是查看所有页对之间的 Jaccard 索引。在社会网络分析中，一个著名的度量是阿达姆-阿达尔指数，广泛用于比较节点邻域集在预测链路的重要问题。然而，随着需要处理的数据量的增加，计算所有对之间的精确相似度是很困难的。在这种规模下工作的挑战促使人们研究集合相似度量的有效估计器。MinHash 和 SimHash 这两个最流行的估计器确实用于需要处理大量数据的应用程序，如文档删除重复数据和推荐系统。鉴于这些任务的重要性，提前估算的需求是显而易见的。我们提出了 DotHash，一个两个集合的交集大小的无偏估计。DotHash 可以用来估计 Jaccard 指数，据我们所知，DotHash 是第一种也可以估计 Adam-Adar 指数和一系列相关指标的方法。我们正式地定义了这个度量族，给出了估计误差概率的理论界限，并分析了它的经验性能。实验结果表明，在相同复杂度和相似比较时间的链路预测和重复文档检测方面，DotHash 比其他估计器具有更高的精度。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DotHash:+Estimating+Set+Similarity+Metrics+for+Link+Prediction+and+Document+Deduplication)|0|
|[Domain-Guided Spatio-Temporal Self-Attention for Egocentric 3D Pose Estimation](https://doi.org/10.1145/3580305.3599312)|Jinman Park, Kimathi Kaai, Saad Hossain, Norikatsu Sumi, Sirisha Rambhatla, Paul W. Fieguth||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Domain-Guided+Spatio-Temporal+Self-Attention+for+Egocentric+3D+Pose+Estimation)|0|
|[Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization](https://doi.org/10.1145/3580305.3599481)|Yunze Tong, Junkun Yuan, Min Zhang, Didi Zhu, Keli Zhang, Fei Wu, Kun Kuang|Noah’s Ark Lab, Huawei Technologies; Zhejiang University|Domain generalization (DG) is a prevalent problem in real-world applications, which aims to train well-generalized models for unseen target domains by utilizing several source domains. Since domain labels, i.e., which domain each data point is sampled from, naturally exist, most DG algorithms treat them as a kind of supervision information to improve the generalization performance. However, the original domain labels may not be the optimal supervision signal due to the lack of domain heterogeneity, i.e., the diversity among domains. For example, a sample in one domain may be closer to another domain, its original label thus can be the noise to disturb the generalization learning. Although some methods try to solve it by re-dividing domains and applying the newly generated dividing pattern, the pattern they choose may not be the most heterogeneous due to the lack of the metric for heterogeneity. In this paper, we point out that domain heterogeneity mainly lies in variant features under the invariant learning framework. With contrastive learning, we propose a learning potential-guided metric for domain heterogeneity by promoting learning variant features. Then we notice the differences between seeking variance-based heterogeneity and training invariance-based generalizable model. We thus propose a novel method called Heterogeneity-based Two-stage Contrastive Learning (HTCL) for the DG task. In the first stage, we generate the most heterogeneous dividing pattern with our contrastive metric. In the second stage, we employ an invariance-aimed contrastive learning by re-building pairs with the stable relation hinted by domains and classes, which better utilizes generated domain labels for generalization learning. Extensive experiments show HTCL better digs heterogeneity and yields great generalization performance.|领域广义化(DG)是现实应用中的一个普遍问题，其目的是利用多个源域来训练未知目标域的广义模型。由于领域标签(即每个数据点从哪个领域采样)的自然存在，大多数 DG 算法都将它们视为一种监督信息，以提高泛化性能。然而，由于缺乏领域异质性，即领域之间的差异性，原始的领域标签可能不是最佳的监督信号。例如，一个领域中的样本可能更接近另一个领域，其原始标签因此可能是噪声干扰推广学习。尽管有些方法试图通过重新划分域并应用新生成的划分模式来解决这个问题，但是由于缺乏对异构性的度量，所选择的模式可能不是最异构的。本文指出，在不变学习框架下，领域异质性主要表现在变异特征上。在对比学习的基础上，提出了一种基于学习势引导的领域异构度量方法。然后我们注意到基于方差的异质性寻求和基于训练不变性的可推广模型之间的区别。因此，我们提出了一种新的方法称为异质性为基础的两阶段对比学习(HTCL)的 DG 任务。在第一阶段，我们使用对比度量生成最不均匀的分割模式。在第二阶段，我们采用不变性对比学习方法，通过重新构建由领域和类提示的稳定关系的对，更好地利用生成的领域标签进行泛化学习。大量实验表明，HTCL 能够更好地挖掘异构性，并产生很好的泛化性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantitatively+Measuring+and+Contrastively+Exploring+Heterogeneity+for+Domain+Generalization)|0|
|[Grace: Graph Self-Distillation and Completion to Mitigate Degree-Related Biases](https://doi.org/10.1145/3580305.3599368)|Hui Xu, Liyao Xiang, Femke Huang, Yuting Weng, Ruijie Xu, Xinbing Wang, Chenghu Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Grace:+Graph+Self-Distillation+and+Completion+to+Mitigate+Degree-Related+Biases)|0|
|[DisasterNet: Causal Bayesian Networks with Normalizing Flows for Cascading Hazards Estimation from Satellite Imagery](https://doi.org/10.1145/3580305.3599807)|Xuechun Li, Paula M. Bürgi, Wei Ma, Hae Young Noh, David Jay Wald, Susu Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DisasterNet:+Causal+Bayesian+Networks+with+Normalizing+Flows+for+Cascading+Hazards+Estimation+from+Satellite+Imagery)|0|
|[Explicit Feature Interaction-aware Uplift Network for Online Marketing](https://doi.org/10.1145/3580305.3599820)|Dugang Liu, Xing Tang, Han Gao, Fuyuan Lyu, Xiuqiang He|FiT, Tencent; Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ; McGill University|As a key component in online marketing, uplift modeling aims to accurately capture the degree to which different treatments motivate different users, such as coupons or discounts, also known as the estimation of individual treatment effect (ITE). In an actual business scenario, the options for treatment may be numerous and complex, and there may be correlations between different treatments. In addition, each marketing instance may also have rich user and contextual features. However, existing methods still fall short in both fully exploiting treatment information and mining features that are sensitive to a particular treatment. In this paper, we propose an explicit feature interaction-aware uplift network (EFIN) to address these two problems. Our EFIN includes four customized modules: 1) a feature encoding module encodes not only the user and contextual features, but also the treatment features; 2) a self-interaction module aims to accurately model the user's natural response with all but the treatment features; 3) a treatment-aware interaction module accurately models the degree to which a particular treatment motivates a user through interactions between the treatment features and other features, i.e., ITE; and 4) an intervention constraint module is used to balance the ITE distribution of users between the control and treatment groups so that the model would still achieve a accurate uplift ranking on data collected from a non-random intervention marketing scenario. We conduct extensive experiments on two public datasets and one product dataset to verify the effectiveness of our EFIN. In addition, our EFIN has been deployed in a credit card bill payment scenario of a large online financial platform with a significant improvement.|作为在线营销的一个关键组成部分，提升模型旨在准确地捕捉不同的治疗激励不同的用户的程度，如优惠券或折扣，也被称为个体治疗效果(ITE)的估计。在实际的业务场景中，治疗的选择可能是多种多样和复杂的，不同治疗之间可能存在相关性。此外，每个营销实例还可能具有丰富的用户和上下文特性。然而，现有方法仍然不能充分利用对特定治疗敏感的治疗信息和挖掘特征。针对这两个问题，本文提出了一种显式的特征交互感知提升网络(EFIN)。我们的 EFIN 包括四个定制模块: 1)特征编码模块不仅编码用户和上下文特征，而且还编码治疗特征; 2)自我交互模块旨在准确地模拟除治疗特征以外的所有用户的自然反应; 3)治疗感知交互模块准确地模拟特定治疗通过治疗特征和其他特征之间的交互激励用户的程度，即 ITE; 和4)干预约束模块用于平衡控制和治疗组之间的用户 ITE 分布，以便该模型仍然能够实现从非随机干预营销场景收集的数据的准确提升排名。我们对两个公共数据集和一个产品数据集进行了广泛的实验，以验证我们的 EFIN 的有效性。此外，我们的 EFIN 已经部署在一个大型在线金融平台的信用卡账单支付场景中，并得到了显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explicit+Feature+Interaction-aware+Uplift+Network+for+Online+Marketing)|0|
|[Online Quality Prediction in Windshield Manufacturing using Data-Efficient Machine Learning](https://doi.org/10.1145/3580305.3599880)|Hasan Tercan, Tobias Meisen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Quality+Prediction+in+Windshield+Manufacturing+using+Data-Efficient+Machine+Learning)|0|
|[C-AOI: Contour-based Instance Segmentation for High-Quality Areas-of-Interest in Online Food Delivery Platform](https://doi.org/10.1145/3580305.3599786)|Yida Zhu, Liying Chen, Daping Xiong, Shuiping Chen, Fangxiao Du, Jinghua Hao, Renqing He, Zhizhao Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=C-AOI:+Contour-based+Instance+Segmentation+for+High-Quality+Areas-of-Interest+in+Online+Food+Delivery+Platform)|0|
|[Addressing Bias and Fairness in Machine Learning: A Practical Guide and Hands-on Tutorial](https://doi.org/10.1145/3580305.3599180)|Rayid Ghani, Kit T. Rodolfa, Pedro Saleiro, Sérgio M. Jesus||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Addressing+Bias+and+Fairness+in+Machine+Learning:+A+Practical+Guide+and+Hands-on+Tutorial)|0|
|[Causal Inference and Machine Learning in Practice: Use Cases for Product, Brand, Policy and Beyond](https://doi.org/10.1145/3580305.3599221)|JeongYoon Lee, Yifeng Wu, Keith Battocchi, Fabio Vera, Zhenyu Zhao, Totte Harinen, Jing Pan, Huigang Chen, Zeyu Zheng, Chu Wang, Yingfei Wang, Xinwei Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Inference+and+Machine+Learning+in+Practice:+Use+Cases+for+Product,+Brand,+Policy+and+Beyond)|0|
|[Sketch-Based Anomaly Detection in Streaming Graphs](https://doi.org/10.1145/3580305.3599504)|Siddharth Bhatia, Mohit Wadhwa, Kenji Kawaguchi, Neil Shah, Philip S. Yu, Bryan Hooi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sketch-Based+Anomaly+Detection+in+Streaming+Graphs)|0|
|[On Hierarchical Disentanglement of Interactive Behaviors for Multimodal Spatiotemporal Data with Incompleteness](https://doi.org/10.1145/3580305.3599448)|Jiayi Chen, Aidong Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Hierarchical+Disentanglement+of+Interactive+Behaviors+for+Multimodal+Spatiotemporal+Data+with+Incompleteness)|0|
|[Multiplex Heterogeneous Graph Neural Network with Behavior Pattern Modeling](https://doi.org/10.1145/3580305.3599441)|Chaofan Fu, Guanjie Zheng, Chao Huang, Yanwei Yu, Junyu Dong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multiplex+Heterogeneous+Graph+Neural+Network+with+Behavior+Pattern+Modeling)|0|
|[Pyramid Graph Neural Network: A Graph Sampling and Filtering Approach for Multi-scale Disentangled Representations](https://doi.org/10.1145/3580305.3599478)|Haoyu Geng, Chao Chen, Yixuan He, Gang Zeng, Zhaobing Han, Hua Chai, Junchi Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pyramid+Graph+Neural+Network:+A+Graph+Sampling+and+Filtering+Approach+for+Multi-scale+Disentangled+Representations)|0|
|[Detecting Interference in Online Controlled Experiments with Increasing Allocation](https://doi.org/10.1145/3580305.3599308)|Kevin Han, Shuangning Li, Jialiang Mao, Han Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Interference+in+Online+Controlled+Experiments+with+Increasing+Allocation)|0|
|[CLUR: Uncertainty Estimation for Few-Shot Text Classification with Contrastive Learning](https://doi.org/10.1145/3580305.3599276)|Jianfeng He, Xuchao Zhang, Shuo Lei, Abdulaziz Alhamadani, Fanglan Chen, Bei Xiao, ChangTien Lu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLUR:+Uncertainty+Estimation+for+Few-Shot+Text+Classification+with+Contrastive+Learning)|0|
|[Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting](https://doi.org/10.1145/3580305.3599467)|Christine Herlihy, Aviva Prins, Aravind Srinivasan, John P. Dickerson|University of Maryland|Restless and collapsing bandits are commonly used to model constrained resource allocation in settings featuring arms with action-dependent transition probabilities, such as allocating health interventions among patients [Whittle, 1988; Mate et al., 2020]. However, state-of-the-art Whittle-index-based approaches to this planning problem either do not consider fairness among arms, or incentivize fairness without guaranteeing it [Mate et al., 2021]. Additionally, their optimality guarantees only apply when arms are indexable and threshold-optimal. We demonstrate that the incorporation of hard fairness constraints necessitates the coupling of arms, which undermines the tractability, and by extension, indexability of the problem. We then introduce ProbFair, a probabilistically fair stationary policy that maximizes total expected reward and satisfies the budget constraint, while ensuring a strictly positive lower bound on the probability of being pulled at each timestep. We evaluate our algorithm on a real-world application, where interventions support continuous positive airway pressure (CPAP) therapy adherence among obstructive sleep apnea (OSA) patients, as well as simulations on a broader class of synthetic transition matrices.|不安分和崩溃的土匪通常用于在具有行动依赖性转换概率的武器的环境中建模有限的资源分配，例如在患者中分配卫生干预措施[ Whittle，1988; Mate 等，2020]。然而，基于惠特尔指数的最先进的方法来解决这个规划问题，要么不考虑武器之间的公平，要么在不保证公平的情况下激励公平[ Mate et al。 ，2021]。此外，它们的最优性保证只适用于武器是可索引和阈值最优的情况。我们证明了硬公平约束的加入使得武器的耦合成为必然，这破坏了问题的易处理性，进而也破坏了问题的可索引性。然后，我们引入了“概率公平”，这是一种概率公平的静态策略，它能使预期回报总额最大化并满足预算线，同时确保在每个时间步骤中被拉出的概率有一个严格的正下限。我们评估我们的算法在现实世界中的应用，其中干预支持连续式阳压唿吸机(CPAP)治疗在阻塞性睡眠唿吸暂停(OSA)患者中的依从性，以及在更广泛类别的合成转换矩阵上的模拟。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Planning+to+Fairly+Allocate:+Probabilistic+Fairness+in+the+Restless+Bandit+Setting)|0|
|[Similarity Preserving Adversarial Graph Contrastive Learning](https://doi.org/10.1145/3580305.3599503)|Yeonjun In, Kanghoon Yoon, Chanyoung Park|KAIST|Recent works demonstrate that GNN models are vulnerable to adversarial attacks, which refer to imperceptible perturbation on the graph structure and node features. Among various GNN models, graph contrastive learning (GCL) based methods specifically suffer from adversarial attacks due to their inherent design that highly depends on the self-supervision signals derived from the original graph, which however already contains noise when the graph is attacked. To achieve adversarial robustness against such attacks, existing methods adopt adversarial training (AT) to the GCL framework, which considers the attacked graph as an augmentation under the GCL framework. However, we find that existing adversarially trained GCL methods achieve robustness at the expense of not being able to preserve the node feature similarity. In this paper, we propose a similarity-preserving adversarial graph contrastive learning (SP-AGCL) framework that contrasts the clean graph with two auxiliary views of different properties (i.e., the node similarity-preserving view and the adversarial view). Extensive experiments demonstrate that SP-AGCL achieves a competitive performance on several downstream tasks, and shows its effectiveness in various scenarios, e.g., a network with adversarial attacks, noisy labels, and heterophilous neighbors. Our code is available at https://github.com/yeonjun-in/torch-SP-AGCL.|最近的工作表明，GNN 模型是脆弱的对手攻击，这是指不可察觉的扰动图结构和节点特征。在各种 GNN 模型中，基于图形对比学习(GCL)的方法由于其固有的设计，高度依赖于来自原始图形的自我监督信号，而这些信号在图形受到攻击时已经含有噪声，因此特别容易受到攻击。为了实现对抗这种攻击的鲁棒性，现有的方法对 GCL 框架采用了对抗训练(AT) ，该框架将被攻击图视为 GCL 框架下的一种增强。然而，我们发现现有的对抗训练的 GCL 方法在不能保持节点特征相似性的前提下达到了鲁棒性。本文提出了一个保持相似性的对抗图对比学习(SP-AGCL)框架，该框架将干净图与具有不同性质的两个辅助视图(即节点相似性保持视图和对抗视图)进行对比。广泛的实验表明，SP-AGCL 在几个下游任务上取得了有竞争力的性能，并且在多种情况下显示了其有效性，例如，一个具有对抗性攻击、噪声标签和异质邻居的网络。我们的代码可以在 https://github.com/yeonjun-in/torch-sp-agcl 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Similarity+Preserving+Adversarial+Graph+Contrastive+Learning)|0|
|[Fast and Accurate Dual-Way Streaming PARAFAC2 for Irregular Tensors - Algorithm and Application](https://doi.org/10.1145/3580305.3599342)|JunGi Jang, Jeongyoung Lee, Yongchan Park, U Kang|Seoul National University|How can we efficiently and accurately analyze an irregular tensor in a dual-way streaming setting where the sizes of two dimensions of the tensor increase over time? What types of anomalies are there in the dual-way streaming setting? An irregular tensor is a collection of matrices whose column lengths are the same while their row lengths are different. In a dual-way streaming setting, both new rows of existing matrices and new matrices arrive over time. PARAFAC2 decomposition is a crucial tool for analyzing irregular tensors. Although real-time analysis is necessary in the dual-way streaming, static PARAFAC2 decomposition methods fail to efficiently work in this setting since they perform PARAFAC2 decomposition for accumulated tensors whenever new data arrive. Existing streaming PARAFAC2 decomposition methods work in a limited setting and fail to handle new rows of matrices efficiently. In this paper, we propose Dash, an efficient and accurate PARAFAC2 decomposition method working in the dual-way streaming setting. When new data are given, Dash efficiently performs PARAFAC2 decomposition by carefully dividing the terms related to old and new data and avoiding naive computations involved with old data. Furthermore, applying a forgetting factor makes Dash follow recent movements. Extensive experiments show that Dash achieves up to 14.0x faster speed than existing PARAFAC2 decomposition methods for newly arrived data. We also provide discoveries for detecting anomalies in real-world datasets, including Subprime Mortgage Crisis and COVID-19.|我们如何才能有效和准确地分析一个不规则张量在双向流设置，其中张量的二维尺寸随着时间的推移增加？在双向流设置中有哪些类型的异常？不规则张量是列长相同而行长不同的矩阵集合。在双向流设置中，现有矩阵的新行和新矩阵都会随着时间的推移到达。PARAFAC2分解是分析不规则张量的重要工具。尽管实时分析在双向流中是必需的，但是静态 PARAFAC2分解方法在这种情况下无法有效工作，因为每当新数据到达时，它们都会对累积的张量执行 PARAFAC2分解。现有的流 PARAFAC2分解方法在有限的设置下工作，无法有效地处理新的矩阵行。本文提出了一种高效、准确的双向流设置 PARAFAC2分解方法 Dash。当给定新数据时，Dash 通过仔细划分与旧数据和新数据相关的术语并避免涉及旧数据的幼稚计算，有效地执行 PARAFAC2分解。此外，应用遗忘因子使达什跟随最近的动作。大量的实验表明，Dash 对新到达的数据的分解速度比现有的 PARAFAC2分解方法快14.0倍。我们还提供发现，以检测现实世界数据集中的异常，包括次贷危机和2019冠状病毒疾病。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+and+Accurate+Dual-Way+Streaming+PARAFAC2+for+Irregular+Tensors+-+Algorithm+and+Application)|0|
|[Predicting Information Pathways Across Online Communities](https://doi.org/10.1145/3580305.3599470)|Yiqiao Jin, YeonChang Lee, Kartik Sharma, Meng Ye, Karan Sikka, Ajay Divakaran, Srijan Kumar|SRI International; Georgia Institute of Technology|The problem of community-level information pathway prediction (CLIPP) aims at predicting the transmission trajectory of content across online communities. A successful solution to CLIPP holds significance as it facilitates the distribution of valuable information to a larger audience and prevents the proliferation of misinformation. Notably, solving CLIPP is non-trivial as inter-community relationships and influence are unknown, information spread is multi-modal, and new content and new communities appear over time. In this work, we address CLIPP by collecting large-scale, multi-modal datasets to examine the diffusion of online YouTube videos on Reddit. We analyze these datasets to construct community influence graphs (CIGs) and develop a novel dynamic graph framework, INPAC (Information Pathway Across Online Communities), which incorporates CIGs to capture the temporal variability and multi-modal nature of video propagation across communities. Experimental results in both warm-start and cold-start scenarios show that INPAC outperforms seven baselines in CLIPP.|社区层面的信息路径预测(CLIPP)问题旨在预测内容在网络社区之间的传播轨迹。CLIPP 的成功解决方案具有重要意义，因为它有助于向更多的受众传播有价值的信息，并防止错误信息的扩散。值得注意的是，解决 CLIPP 是非常重要的，因为社区之间的关系和影响是未知的，信息传播是多模式的，新的内容和新的社区随着时间的推移出现。在这项工作中，我们通过收集大规模的多模态数据集来检查在线 YouTube 视频在 Reddit 上的传播，从而解决 CLIPP 问题。我们分析这些数据集来构建社区影响图(CIGs) ，并开发一种新的动态图框架 INPAC (在线社区信息路径) ，其结合 CIGs 来捕获跨社区视频传播的时间变异性和多模态性质。在热启动和冷启动两种情况下的实验结果表明，INPAC 的性能优于 CLIPP 中的七个基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+Information+Pathways+Across+Online+Communities)|0|
|[Task Relation-aware Continual User Representation Learning](https://doi.org/10.1145/3580305.3599516)|Sein Kim, Namkyeong Lee, Donghyun Kim, MinChul Yang, Chanyoung Park|KAIST; NAVER Corporation|User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number of learned tasks increases while capturing the relationship between the tasks. The main idea is to introduce an embedding for each task, i.e., task embedding, which is utilized to generate task-specific soft masks that not only allow the entire model parameters to be updated until the end of training sequence, but also facilitate the relationship between the tasks to be captured. Moreover, we introduce a novel knowledge retention module with pseudo-labeling strategy that successfully alleviates the long-standing problem of continual learning, i.e., catastrophic forgetting. Extensive experiments on public and proprietary real-world datasets demonstrate the superiority and practicality of TERACON. Our code is available at https://github.com/Sein-Kim/TERACON.|用户建模是根据用户过去的行为学习如何将用户表示成一个低维的表示空间，因此为用户提供个性化的服务引起了业界的极大兴趣。以前的用户建模工作主要集中在学习为单个任务设计的特定于任务的用户表示。然而，由于学习任务特定的每个任务的用户表示是不可行的，最近的研究引入了通用用户表示的概念，这是一个更广泛的用户表示相关的各种任务。尽管现有的学习通用用户表示的方法很有效，但是由于数据需求、灾难性遗忘以及对不断增加的任务的学习能力有限，这些方法在实际应用中是不切实际的。在本文中，我们提出了一种新的持续用户表征学习方法，称为 TERACON，它的学习能力不受任务数量增加的限制，同时捕捉任务之间的关系。其主要思想是为每个任务引入一个嵌入，即任务嵌入，用于生成任务特定的软掩码，不仅允许整个模型参数更新直到训练序列结束，而且有利于任务之间的关系被捕获。此外，我们还引入了一个新的知识保留模块，该模块采用伪标记策略，成功地解决了长期以来存在的连续学习问题，即灾难性遗忘问题。在公共和专有的真实世界数据集上的大量实验证明了 TERACON 的优越性和实用性。我们的代码可以在 https://github.com/sein-kim/teracon 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Task+Relation-aware+Continual+User+Representation+Learning)|0|
|[GraphSHA: Synthesizing Harder Samples for Class-Imbalanced Node Classification](https://doi.org/10.1145/3580305.3599374)|WenZhi Li, ChangDong Wang, Hui Xiong, JianHuang Lai|Sun Yat-sen University; The Hong Kong University of Science and Technology (Guangzhou)|Class imbalance is the phenomenon that some classes have much fewer instances than others, which is ubiquitous in real-world graph-structured scenarios. Recent studies find that off-the-shelf Graph Neural Networks (GNNs) would under-represent minor class samples. We investigate this phenomenon and discover that the subspaces of minor classes being squeezed by those of the major ones in the latent space is the main cause of this failure. We are naturally inspired to enlarge the decision boundaries of minor classes and propose a general framework GraphSHA by Synthesizing HArder minor samples. Furthermore, to avoid the enlarged minor boundary violating the subspaces of neighbor classes, we also propose a module called SemiMixup to transmit enlarged boundary information to the interior of the minor classes while blocking information propagation from minor classes to neighbor classes. Empirically, GraphSHA shows its effectiveness in enlarging the decision boundaries of minor classes, as it outperforms various baseline methods in class-imbalanced node classification with different GNN backbone encoders over seven public benchmark datasets. Code is avilable at https://github.com/wenzhilics/GraphSHA.|类不平衡是一些类的实例比其他类少得多的现象，这种现象在真实世界的图形结构场景中普遍存在。最近的研究发现，现成的图形神经网络(GNN)会低估次要类样本。我们研究了这一现象，发现潜空间中次类的子空间被主类的子空间挤压是导致这一失败的主要原因。我们很自然地受到了扩大次类决策边界的启发，并通过综合 HArder 次样本提出了一个通用的 GraphSHA 框架。此外，为了避免扩大的次边界侵犯邻居类的子空间，我们还提出了一个称为 SemiMixup 的模块来传输扩大的边界信息到次类的内部，同时阻止信息从次类传播到邻居类。实验表明，GraphSHA 在扩大次类的决策边界方面是有效的，因为它在七个公共基准数据集上使用不同的 GNN 骨干编码器进行类不平衡节点分类时优于各种基准方法。代码可在 https://github.com/wenzhilics/graphsha 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphSHA:+Synthesizing+Harder+Samples+for+Class-Imbalanced+Node+Classification)|0|
|[Physics-Guided Discovery of Highly Nonlinear Parametric Partial Differential Equations](https://doi.org/10.1145/3580305.3599466)|Yingtao Luo, Qiang Liu, Yuntian Chen, Wenbo Hu, Tian Tian, Jun Zhu||Partial differential equations (PDEs) fitting scientific data can represent physical laws with explainable mechanisms for various mathematically-oriented subjects. The data-driven discovery of PDEs from scientific data thrives as a new attempt to model complex phenomena in nature, but the effectiveness of current practice is typically limited by the scarcity of data and the complexity of phenomena. Especially, the discovery of PDEs with highly nonlinear coefficients from low-quality data remains largely under-addressed. To deal with this challenge, we propose a novel physics-guided learning method, which can not only encode observation knowledge such as initial and boundary conditions but also incorporate the basic physical principles and laws to guide the model optimization. We empirically demonstrate that the proposed method is more robust against data noise and sparsity, and can reduce the estimation error by a large margin; moreover, for the first time we are able to discover PDEs with highly nonlinear coefficients. With the promising performance, the proposed method pushes forward the boundary of the PDEs that can be found by machine learning models for scientific discovery.|拟合科学数据的偏微分方程(PDE)可以表示各种数学导向学科的具有可解释机制的物理规律。数据驱动发现偏微分方程的科学数据蓬勃发展，作为一种新的尝试模拟自然界中的复杂现象，但目前的做法的有效性通常受到数据稀缺和现象复杂性的限制。特别是，从低质量数据中发现具有高度非线性系数的偏微分方程仍然是一个很大的问题。为了解决这一问题，我们提出了一种新的物理导向学习方法，它不仅可以对初始条件和边界条件等观测知识进行编码，而且可以结合基本的物理原理和规律来指导模型的优化。实验结果表明，该方法对数据噪声和稀疏性具有较强的鲁棒性，能够大幅度地减小估计误差，并且首次发现了具有高度非线性系数的偏微分方程。该方法具有良好的性能，为科学发现推进了机器学习模型所能找到的偏微分方程的边界。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Physics-Guided+Discovery+of+Highly+Nonlinear+Parametric+Partial+Differential+Equations)|0|
|[Online Fairness Auditing through Iterative Refinement](https://doi.org/10.1145/3580305.3599454)|Pranav Maneriker, Codi Burley, Srinivasan Parthasarathy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Fairness+Auditing+through+Iterative+Refinement)|0|
|[Online Level-wise Hierarchical Clustering](https://doi.org/10.1145/3580305.3599455)|Nicholas Monath, Manzil Zaheer, Andrew McCallum||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Level-wise+Hierarchical+Clustering)|0|
|[Cracking White-box DNN Watermarks via Invariant Neuron Transforms](https://doi.org/10.1145/3580305.3599291)|Xudong Pan, Mi Zhang, Yifan Yan, Yining Wang, Min Yang|; Fudan University|Recently, how to protect the Intellectual Property (IP) of deep neural networks (DNN) becomes a major concern for the AI industry. To combat potential model piracy, recent works explore various watermarking strategies to embed secret identity messages into the prediction behaviors or the internals (e.g., weights and neuron activation) of the target model. Sacrificing less functionality and involving more knowledge about the target model, the latter branch of watermarking schemes (i.e., white-box model watermarking) is claimed to be accurate, credible and secure against most known watermark removal attacks, with emerging research efforts and applications in the industry.   In this paper, we present the first effective removal attack which cracks almost all the existing white-box watermarking schemes with provably no performance overhead and no required prior knowledge. By analyzing these IP protection mechanisms at the granularity of neurons, we for the first time discover their common dependence on a set of fragile features of a local neuron group, all of which can be arbitrarily tampered by our proposed chain of invariant neuron transforms. On $9$ state-of-the-art white-box watermarking schemes and a broad set of industry-level DNN architectures, our attack for the first time reduces the embedded identity message in the protected models to be almost random. Meanwhile, unlike known removal attacks, our attack requires no prior knowledge on the training data distribution or the adopted watermark algorithms, and leaves model functionality intact.|近年来，如何保护深层神经网络(DNN)的知识产权成为人工智能产业关注的主要问题。为了打击潜在的盗版模型，最近的研究探索了各种水印策略，将秘密身份信息嵌入到目标模型的预测行为或内部(如权重和神经元激活)中。水印技术的后一个分支(即白盒模型水印)牺牲了较少的功能，涉及更多关于目标模型的知识，被认为是准确、可靠和安全的，能够抵御大多数已知的水印去除攻击，并且在业界中得到了新的研究和应用。在本文中，我们提出了第一个有效的移除攻击，这个攻击破坏了几乎所有现有的白盒水印方案，并且可以证明没有性能开销和不需要先验知识。通过分析神经元粒度上的这些 IP 保护机制，我们首次发现它们对局部神经元群的一组脆弱特征的共同依赖性，所有这些特征都可以被我们提出的不变神经元变换链任意篡改。在9美元的国家最先进的白盒水印方案和广泛的行业级 DNN 架构，我们的攻击第一次减少嵌入的身份信息在受保护的模型几乎是随机的。同时，与已知的移除攻击不同，我们的攻击不需要关于训练数据分布或所采用的水印算法的先验知识，并且保留了模型的功能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cracking+White-box+DNN+Watermarks+via+Invariant+Neuron+Transforms)|0|
|[Graph Neural Bandits](https://doi.org/10.1145/3580305.3599371)|Yunzhe Qi, Yikun Ban, Jingrui He|University of Illinois at Urbana-Champaign; University of Illinois, Urbana Champaign|Contextual bandits aim to choose the optimal arm with the highest reward out of a set of candidates based on their contextual information, and various bandit algorithms have been applied to personalized recommendation due to their ability of solving the exploitation-exploration dilemma. Motivated by online recommendation scenarios, in this paper, we propose a framework named Graph Neural Bandits (GNB) to leverage the collaborative nature among users empowered by graph neural networks (GNNs). Instead of estimating rigid user clusters, we model the "fine-grained'' collaborative effects through estimated user graphs in terms of exploitation and exploration individually. Then, to refine the recommendation strategy, we utilize separate GNN-based models on estimated user graphs for exploitation and adaptive exploration. Theoretical analysis and experimental results on multiple real data sets in comparison with state-of-the-art baselines are provided to demonstrate the effectiveness of our proposed framework.|关联强盗的目标是根据候选人的关联信息从一组候选人中选择报酬最高的最优组合，各种强盗算法因其解决开发-探索两难问题的能力而被应用于个性化推荐。受在线推荐场景的启发，本文提出了一种基于图形神经网络的用户协作框架 GNB。我们没有对刚性用户集群进行估计，而是根据开发和探索的不同，通过估计的用户图对“细粒度”的协作效果进行建模。然后，为了完善推荐策略，我们利用基于 GNN 的分离模型对估计用户图进行开发和自适应探索。通过对多个实际数据集的理论分析和实验结果与最新基线的比较，证明了我们提出的框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Neural+Bandits)|0|
|[Source-Free Domain Adaptation with Temporal Imputation for Time Series Data](https://doi.org/10.1145/3580305.3599507)|Mohamed Ragab, Emadeldeen Eldele, Min Wu, ChuanSheng Foo, Xiaoli Li, Zhenghua Chen|Institute for Infocomm Research, Agency for Science Technology and Research (A*STAR); Center for Frontier AI Research, Agency for Science and Technology and Research (A*STAR)|Source-free domain adaptation (SFDA) aims to adapt a pretrained model from a labeled source domain to an unlabeled target domain without access to the source domain data, preserving source domain privacy. Despite its prevalence in visual applications, SFDA is largely unexplored in time series applications. The existing SFDA methods that are mainly designed for visual applications may fail to handle the temporal dynamics in time series, leading to impaired adaptation performance. To address this challenge, this paper presents a simple yet effective approach for source-free domain adaptation on time series data, namely MAsk and imPUte (MAPU). First, to capture temporal information of the source domain, our method performs random masking on the time series signals while leveraging a novel temporal imputer to recover the original signal from a masked version in the embedding space. Second, in the adaptation step, the imputer network is leveraged to guide the target model to produce target features that are temporally consistent with the source features. To this end, our MAPU can explicitly account for temporal dependency during the adaptation while avoiding the imputation in the noisy input space. Our method is the first to handle temporal consistency in SFDA for time series data and can be seamlessly equipped with other existing SFDA methods. Extensive experiments conducted on three real-world time series datasets demonstrate that our MAPU achieves significant performance gain over existing methods. Our code is available at \url{https://github.com/mohamedr002/MAPU_SFDA_TS}.|无源域适应(SFDA)的目标是在不访问源域数据的情况下，将预先训练好的模型从标记的源域适应到未标记的目标域，从而保护源域的隐私。尽管 SFDA 在视觉应用方面很流行，但在时间序列应用方面却很大程度上没有得到探索。现有的 SFDA 方法主要是为视觉应用而设计的，可能无法处理时间序列中的时间动态，导致适应性能受损。为了解决这一问题，本文提出了一种简单而有效的时间序列数据无源域自适应方法，即 MAsk 和 imPUte (MAPU)。首先，为了获取源域的时间信息，该方法对时间序列信号进行随机掩蔽，同时利用一种新的时间计算机在嵌入空间中从掩蔽版本中恢复原始信号。其次，在适应步骤中，利用计算机网络引导目标模型产生与源特征在时间上一致的目标特征。为此，我们的 MAPU 可以明确地说明在适应期间的时间依赖性，同时避免了在噪声输入空间的插补。我们的方法是第一个处理时间序列数据时间一致性的 SFDA 方法，可以与其他现有的 SFDA 方法无缝配备。在三个实际时间序列数据集上进行的大量实验表明，我们的 MAPU 比现有的方法获得了显著的性能提高。我们的代码可以在 url { https://github.com/mohamedr002/mapu_sfda_ts }找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Source-Free+Domain+Adaptation+with+Temporal+Imputation+for+Time+Series+Data)|0|
|[Causal Effect Estimation on Hierarchical Spatial Graph Data](https://doi.org/10.1145/3580305.3599269)|Koh Takeuchi, Ryo Nishida, Hisashi Kashima, Masaki Onishi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Effect+Estimation+on+Hierarchical+Spatial+Graph+Data)|0|
|[Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders](https://doi.org/10.1145/3580305.3599444)|Dingsu Wang, Yuchen Yan, Ruizhong Qiu, Yada Zhu, Kaiyu Guan, Andrew Margenot, Hanghang Tong|University of Illinois at Urbana-Champaign; IBM Research|Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new model named PoGeVon which leverages variational autoencoder (VAE) to predict missing values over both node time series features and graph structures. In particular, we propose a new node position embedding based on random walk with restart (RWR) in the encoder with provable higher expressive power compared with message-passing based graph neural networks (GNNs). We further design a decoder with 3-stage predictions from the perspective of multi-task learning to impute missing values in both time series and graph structures reciprocally. Experiment results demonstrate the effectiveness of our model over baselines.|多变量时间序列(MTS)插补是近年来研究较多的一个问题。现有的方法可以分为两大类，包括(1)主要关注时间序列特征的深度递归或生成模型，和(2)基于图神经网络(GNN)的模型，这些模型利用 MTS 固有图结构的拓扑信息作为关系归纳偏差进行插补。然而，这些方法或者忽略了拓扑信息，或者假设图的结构是固定的并且已知的。因此，他们未能充分利用图动力学精确插补更具挑战性的 MTS 数据，如网络时间序列(NTS) ，其中底层图是不断变化的，可能有缺失的边。在本文中，我们提出了一种新的方法来克服这些限制。首先，我们定义了包含节点时间序列特征和图结构缺失值的 NTS 插补问题。然后，我们设计了一个新的模型 PoGeVon，它利用变分自动编码器(VAE)来预测节点时间序列特征和图结构上的缺失值。特别地，我们提出了一种新的基于重启随机游走(RWR)的节点位置嵌入编码器，与基于消息传递的图形神经网络(GNN)相比，具有可证明的更高的表达能力。我们进一步从多任务学习的角度设计了一个具有三阶段预测的解码器来相互推算时间序列和图结构中的缺失值。实验结果证明了该模型在基线上的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Networked+Time+Series+Imputation+via+Position-aware+Graph+Enhanced+Variational+Autoencoders)|0|
|[Incremental Causal Graph Learning for Online Root Cause Analysis](https://doi.org/10.1145/3580305.3599392)|Dongjie Wang, Zhengzhang Chen, Yanjie Fu, Yanchi Liu, Haifeng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incremental+Causal+Graph+Learning+for+Online+Root+Cause+Analysis)|0|
|[Treatment Effect Estimation with Adjustment Feature Selection](https://doi.org/10.1145/3580305.3599531)|Haotian Wang, Kun Kuang, Haoang Chi, Longqi Yang, Mingyang Geng, Wanrong Huang, Wenjing Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Treatment+Effect+Estimation+with+Adjustment+Feature+Selection)|0|
|[Adversarial Constrained Bidding via Minimax Regret Optimization with Causality-Aware Reinforcement Learning](https://doi.org/10.1145/3580305.3599254)|Haozhe Wang, Chao Du, Panyan Fang, Li He, Liang Wang, Bo Zheng|Alibaba Group|The proliferation of the Internet has led to the emergence of online advertising, driven by the mechanics of online auctions. In these repeated auctions, software agents participate on behalf of aggregated advertisers to optimize for their long-term utility. To fulfill the diverse demands, bidding strategies are employed to optimize advertising objectives subject to different spending constraints. Existing approaches on constrained bidding typically rely on i.i.d. train and test conditions, which contradicts the adversarial nature of online ad markets where different parties possess potentially conflicting objectives. In this regard, we explore the problem of constrained bidding in adversarial bidding environments, which assumes no knowledge about the adversarial factors. Instead of relying on the i.i.d. assumption, our insight is to align the train distribution of environments with the potential test distribution meanwhile minimizing policy regret. Based on this insight, we propose a practical Minimax Regret Optimization (MiRO) approach that interleaves between a teacher finding adversarial environments for tutoring and a learner meta-learning its policy over the given distribution of environments. In addition, we pioneer to incorporate expert demonstrations for learning bidding strategies. Through a causality-aware policy design, we improve upon MiRO by distilling knowledge from the experts. Extensive experiments on both industrial data and synthetic data show that our method, MiRO with Causality-aware reinforcement Learning (MiROCL), outperforms prior methods by over 30%.|互联网的扩散导致了在线广告的出现，这是由在线拍卖的机制所驱动的。在这些重复的拍卖中，软件代理商代表广告主集合参与，以优化他们的长期效用。为了满足不同的需求，投标策略被用来优化受不同支出约束的广告目标。现有的限制性投标方法通常依赖于身份证培训和测试条件，这与在线广告市场的对抗性质相矛盾，因为在线广告市场中，不同的当事人拥有潜在的相互冲突的目标。在这方面，我们探讨了在不考虑竞争因素的情况下，在竞争性投标环境下的约束投标问题。我们的洞察力不是依赖于内部识别假设，而是使环境的列车分布与潜在的测试分布保持一致，同时最大限度地减少政策遗憾。基于这种观点，我们提出了一种实用的极大极小遗憾优化(Miniax Regret Optimation，MiRO)方法，该方法在教师寻找对抗性的辅导环境和学习者元学习策略之间进行交叉。此外，我们率先采用专家演示学习投标策略。通过一个因果关系感知策略设计，我们从专家那里提取知识来改进 MiRO。对工业数据和合成数据的大量实验表明，我们的方法，带有因果感知强化学习(miROCL)的 miRO，比之前的方法性能高出30% 以上。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adversarial+Constrained+Bidding+via+Minimax+Regret+Optimization+with+Causality-Aware+Reinforcement+Learning)|0|
|[Efficient Sparse Linear Bandits under High Dimensional Data](https://doi.org/10.1145/3580305.3599329)|Xue Wang, Mike Mingcheng Wei, Tao Yao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Sparse+Linear+Bandits+under+High+Dimensional+Data)|0|
|[MicroscopeSketch: Accurate Sliding Estimation Using Adaptive Zooming](https://doi.org/10.1145/3580305.3599432)|Yuhan Wu, Shiqi Jiang, Siyuan Dong, Zheng Zhong, Jiale Chen, Yutong Hu, Tong Yang, Steve Uhlig, Bin Cui||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MicroscopeSketch:+Accurate+Sliding+Estimation+Using+Adaptive+Zooming)|0|
|[Learning Behavior-oriented Knowledge Tracing](https://doi.org/10.1145/3580305.3599407)|Bihan Xu, Zhenya Huang, Jiayu Liu, Shuanghong Shen, Qi Liu, Enhong Chen, Jinze Wu, Shijin Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Behavior-oriented+Knowledge+Tracing)|0|
|[MimoSketch: A Framework to Mine Item Frequency on Multiple Nodes with Sketches](https://doi.org/10.1145/3580305.3599433)|Yuchen Xu, Wenfei Wu, Bohan Zhao, Tong Yang, Yikai Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MimoSketch:+A+Framework+to+Mine+Item+Frequency+on+Multiple+Nodes+with+Sketches)|0|
|[Kernel Ridge Regression-Based Graph Dataset Distillation](https://doi.org/10.1145/3580305.3599398)|Zhe Xu, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hao Yang, Hanghang Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Kernel+Ridge+Regression-Based+Graph+Dataset+Distillation)|0|
|[BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs](https://doi.org/10.1145/3580305.3599263)|Zhen Yang, Tinglin Huang, Ming Ding, Yuxiao Dong, Rex Ying, Yukuo Cen, Yangliao Geng, Jie Tang|Tsinghua University; Yale University|In-Batch contrastive learning is a state-of-the-art self-supervised method that brings semantically-similar instances close while pushing dissimilar instances apart within a mini-batch. Its key to success is the negative sharing strategy, in which every instance serves as a negative for the others within the mini-batch. Recent studies aim to improve performance by sampling hard negatives \textit{within the current mini-batch}, whose quality is bounded by the mini-batch itself. In this work, we propose to improve contrastive learning by sampling mini-batches from the input data. We present BatchSampler\footnote{The code is available at \url{https://github.com/THUDM/BatchSampler}} to sample mini-batches of hard-to-distinguish (i.e., hard and true negatives to each other) instances. To make each mini-batch have fewer false negatives, we design the proximity graph of randomly-selected instances. To form the mini-batch, we leverage random walk with restart on the proximity graph to help sample hard-to-distinguish instances. BatchSampler is a simple and general technique that can be directly plugged into existing contrastive learning models in vision, language, and graphs. Extensive experiments on datasets of three modalities show that BatchSampler can consistently improve the performance of powerful contrastive models, as shown by significant improvements of SimCLR on ImageNet-100, SimCSE on STS (language), and GraphCL and MVGRL on graph datasets.|批内对比学习是一种最先进的自我监督方法，它可以使语义相似的实例关闭，同时在小批内将不同的实例分开。其成功的关键是消极分享策略，在这种策略中，每一个实例对于小批次中的其他实例都是消极的。最近的研究旨在提高性能抽样硬负面文本{在当前的小批量} ，其质量是有界的小批量本身。在这项工作中，我们提出改善对比学习的抽样小批量的输入数据。我们提供 BatchSampler 脚注{该代码可在 url { https://github.com/thudm/BatchSampler }}获得，用于对难以区分(即彼此之间的硬负片和真负片)的迷你批次实例进行抽样。为了使每个小批量产品具有较少的假阴性，我们设计了随机选择实例的接近图。为了形成迷你批处理，我们利用在接近图上重新启动的随机游动来帮助抽样难以区分的实例。BatchSampler 是一种简单而通用的技术，可以直接插入到视觉、语言和图形中现有的对比学习模型中。对三种模式的数据集进行的广泛实验表明，BatchSampler 可以持续改善强大的对比模型的性能，如 ImageNet-100上的 SimCLR，STS (语言)上的 SimCSE 以及图形数据集上的 GraphCL 和 MVGRL 的显着改进所示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BatchSampler:+Sampling+Mini-Batches+for+Contrastive+Learning+in+Vision,+Language,+and+Graphs)|0|
|[Web-based Long-term Spine Treatment Outcome Forecasting](https://doi.org/10.1145/3580305.3599545)|Hangting Ye, Zhining Liu, Wei Cao, Amir M. Amiri, Jiang Bian, Yi Chang, Jon D. Lurie, Jim Weinstein, TieYan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Web-based+Long-term+Spine+Treatment+Outcome+Forecasting)|0|
|[Optimal Dynamic Subset Sampling: Theory and Applications](https://doi.org/10.1145/3580305.3599458)|Lu Yi, Hanzhi Wang, Zhewei Wei|Renmin University of China|We study the fundamental problem of sampling independent events, called subset sampling. Specifically, consider a set of $n$ events $S=\{x_1, \ldots, x_n\}$, where each event $x_i$ has an associated probability $p(x_i)$. The subset sampling problem aims to sample a subset $T \subseteq S$, such that every $x_i$ is independently included in $S$ with probability $p_i$. A naive solution is to flip a coin for each event, which takes $O(n)$ time. However, the specific goal is to develop data structures that allow drawing a sample in time proportional to the expected output size $\mu=\sum_{i=1}^n p(x_i)$, which can be significantly smaller than $n$ in many applications. The subset sampling problem serves as an important building block in many tasks and has been the subject of various research for more than a decade. However, most of the existing subset sampling approaches are conducted in a static setting, where the events or their associated probability in set $S$ is not allowed to be changed over time. These algorithms incur either large query time or update time in a dynamic setting despite the ubiquitous time-evolving events with changing probability in real life. Therefore, it is a pressing need, but still, an open problem, to design efficient dynamic subset sampling algorithms. In this paper, we propose ODSS, the first optimal dynamic subset sampling algorithm. The expected query time and update time of ODSS are both optimal, matching the lower bounds of the subset sampling problem. We present a nontrivial theoretical analysis to demonstrate the superiority of ODSS. We also conduct comprehensive experiments to empirically evaluate the performance of ODSS. Moreover, we apply ODSS to a concrete application: influence maximization. We empirically show that our ODSS can improve the complexities of existing influence maximization algorithms on large real-world evolving social networks.|我们研究抽样独立事件的基本问题，称为子集抽样。具体来说，考虑一组 $n $事件 $S = { x _ 1，ldot，x _ n } $，其中每个事件 $x _ i $具有相关的概率 $p (x _ i) $。子集抽样问题的目标是抽样一个子集 $T 子集 S $，这样每个 $x _ i $都独立地包含在 $S $中，概率为 $p _ i $。一个天真的解决方案是为每个事件抛硬币，这需要花费 $O (n) $时间。然而，我们的具体目标是开发一种数据结构，它允许在与预期输出大小成正比的时间内绘制样本 $mu = sum _ { i = 1} ^ n p (x _ i) $，在许多应用程序中，它可以明显小于 $n $。子集抽样问题是许多工作中的一个重要组成部分，也是近十多年来各种研究的主题。但是，大多数现有的子集抽样方法都是在静态环境中进行的，其中不允许随着时间的推移更改集 $S $中的事件或其相关概率。尽管在现实生活中随时间演化的事件随概率的变化无处不在，但这些算法在动态环境中会产生大量的查询时间或更新时间。因此，设计高效的动态子集采样算法是一个迫切而又尚未解决的问题。本文提出了第一种最优动态子集抽样算法 ODSS。ODSS 的期望查询时间和更新时间均为最优，与子集抽样问题的下界相匹配。我们提出了一个非平凡的理论分析，以证明 ODSS 的优越性。我们还进行了综合性的实验，对 ODSS 的性能进行了实证评估。此外，我们将 ODSS 应用于一个具体的应用: 影响最大化。我们的实验表明，我们的 ODSS 可以改善现有的影响最大化算法在大型真实世界演化的社会网络上的复杂性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimal+Dynamic+Subset+Sampling:+Theory+and+Applications)|0|
|[Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term](https://doi.org/10.1145/3580305.3599501)|Yun Yue, Jiadi Jiang, Zhiling Ye, Ning Gao, Yongchao Liu, Ke Zhang|Ant Group|Deep Neural Networks (DNNs) generalization is known to be closely related to the flatness of minima, leading to the development of Sharpness-Aware Minimization (SAM) for seeking flatter minima and better generalization. In this paper, we revisit the loss of SAM and propose a more general method, called WSAM, by incorporating sharpness as a regularization term. We prove its generalization bound through the combination of PAC and Bayes-PAC techniques, and evaluate its performance on various public datasets. The results demonstrate that WSAM achieves improved generalization, or is at least highly competitive, compared to the vanilla optimizer, SAM and its variants. The code is available at https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.|深度神经网络(DNN)泛化与最小值的平坦性密切相关，导致锐度感知最小化(SAM)的发展，以寻求更平坦的最小值和更好的泛化。在本文中，我们重新审视了 SAM 的损失，并提出了一种更一般的方法，称为 WSAM，通过合并锐度作为一个正则项。通过结合 PAC 和 Bayes-PAC 技术证明了其泛化界，并对其在各种公共数据集上的性能进行了评估。结果表明，与普通的优化器 SAM 及其变体相比，WSAM 实现了改进的泛化，或者至少具有很强的竞争力。密码可在 https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sharpness-Aware+Minimization+Revisited:+Weighted+Sharpness+as+a+Regularization+Term)|0|
|[Doubly Robust AUC Optimization against Noisy and Adversarial Samples](https://doi.org/10.1145/3580305.3599316)|Chenkang Zhang, Wanli Shi, Lei Luo, Bin Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Doubly+Robust+AUC+Optimization+against+Noisy+and+Adversarial+Samples)|0|
|[Finding Favourite Tuples on Data Streams with Provably Few Comparisons](https://doi.org/10.1145/3580305.3599352)|Guangyi Zhang, Nikolaj Tatti, Aristides Gionis|HIIT, University of Helsinki; KTH Royal Institute of Technology; Shenzhen Institute of Computing Sciences|One of the most fundamental tasks in data science is to assist a user with unknown preferences in finding high-utility tuples within a large database. To accurately elicit the unknown user preferences, a widely-adopted way is by asking the user to compare pairs of tuples. In this paper, we study the problem of identifying one or more high-utility tuples by adaptively receiving user input on a minimum number of pairwise comparisons. We devise a single-pass streaming algorithm, which processes each tuple in the stream at most once, while ensuring that the memory size and the number of requested comparisons are in the worst case logarithmic in $n$, where $n$ is the number of all tuples. An important variant of the problem, which can help to reduce human error in comparisons, is to allow users to declare ties when confronted with pairs of tuples of nearly equal utility. We show that the theoretical guarantees of our method can be maintained for this important problem variant. In addition, we show how to enhance existing pruning techniques in the literature by leveraging powerful tools from mathematical programming. Finally, we systematically evaluate all proposed algorithms over both synthetic and real-life datasets, examine their scalability, and demonstrate their superior performance over existing methods.|数据科学中最基本的任务之一是帮助具有未知偏好的用户在大型数据库中查找高效用元组。为了准确地获得未知的用户首选项，一种被广泛采用的方法是要求用户比较元组对。在本文中，我们研究识别一个或多个高效用元组的问题，通过自适应接收用户输入的最小数目的成对比较。我们设计了一个单通道流式算法，它最多处理流中的每个元组一次，同时确保内存大小和请求比较的数量在最坏的情况下是以 $n $为对数的，其中 $n $是所有元组的数量。该问题的一个重要变体是允许用户在遇到效用几乎相等的元组对时声明关系，这有助于减少比较中的人为错误。我们表明，对于这个重要的问题变量，我们方法的理论保证是可以保持的。此外，我们还展示了如何通过利用数学编程中的强大工具来增强文献中现有的剪枝技术。最后，我们系统地评估了所有提出的算法在合成和实际数据集上的性能，检验了它们的可伸缩性，并证明了它们优于现有方法的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Finding+Favourite+Tuples+on+Data+Streams+with+Provably+Few+Comparisons)|0|
|[Domain-Specific Risk Minimization for Domain Generalization](https://doi.org/10.1145/3580305.3599313)|YiFan Zhang, Jindong Wang, Jian Liang, Zhang Zhang, Baosheng Yu, Liang Wang, Dacheng Tao, Xing Xie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Domain-Specific+Risk+Minimization+for+Domain+Generalization)|0|
|[Towards Fair Disentangled Online Learning for Changing Environments](https://doi.org/10.1145/3580305.3599523)|Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Christan Grant, Feng Chen|University of Florida; Baylor University; The University of Texas at Dallas; University of Texas at Dallas; University of Arkansas|In the problem of online learning for changing environments, data are sequentially received one after another over time, and their distribution assumptions may vary frequently. Although existing methods demonstrate the effectiveness of their learning algorithms by providing a tight bound on either dynamic regret or adaptive regret, most of them completely ignore learning with model fairness, defined as the statistical parity across different sub-population (e.g., race and gender). Another drawback is that when adapting to a new environment, an online learner needs to update model parameters with a global change, which is costly and inefficient. Inspired by the sparse mechanism shift hypothesis, we claim that changing environments in online learning can be attributed to partial changes in learned parameters that are specific to environments and the rest remain invariant to changing environments. To this end, in this paper, we propose a novel algorithm under the assumption that data collected at each time can be disentangled with two representations, an environment-invariant semantic factor and an environment-specific variation factor. The semantic factor is further used for fair prediction under a group fairness constraint. To evaluate the sequence of model parameters generated by the learner, a novel regret is proposed in which it takes a mixed form of dynamic and static regret metrics followed by a fairness-aware long-term constraint. The detailed analysis provides theoretical guarantees for loss regret and violation of cumulative fairness constraints. Empirical evaluations on real-world datasets demonstrate our proposed method sequentially outperforms baseline methods in model accuracy and fairness.|在变化环境下的在线学习问题中，数据随着时间的推移依次接收，其分布假设可能会频繁变化。尽管现有的方法通过提供动态后悔或适应性后悔的紧密界限来证明其学习算法的有效性，但大多数方法完全忽略了模型公平性的学习，这种模型公平性被定义为不同子群(例如种族和性别)的统计平价。另一个缺点是，在适应新环境时，在线学习者需要根据全局变化更新模型参数，这样做成本高，效率低。受稀疏机制转移假说的启发，我们认为在线学习中不断变化的环境可以归因于特定于环境的学习参数的部分变化，而其余的参数对不断变化的环境保持不变。为此，本文提出了一种新的算法，该算法假设每次采集的数据可以分解为两种表示: 环境不变的语义因子和环境特定的变化因子。语义因子进一步用于群体公平约束下的公平预测。为了评估学习者生成的模型参数序列，提出了一种新的遗憾度量方法，该方法采用动态和静态遗憾度量的混合形式，并且具有公平意识的长期约束。详细的分析为损失后悔和违反累积公平约束提供了理论保证。对实际数据集的实证分析表明，该方法在模型精度和公平性方面均优于基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Fair+Disentangled+Online+Learning+for+Changing+Environments)|0|
|[SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding](https://doi.org/10.1145/3580305.3599907)|Vasilisa Bashlovkina, Riley Matthews, Zhaobin Kuang, Simon Baumgartner, Michael Bendersky|Google Research|We study the ability of transformer-based language models (LMs) to understand social media language. Social media (SM) language is distinct from standard written language, yet existing benchmarks fall short of capturing LM performance in this socially, economically, and politically important domain. We quantify the degree to which social media language differs from conventional language and conclude that the difference is significant both in terms of token distribution and rate of linguistic shift. Next, we introduce a new benchmark for Social MedIa Language Evaluation (SMILE) that covers four SM platforms and eleven tasks. Finally, we show that learning a tokenizer and pretraining on a mix of social media and conventional language yields an LM that outperforms the best similar-sized alternative by 4.2 points on the overall SMILE score.|我们研究了基于转换器的语言模型(LM)理解社交媒体语言的能力。社会媒体语言(SM)与标准的书面语言不同，然而现有的基准在这个社会、经济和政治重要的领域还不足以捕捉 LM 的表现。我们量化了社交媒体语言与传统语言的差异程度，并得出结论: 社交媒体语言与传统语言的差异在表征分布和语言转换率方面都是显著的。接下来，我们将为社会媒体语言评估(SMILE)引入一个新的基准，它涵盖了四个 SM 平台和十一个任务。最后，我们表明，学习一个标记器和预训练的社会媒体和传统语言的混合产生了一个 LM 的表现最好的类似大小的选择4.2分的总体 SMILE 得分。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SMILE:+Evaluation+and+Domain+Adaptation+for+Social+Media+Language+Understanding)|0|
|[Augmenting Rule-based DNS Censorship Detection at Scale with Machine Learning](https://doi.org/10.1145/3580305.3599775)|Jacob Alexander Markson Brown, Xi Jiang, Van Tran, Arjun Nitin Bhagoji, Nguyen Phong Hoang, Nick Feamster, Prateek Mittal, Vinod Yegneswaran|Princeton University; SRI International; University of Chicago|The proliferation of global censorship has led to the development of a plethora of measurement platforms to monitor and expose it. Censorship of the domain name system (DNS) is a key mechanism used across different countries. It is currently detected by applying heuristics to samples of DNS queries and responses (probes) for specific destinations. These heuristics, however, are both platform-specific and have been found to be brittle when censors change their blocking behavior, necessitating a more reliable automated process for detecting censorship. In this paper, we explore how machine learning (ML) models can (1) help streamline the detection process, (2) improve the usability of large-scale datasets for censorship detection, and (3) discover new censorship instances and blocking signatures missed by existing heuristic methods. Our study shows that supervised models, trained using expert-derived labels on instances of known anomalies and possible censorship, can learn the detection heuristics employed by different measurement platforms. More crucially, we find that unsupervised models, trained solely on uncensored instances, can identify new instances and variations of censorship missed by existing heuristics. Moreover, both methods demonstrate the capability to uncover a substantial number of new DNS blocking signatures, i.e., injected fake IP addresses overlooked by existing heuristics. These results are underpinned by an important methodological finding: comparing the outputs of models trained using the same probes but with labels arising from independent processes allows us to more reliably detect cases of censorship in the absence of ground-truth labels of censorship.|全球审查制度的扩散导致了监测和揭露它的大量测量平台的发展。域名系统(DNS)的审查是各国使用的一个关键机制。目前，通过对特定目的地的 DNS 查询和响应(探测)样本应用启发式方法来检测它。然而，这些启发式方法都是针对特定平台的，当审查者改变他们的拦截行为时，这些方法被发现是脆弱的，这就需要一个更可靠的自动化过程来检测审查。本文探讨了机器学习(ML)模型在以下几个方面的作用: (1)简化检测过程; (2)提高大规模数据集在检测中的可用性; (3)发现新的检测实例和现有启发式方法遗漏的阻塞签名。我们的研究表明，监督模型，训练使用专家派生的标签对已知的异常和可能的检查的实例，可以学习检测启发采用不同的测量平台。更重要的是，我们发现，无监督模型，仅仅训练未经审查的实例，可以识别新的实例和变化的审查错过了现有的启发。此外，这两种方法都证明了能够发现大量新的 DNS 阻塞签名，即注入的假 IP 地址被现有的启发式方法忽略。这些结果得到了一个重要的方法论发现的支持: 比较使用相同探针训练的模型的输出，但是与独立过程产生的标签进行比较，使我们能够更可靠地检测在没有审查的地面真相标签的情况下的审查情况。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Augmenting+Rule-based+DNS+Censorship+Detection+at+Scale+with+Machine+Learning)|0|
|[Taming the Domain Shift in Multi-source Learning for Energy Disaggregation](https://doi.org/10.1145/3580305.3599910)|Xiaomin Chang, Wei Li, Yunchuan Shi, Albert Y. Zomaya||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Taming+the+Domain+Shift+in+Multi-source+Learning+for+Energy+Disaggregation)|0|
|[Variance Reduction Using In-Experiment Data: Efficient and Targeted Online Measurement for Sparse and Delayed Outcomes](https://doi.org/10.1145/3580305.3599928)|Alex Deng, Michelle Du, Anna Matlin, Qing Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Variance+Reduction+Using+In-Experiment+Data:+Efficient+and+Targeted+Online+Measurement+for+Sparse+and+Delayed+Outcomes)|0|
|[Modelling Delayed Redemption with Importance Sampling and Pre-Redemption Engagement](https://doi.org/10.1145/3580305.3599867)|Samik Datta, Anshuman Mourya, Anirban Majumder, Vineet Chaoji||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modelling+Delayed+Redemption+with+Importance+Sampling+and+Pre-Redemption+Engagement)|0|
|[From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams](https://doi.org/10.1145/3580305.3599827)|Iddo Drori, Sarah J. Zhang, Reece Shuttleworth, Sarah Zhang, Keith Tyser, Zad Chin, Pedro Lantigua, Saisamrit Surbehera, Gregory Hunter, Derek Austin, Leonard Tang, Yann Hicke, Sage Simhon, Sathwik Karnik, Darnell Granberry, Madeleine Udell||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Human+Days+to+Machine+Seconds:+Automatically+Answering+and+Generating+Machine+Learning+Final+Exams)|0|
|[Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance](https://doi.org/10.1145/3580305.3599856)|Yuchen Fang, Zhenggang Tang, Kan Ren, Weiqing Liu, Li Zhao, Jiang Bian, Dongsheng Li, Weinan Zhang, Yong Yu, TieYan Liu|Shanghai Jiao Tong University; Microsoft; University of Illinois Urbana-Champaign; Microsoft Research Asia|Order execution is a fundamental task in quantitative finance, aiming at finishing acquisition or liquidation for a number of trading orders of the specific assets. Recent advance in model-free reinforcement learning (RL) provides a data-driven solution to the order execution problem. However, the existing works always optimize execution for an individual order, overlooking the practice that multiple orders are specified to execute simultaneously, resulting in suboptimality and bias. In this paper, we first present a multi-agent RL (MARL) method for multi-order execution considering practical constraints. Specifically, we treat every agent as an individual operator to trade one specific order, while keeping communicating with each other and collaborating for maximizing the overall profits. Nevertheless, the existing MARL algorithms often incorporate communication among agents by exchanging only the information of their partial observations, which is inefficient in complicated financial market. To improve collaboration, we then propose a learnable multi-round communication protocol, for the agents communicating the intended actions with each other and refining accordingly. It is optimized through a novel action value attribution method which is provably consistent with the original learning objective yet more efficient. The experiments on the data from two real-world markets have illustrated superior performance with significantly better collaboration effectiveness achieved by our method.|定单执行是定量金融的一项基本任务，其目的是完成对特定资产的多个交易定单的收购或清算。无模型强化学习的最新进展为订单执行问题提供了一个数据驱动的解决方案。然而，现有的工作总是优化单个订单的执行，忽视了多个订单指定同时执行的做法，导致次优性和偏差。本文首先提出了一种考虑实际约束的多代理 RL (MARL)多订单执行方法。具体来说，我们把每个代理当作一个独立的经营者来交易一个特定的订单，同时保持相互之间的沟通和合作，以实现总体利润的最大化。然而，现有的 MARL 算法往往只交换代理人的部分观测信息，而不考虑代理人之间的通信，在复杂的金融市场中效率低下。为了改进协作，我们提出了一个可学习的多轮通信协议，用于代理之间相互通信预期的操作并相应地进行细化。通过一种新的行为价值归因方法对其进行优化，该方法与原有的学习目标一致，但效率更高。对两个实际市场的数据进行的实验表明，该方法具有更好的协作效率和更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Multi-Agent+Intention-Aware+Communication+for+Optimal+Multi-Order+Execution+in+Finance)|0|
|[iETA: A Robust and Scalable Incremental Learning Framework for Time-of-Arrival Estimation](https://doi.org/10.1145/3580305.3599842)|Jindong Han, Hao Liu, Shui Liu, Xi Chen, Naiqiang Tan, Hua Chai, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=iETA:+A+Robust+and+Scalable+Incremental+Learning+Framework+for+Time-of-Arrival+Estimation)|0|
|[Identifying Complicated Contagion Scenarios from Cascade Data](https://doi.org/10.1145/3580305.3599841)|Galen Harrison, Amro Alabsi Aljundi, Jiangzhuo Chen, S. S. Ravi, Anil Kumar S. Vullikanti, Madhav V. Marathe, Abhijin Adiga||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifying+Complicated+Contagion+Scenarios+from+Cascade+Data)|0|
|[Large-scale Urban Cellular Traffic Generation via Knowledge-Enhanced GANs with Multi-Periodic Patterns](https://doi.org/10.1145/3580305.3599853)|Shuodi Hui, Huandong Wang, Tong Li, Xinghao Yang, Xing Wang, Junlan Feng, Lin Zhu, Chao Deng, Pan Hui, Depeng Jin, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large-scale+Urban+Cellular+Traffic+Generation+via+Knowledge-Enhanced+GANs+with+Multi-Periodic+Patterns)|0|
|[SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis Dataset and Its Evaluation](https://doi.org/10.1145/3580305.3599904)|Md. Ekramul Islam, Labib Chowdhury, Faisal Ahamed Khan, Shazzad Hossain, Md. Sourave Hossain, Mohammad Mamun Or Rashid, Nabeel Mohammed, Mohammad Ruhul Amin|Fordham University; North South University; Bangladesh Computer Council; Giga Tech Limited|This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis dataset. Comprising 70,000 samples, it was created from diverse sources and annotated by a gender-balanced team of linguists. SentiGOLD adheres to established linguistic conventions agreed upon by the Government of Bangladesh and a Bangla linguistics committee. Unlike English and other languages, Bangla lacks standard sentiment analysis datasets due to the absence of a national linguistics framework. The dataset incorporates data from online video comments, social media posts, blogs, news, and other sources while maintaining domain and class distribution rigorously. It spans 30 domains (e.g., politics, entertainment, sports) and includes 5 sentiment classes (strongly negative, weakly negative, neutral, and strongly positive). The annotation scheme, approved by the national linguistics committee, ensures a robust Inter Annotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and cross-dataset evaluation protocols are applied to establish a standard classification system. Cross-dataset evaluation on the noisy SentNoB dataset presents a challenging test scenario. Additionally, zero-shot experiments demonstrate the generalizability of SentiGOLD. The top model achieves a macro f1 score of 0.62 (intra-dataset) across 5 classes, setting a benchmark, and 0.61 (cross-dataset from SentNoB) across 3 classes, comparable to the state-of-the-art. Fine-tuned sentiment analysis model can be accessed at https://sentiment.bangla.gov.bd.|本文介绍了孟加拉语多领域情感分析数据集 SentiGOLD。它包括70,000个样本，由不同的来源创建，并由一个性别平衡的语言学家团队进行注释。SentiGOLD 遵守孟加拉国政府和孟加拉语言学委员会商定的既定语言公约。与英语和其他语言不同，由于缺乏国家语言学框架，孟加拉语缺乏标准的情感分析数据集。该数据集合并了来自在线视频评论、社交媒体帖子、博客、新闻和其他来源的数据，同时严格维护了域和类的分布。它跨越30个领域(例如，政治，娱乐，体育) ，包括5个情绪类(强烈消极，弱消极，中立，和强烈积极)。由国家语言学委员会批准的注释方案确保了一个强有力的内部注释协议(IAA) ，Fleiss 的 kappa 得分为0.88。数据集内和数据集间的评估协议被用来建立一个标准的分类方案。对噪声 SentNoB 数据集进行跨数据集评估是一个具有挑战性的测试场景。此外，零拍实验证明了 SentiGOLD 的通用性。顶级模型在5个类别中实现了0.62(数据集内)的宏观 f1评分，设定了基准，并在3个类别中实现了0.61(来自 SentNoB 的跨数据集) ，与最先进的技术相当。微调的情绪分析模型可以在 https://sentiment.bangla.gov.bd 访问。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SentiGOLD:+A+Large+Bangla+Gold+Standard+Multi-Domain+Sentiment+Analysis+Dataset+and+Its+Evaluation)|0|
|[Off-Policy Learning-to-Bid with AuctionGym](https://doi.org/10.1145/3580305.3599877)|Olivier Jeunen, Sean Murphy, Ben Allison||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Off-Policy+Learning-to-Bid+with+AuctionGym)|0|
|[FairCod: A Fairness-aware Concurrent Dispatch System for Large-scale Instant Delivery Services](https://doi.org/10.1145/3580305.3599824)|Lin Jiang, Shuai Wang, Baoshen Guo, Hai Wang, Desheng Zhang, Guang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FairCod:+A+Fairness-aware+Concurrent+Dispatch+System+for+Large-scale+Instant+Delivery+Services)|0|
|[CBLab: Supporting the Training of Large-scale Traffic Control Policies with Scalable Traffic Simulation](https://doi.org/10.1145/3580305.3599789)|Chumeng Liang, Zherui Huang, Yicheng Liu, Zhanyu Liu, Guanjie Zheng, Hanyuan Shi, Kan Wu, Yuhao Du, Fuliang Li, Zhenhui Jessie Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CBLab:+Supporting+the+Training+of+Large-scale+Traffic+Control+Policies+with+Scalable+Traffic+Simulation)|0|
|[Practical Synthetic Human Trajectories Generation Based on Variational Point Processes](https://doi.org/10.1145/3580305.3599888)|Qingyue Long, Huandong Wang, Tong Li, Lisi Huang, Kun Wang, Qiong Wu, Guangyu Li, Yanping Liang, Li Yu, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+Synthetic+Human+Trajectories+Generation+Based+on+Variational+Point+Processes)|0|
|[Deep Landscape Forecasting in Multi-Slot Real-Time Bidding](https://doi.org/10.1145/3580305.3599799)|Weitong Ou, Bo Chen, Yingxuan Yang, Xinyi Dai, Weiwen Liu, Weinan Zhang, Ruiming Tang, Yong Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Landscape+Forecasting+in+Multi-Slot+Real-Time+Bidding)|0|
|[NFT-Based Data Marketplace with Digital Watermarking](https://doi.org/10.1145/3580305.3599876)|Saeed Ranjbar Alvar, Mohammad Akbari, David (Ming Xuan) Yue, Yong Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NFT-Based+Data+Marketplace+with+Digital+Watermarking)|0|
|[Rover: An Online Spark SQL Tuning Service via Generalized Transfer Learning](https://doi.org/10.1145/3580305.3599953)|Yu Shen, Xinyuyang Ren, Yupeng Lu, Huaijun Jiang, Huanyong Xu, Di Peng, Yang Li, Wentao Zhang, Bin Cui|Mila – Québec AI Institute; ByteDance Inc.; Peking University|Distributed data analytic engines like Spark are common choices to process massive data in industry. However, the performance of Spark SQL highly depends on the choice of configurations, where the optimal ones vary with the executed workloads. Among various alternatives for Spark SQL tuning, Bayesian optimization (BO) is a popular framework that finds near-optimal configurations given sufficient budget, but it suffers from the re-optimization issue and is not practical in real production. When applying transfer learning to accelerate the tuning process, we notice two domain-specific challenges: 1) most previous work focus on transferring tuning history, while expert knowledge from Spark engineers is of great potential to improve the tuning performance but is not well studied so far; 2) history tasks should be carefully utilized, where using dissimilar ones lead to a deteriorated performance in production. In this paper, we present Rover, a deployed online Spark SQL tuning service for efficient and safe search on industrial workloads. To address the challenges, we propose generalized transfer learning to boost the tuning performance based on external knowledge, including expert-assisted Bayesian optimization and controlled history transfer. Experiments on public benchmarks and real-world tasks show the superiority of Rover over competitive baselines. Notably, Rover saves an average of 50.1% of the memory cost on 12k real-world Spark SQL tasks in 20 iterations, among which 76.2% of the tasks achieve a significant memory reduction of over 60%.|像 Spark 这样的分布式数据分析引擎是工业中处理海量数据的常见选择。然而，Spark SQL 的性能在很大程度上取决于配置的选择，其中最佳配置随执行的工作负载而变化。在各种 Spark SQL 调优方案中，贝叶斯优化(BO)是一种流行的框架，它能够在预算充足的情况下找到接近最优的配置，但是它存在重新优化的问题，在实际生产中并不实用。当应用转移学习来加速调优过程时，我们注意到两个领域特有的挑战: 1)大多数以前的工作集中在转移调优历史，而来自 Spark 工程师的专家知识对于提高调优性能具有巨大的潜力，但是目前还没有得到很好的研究; 2)历史任务应该被仔细地利用，在使用不同的任务导致生产性能恶化的情况下。在本文中，我们介绍了 Rover，一个已部署的在线 Spark SQL 调优服务，用于在工业工作负载上进行高效和安全的搜索。针对这一挑战，我们提出了基于外部知识的广义迁移学习来提高调优性能，包括专家辅助的贝叶斯优化和受控历史迁移。在公共基准测试和实际任务上的实验表明，Rover 优于竞争基准测试。值得注意的是，在20次迭代中，Rover 为12k 实际 Spark SQL 任务平均节省了50.1% 的内存成本，其中76.2% 的任务实现了超过60% 的显著内存减少。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rover:+An+Online+Spark+SQL+Tuning+Service+via+Generalized+Transfer+Learning)|0|
|[Root Cause Analysis for Microservice Systems via Hierarchical Reinforcement Learning from Human Feedback](https://doi.org/10.1145/3580305.3599934)|Lu Wang, Chaoyun Zhang, Ruomeng Ding, Yong Xu, Qihang Chen, Wentao Zou, Qingjun Chen, Meng Zhang, Xuedong Gao, Hao Fan, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Root+Cause+Analysis+for+Microservice+Systems+via+Hierarchical+Reinforcement+Learning+from+Human+Feedback)|0|
|[Knowledge Based Prohibited Item Detection on Heterogeneous Risk Graphs](https://doi.org/10.1145/3580305.3599852)|Tingyan Xiang, Ao Li, Yugang Ji, Dong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Based+Prohibited+Item+Detection+on+Heterogeneous+Risk+Graphs)|0|
|[A Data-Driven Decision Support Framework for Player Churn Analysis in Online Games](https://doi.org/10.1145/3580305.3599759)|Yu Xiong, Runze Wu, Shiwei Zhao, Jianrong Tao, Xudong Shen, Tangjie Lyu, Changjie Fan, Peng Cui||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Data-Driven+Decision+Support+Framework+for+Player+Churn+Analysis+in+Online+Games)|0|
|[Multi Datasource LTV User Representation (MDLUR)](https://doi.org/10.1145/3580305.3599871)|Junwoo Yun, Wonryeol Kwak, Joohyun Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi+Datasource+LTV+User+Representation+(MDLUR))|0|
|[Understanding the Semantics of GPS-based Trajectories for Road Closure Detection](https://doi.org/10.1145/3580305.3599926)|Jiasheng Zhang, Kaiqiang An, Guoping Liu, Xiang Wen, Runbo Hu, Jie Shao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+the+Semantics+of+GPS-based+Trajectories+for+Road+Closure+Detection)|0|
|[TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter](https://doi.org/10.1145/3580305.3599921)|Xinyang Zhang, Yury Malkov, Omar Florez, Serim Park, Brian McWilliams, Jiawei Han, Ahmed ElKishky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TwHIN-BERT:+A+Socially-Enriched+Pre-trained+Language+Model+for+Multilingual+Tweet+Representations+at+Twitter)|0|
|[Online Few-Shot Time Series Classification for Aftershock Detection](https://doi.org/10.1145/3580305.3599879)|Sheng Zhong, Vinicius M. A. Souza, Glenn Eli Baker, Abdullah Mueen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Few-Shot+Time+Series+Classification+for+Aftershock+Detection)|0|
|[A Feature-Based Coalition Game Framework with Privileged Knowledge Transfer for User-tag Profile Modeling](https://doi.org/10.1145/3580305.3599761)|Xianghui Zhu, Peng Du, Shuo Shao, Chenxu Zhu, Weinan Zhang, Yang Wang, Yang Cao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Feature-Based+Coalition+Game+Framework+with+Privileged+Knowledge+Transfer+for+User-tag+Profile+Modeling)|0|
|[Fairness in Graph Machine Learning: Recent Advances and Future Prospectives](https://doi.org/10.1145/3580305.3599555)|Yushun Dong, Oyku Deniz Kose, Yanning Shen, Jundong Li|potsdam institute for climate impact research; ; university of reading; netherlands environmental assessment agency; polish academy of sciences; vu university amsterdam; vienna university of economics and business; university of grenoble; university of cambridge|Scenarios are used to explore the consequences of different adaptation and mitigation strategies under uncertainty. In this paper, two scenarios are used to explore developments with (1) no mitigation leading to an increase of global mean temperature of 4 °C by 2100 and (2) an ambitious mitigation strategy leading to 2 °C increase by 2100. For the second scenario, uncertainties in the climate system imply that a global mean temperature increase of 3 °C or more cannot be ruled out. Our analysis shows that, in many cases, adaptation and mitigation are not trade-offs but supplements. For example, the number of people exposed to increased water resource stress due to climate change can be substantially reduced in the mitigation scenario, but adaptation will still be required for the remaining large numbers of people exposed to increased stress. Another example is sea level rise, for which, from a global and purely monetary perspective, adaptation (up to 2100) seems more effective than mitigation. From the perspective of poorer and small island countries, however, stringent mitigation is necessary to keep risks at manageable levels. For agriculture, only a scenario based on a combination of adaptation and mitigation is able to avoid serious climate change impacts. Keywords Scenarios Integrated assessment Climate change Mitigation Adaptation Climate impacts 1 Introduction Scenario analysis forms a very important tool in the assessment of climate change and climate change policy, allowing analysts to explore the complex and uncertain future interactions between factors like economic development, greenhouse gas (GHG) emissions, climate and ecosystems. Together these factors determine the need and the possibilities for mitigation and adaptation policy. Scenarios can also act as a means to harmonize assumptions across very different research communities that are involved in the fields of climate research, allowing a better comparison of their results. As such, scenarios have been used extensively in both mitigation and adaptation studies (see Metz et al., 2007; Parry et al., 2007 ) (especially the scenarios from Special Report on Emission Scenarios (SRES) ( Nakicenovic et al., 2000 )). Moss et al. (2010) point out that since the SRES information requirements from scenario analysis are changing. First, there is an increasing interest in exploring the relationships between adaptation and mitigation. As indicated by Moss et al. (2010) , this would require a further integration of information across the different analytical traditions involved in climate research. Secondly, there is also an increased interest in scenarios that explicitly explore the impact of climate policies in addition to the climate policy-free scenarios explored so far. Specifically, there is a strong interest in being able to evaluate the “costs” and “benefits” of long-term climate goals vis-à-vis the situation without climate policy. In this paper, we follow this line of thought and explore how scenario analysis can contribute to a joint assessment of future adaptation and mitigation strategies. Such a joint assessment can be useful for several reasons: (1) the preferred mitigation strategy depends on expected climate impacts and adaptation costs, (2) it takes account of the limitations of adaptation to climate change, (3) some adaptation and mitigation strategies may interact and (4) finally, impacts of climate change may have important feedbacks that need to be taken into account. Such analysis is most useful at a strategic level, and not for individual adaptation (or mitigation) decisions. Given this purpose, we discuss in the paper two main scenarios that include elements of adaptation and mitigation strategies (see further in this paper), resulting in an increase of global mean temperature of 4 °C and 2 °C by the end of this century. These two temperature levels have started to become iconic numbers, representing a potential outcome in the situation without mitigation policy (4 °C) and the temperature target of international climate negotiations (2 °C) ( Copenhagen Accord, 2009 ). Arguably, understanding the implications of these two temperature levels is essential if political leaders are to make informed choices about the balance between mitigation, adaptation and climate impacts ( Environmental Change Institute, 2009 ). Integrated assessment of mitigation and adaptation strategies is hampered by methodological differences. Integrated assessment models have difficulties describing adaptation processes given the importance of local circumstances ( Patt et al., 2010 ). A practical problem is that to date a considerable part of the impact literature has concentrated on impacts under no-policy scenarios (exceptions include Arnell et al., 2002; Bakkenes et al., 2006; Hayashi et al., 2010; Krol et al., 1997; Nicholls and Lowe, 2004 ). This paper therefore presents a generalised scenario assessment based on coupled pieces of information – but without pretending to be complete or to be fully integrated. As a learning-by-doing exercise, the paper intends to show important differences between a 4 °C and a 2 °C world, but also to identify some of the practical issues involved in performing integrated scenario analysis. This implies that the most important advancement compared to existing literature is that we present a multi-sector analysis based on consistent scenarios. Given the state-of-the-art of current integrated assessment models, the experiments have been done using several loosely coupled models. As a result, several important linkages could not be addressed such as between the adaptation responses for agriculture, which may involve irrigation (see Section 5.3 ) and water demand (Section 5.4 ). In fact, an important question raised in the paper is whether a fully integrated analysis is needed or whether partial integration is sufficient. The paper is organized as follows: we first discuss some of the methodological complications in developing scenarios that can provide information for both adaptation and mitigation policy decisions. Next, we discuss the differences between the two main scenarios in terms of socio-economic drivers (Sections 3 and 4 ). In Section 5 we explore the potential consequences of adaptation and mitigation strategies on various impacts of climate change. 2 Assessment of climate strategies and scenario development (theory and methods) 2.1 Different strategies in response to climate change Climate change and the responses to it can lead to three forms of costs (not necessarily monetary): (1) the (residual) costs of climate impacts, (2) the costs of adaptation and (3) the costs of mitigation. At least theoretically, this corresponds to three different strategies: (1) “laissez faire” (accept climate change), (2) focus on adaptation and (3) focus on mitigation as illustrated conceptually in Fig. 1 (see also Klein et al., 2007 ). While Fig. 1 suggests that the costs and benefits of mitigation, adaptation and residual damages can be traded-off against each other, there are conceptual and analytical problems that complicate such an approach. These relate to spatial and temporal scales, and risks and uncertainty ( Swart and Raes, 2007 ). Mitigation and adaptation are processes that take place at different spatial s cales. While mitigation action is often taken at the national or local scale, the benefits are shared globally. As a result, a critical factor in the success and costs of climate policy is the degree of international cooperation ( Barker et al., 2009; Clarke et al., 2010; van Vliet et al., 2009; van Vuuren et al., 2009 ). For adaptation, in contrast, both costs and benefits occur on multiple scales from local to national and even international. An enabling environment at a larger scale can still enhance adaptation at a smaller scale (e.g. local capacity-building funded by international financing mechanisms). For these kinds of reasons, assessment of mitigation tend to concentrate on the global level, while by contrast, adaptation research is mostly focusing at the local scale. The dynamics over time of mitigation and adaptation is also an important factor. Stringent mitigation scenarios typically require strong, early reduction of emissions. Climate change impacts of these scenarios, however, will in the short-term (first decades) hardly differ from those in scenarios without climate change policy due to the large inertia within the climate system. In contrast, some associated impacts (e.g. co-benefits in reduced local air pollution) may be realized at a much faster pace. Adaptation measures are likely to yield private and social benefits over the near-term. For instance, simple adaptation measures such as air conditioning can bring clear short-term benefits. Some important exceptions exist which may require decades to implement, such as changes in spatial planning or large-scale engineering works for flood protection (see Hallegatte, 2009 ). Other important factors are risk and uncertainty . Our understanding of climate change faces many uncertainties. Key uncertainties to be identified comprise epistemic, data, model, and ontic uncertainties ( Schneider and Kuntz-Duriseti, 2002; van Vuuren et al., 2008a ). Examples of factors that involve uncertainty are (i) future emissions, (ii) the climate system, (iii) future vulnerability and exposure to climate risks and (iv) mitigation costs. Taking mitigative action reduces some uncertainties, since it reduces the originating sources of climate change and reveals the actual mitigation costs ( Barker, 2003; Piani et al., 2005 ). Mitigation may, however, also add to risks. For example, bio-energy, if implemented unsustainably, may offset one set of risks (climate change) while creating another set of different risks (biodiversity loss and reduced food security). One way of dealing with risks is to include assessments of probabilities. This is often done using past evidence, extrapolated to cover specific future circumstances. Other uncertainties (for instance unknowable shocks and surprises) are more difficult to deal with in quantitative sense, but justify acknowledgement of ignorance. Scenarios can be used to explore the potential for extreme events and the robustness of various policy portfolios but this is not often done ( Berkhout et al., 2002 ). Traditionally, the disciplines involved in mitigation research and adaptation research have different ways of describing uncertainty. While mitigation research often uses quantitative methods and concentrates on mean estimates, adaptation research often focuses more on qualitative descriptions of uncertainty and concentrates on the risks of hazardous events even if these have a low probability of occurrence. These different perceptions of uncertainty may complicate an integrated assessment of different strategies ( Swart et al., 2009 ). 2.2 Types of scenarios We can characterize scenarios into different classes based on the considerations about mitigation and adaptation. First, we define a baseline scenario, as a trajectory of events assuming no major feedbacks from climate change and no specific policy efforts on either mitigation or adaptation (such a scenario may still include many actions that indirectly influence the ability to mitigate or adapt to climate change; for instance, increasing income levels can be expected to coincide with greater investment in health services reducing the risks of climate-related diseases such as malaria). The main purpose of this type of scenario is analytical, serving as a point of reference for other scenarios. Second, adaptation scenarios describe a world in which societies are responding to climate change impacts. Their purpose is to explore the type of technologies and policies required to adapt to climate change, the avoided damage and the associated costs. Adaptation includes so-called autonomous adaptation (i.e. actions that occur without specific government action) and planned adaptation. Third, mitigation scenarios describe a world including policies aiming to limit climate change. Their purpose is to explore the type of technologies and policies required to minimize climate change and the associated costs. As there will always be remaining impacts, the fourth set, adaptation and mitigation scenarios combine both types of responses to climate change. Possibly, this fourth category of scenarios could re-order policy options according to the synergies that might exists between adaptation and mitigation options, e.g. for some re-afforestation options. Each of these scenarios is connected to a broader social, political and cultural context in which they are assumed to arise. In exploring a preferred mix of mitigation, adaptation and residual damage, two main approaches exist: (i) the impact and risk-based approach that describes potential impacts as function of global mean temperature increase (and thus mitigation), and (ii) the cost–benefit analysis, which identifies monetary costs and benefits in order to maximize welfare (see for instance Nordhaus, 2008; Tol, 2002c ). In both cases, we believe it to be more useful and reflective of the issue to describe the relationships between different response strategies than to seek to determine an optimum. Given the complexities and uncertainties laid out in Section 2.1 , we believe no optimal mitigation, adaptation or combined strategy can be pursued in reality. 2.3 Integrated analysis An integrated analysis of mitigation and adaptation can be achieved in different ways: e.g., by using one single, so-called integrated assessment model, or by exchanging information between different models and disciplines, assessing available literature and making results comparable. Both methods are organized around the cause–effect chain of climate change, i.e. describing the relationship between economic activities (income, energy use, agriculture, etc.), emissions, climate change and impacts – and the related feedbacks ( Fig. 2 ). The scheme in fact also forms the backbone of information flows around scenarios for the IPCC reports ( Moss et al., 2010 ). Scenarios are developed first by integrated assessment and emission modelers (focusing on economic driving forces, energy and land use and GHG emissions (IPCC “Working Group III”)). Subsequently, the emission trajectories are used in climate models to assess the impacts of climate change (IPCC “Working Group I”). Finally, the scenarios are used for impact, adaptation and vulnerability analyses (IPCC “Working Group II”). The involvement of different research disciplines and working groups implies that it is difficult to account for feedbacks between the different areas. Integrated Assessment models capture only a limited number of the possible feedbacks (frequently omitted feedbacks include the impact of food and water security on population and economic drivers; relationships between water scarcity and food production, impact of climate change on energy use, etc.). Ignoring (some of) these feedbacks may be reasonable if they are not substantial enough to significantly influence the system. For analytical reasons, there are major advantages to organizing scenario development within disciplinary fields and consider a limited number of feedbacks. It allows researchers to focus on elements of the chain that they understand well and to add the required amount of detail, without being confronted with the complications of interlinkages. However, this may change in a situation of increased focus on integrated analysis of mitigation and adaptation strategies. Some examples of why an integrated approach may be necessary are: i. Climate impacts, such as those triggered by extreme events, may be so severe that they undermine the economic assumptions of the original scenario; ii. Climate impacts could be substantial in agriculture so that estimates of land-use related emissions not taking impacts into account might be wrong, and the mitigation potential of bio-energy may be affected; and iii. There may be competing claims for land areas attractive for both mitigation and adaptation purposes. Thus, an interesting question is whether the need for more integrated analysis is so urgent that more complex modes of integration are needed (interactive coupling of models; one complex model), or whether the impacts can be handled separately simplifying the analysis framework. The time horizon and the decision focus may also be important here, e.g. whether potential tipping points are taken into account ( Lenton et al., 2008 ). The few available studies that have looked into this question seem to suggest that in most sectors the adaptation implications of any mitigation project are small as well as the emissions generated by most adaptation activities ( Klein et al., 2007 ). The most integrated analyses to date come from the cost–benefit oriented integrated assessment models like FUND, DICE and MERGE ( Manne and Richels, 2005; Nordhaus, 2008; Tol, 2002c ) – but these models typically aggregated climate impacts into a limited amount of rather abstract damage functions. We believe that over time, with growing intensity of both mitigation and adaptation measures across many sectors, the need for joint assessment with sufficient detail will intensify. The scenarios presented here, based on the current state of the art in modeling and scenario development, take a first step. The same scenarios are used in one assessment for mitigation and impact assessment and we explicitly address mitigation and adaptation strategies (either as part of the scenarios or within the models used for the different impacts). However, many feedbacks are not accounted for. We come back at the end of the paper to the role of more integrated (but also more complex) scenarios. 2.4 Methods used in this paper As described above, several types of scenarios can be identified: baseline, mitigation, adaptation and adaptation–mitigation scenarios. These scenario types are also presented in this paper. For the baseline/adaptation scenario, we assume intermediate assumptions for most socio-economic drivers. Scenarios assumptions are described in Sections 3 and 4 . The scenarios do not include mitigation, leading to a global mean temperature increase of 4 °C above pre-industrial levels by 2100. While we describe possible impacts and adaptation in these scenarios, we do not include feedbacks on the original drivers. In the mitigation scenarios, stringent mitigation efforts are included leading to a global mean temperature increase of 2 °C. Using the median value for climate sensitivity given by IPCC of 3 °C ( Meehl et al., 2007 ), this translates into a stabilization level of around 450 ppm CO 2 -equivalent (CO 2 -equiv.). The impacts of climate policy on economic drivers are not accounted for – but several other relationships are coupled (e.g. land use). In most of the paper, we thus ignore potential impacts of climate change and climate policy on the economic assumptions. In Section 5.8 , however, we discuss their impacts within a simple, economic model (FAIR) to provide some insight in the possible size of the economic consequences on the global scale. Several model tools are used. The scenarios are mainly developed using the IMAGE integrated assessment model ( Bouwman et al., 2006 ). The IMAGE model describes developments in energy and land use in the 21st century based on assumptions for population and the world economy, combined with assumptions for technology development and consumption patterns. The model projects climate change (as indexed by global mean temperature change and sea level rise) at the global scale, and constructs spatial scenarios for change in monthly temperature and rainfall at a 0.5° × 0.5° grid by pattern-scaling downscaled climate model patterns. The output of IMAGE is used in the model DIVA to describe sea-level rise; in the global hydrology model Mac-PDM to estimate consequences for water stress; in the TIMER energy model to estimate implications for heating and cooling demand; in the MARA/ARMA malaria suitability model for impacts on malaria and in the FAIR model for a monetary cost–benefit analysis. Moreover, we discuss more generally the implications for agriculture (based on IPCC AR4) and extreme events. Appendix A provides a brief description of all models used. In our descriptions, we focus on the global level (in view of the limited space). Clearly, this leads to limitations in our discussion of adaptation. The experiments depend on the design each model and thus the number of scenarios that can be presented differs between different impacts. This implies that the study should be interpreted as a first illustration of an integrated assessment, and not as a holistic study on adaptation and its limits. 3 Results: socio-economic trends in the baseline scenario 3.1 Population development and economic growth We assume that population follows medium-fertility variant of the 2004 revision of the World Population Projections ( UN, 2005 ) up to 2050, and the UN's long-range medium projections up to 2100 ( Fig. 3 ). This implies that the global population steadily increases to almost 9.1 billion people by 2050 and stabilizes at about 9.2 billion people over the subsequent 50 years up to 2100. The scenario takes a middle ground within the range of population forecasting (see Fig. 3 ). For economic growth up to 2050, the scenario follows projections linked to the Cambridge model E3MG ( Barker and Scrieciu, 2010; Barker et al., 2008 ). The scenario was extended beyond 2050 using the economic growth projections of the SRES-based B2 scenario ( IMAGE-team, 2001 ). Quantitatively, the scenario is a medium to high economic growth scenario, which is mainly the result of optimistic growth assumptions for China and India. The OECD economies are projected to remain the richest in the world in per capita terms, but in terms of total economic activity the importance of developing regions grows rapidly. The growth of GDP per capita is between 0 and 2% per annum in Africa, the Middle East and Latin America. In Asia, it falls from the current high levels to 3% per annum in 2050. 3.2 Energy use and greenhouse gas emissions for the baseline scenario Energy use in the baseline scenario is made consistent with a baseline published by the European Commission ( EC, 2006 ). Despite a further decrease of energy intensity, world energy consumption more than doubles in the 2000–2050 period and increases by another 25% in the 2050–2100 period ( Fig. 4 ). Over the whole century, energy supply remains dominated by fossil fuels. While oil and natural gas production peak and decline during the century, the use of coal increases during the whole scenario period. Also non-fossil energy production increases rapidly. Nuclear energy use increases by a factor of two to three to 76 EJ over the period until 2100, the use of biomass increases strongly, while hydro-electricity production increases by about 60–80%. The largest relative increase is that of wind and solar energy; this rises from less than 1% of all non-fossil energy to between 10 and 14% in 2050. Total renewable energy use in 2050 is 120–140 EJ, and 190 EJ in 2100. The trends described above imply that emissions of CO 2 from energy activities more than double in the period to 2050, and rise by another third between 2050 and 2100 (see Fig. 3 ). As such, the scenario forms an intermediate baseline scenario within the literature range ( Fisher et al., 2007 ). Non-CO 2 GHGs (in particular methane) increase steadily in the period 2000–2050, but at a slower rate than CO 2 (as their driver, agriculture, is expected to grow more slowly than the energy sector). CO 2 emissions from land-use fall back to zero during the first half of the century. The area of agricultural land lies within the range of similar scenarios that have recently been published, although at the low end of the range ( Rose et al., 2007 ). 4 Results for the mitigation scenario and climate scenarios 4.1 Energy use and greenhouse gas emissions The mitigation scenario aims at stabilising GHGs at around 450 ppm CO 2 -equiv. (see also van Vuuren et al., 2007, 2010 ). The scenario allows for an initial overshoot of concentration to about 510 ppm CO 2 -equiv. Den Elzen and van Vuuren (2007) have shown earlier that a limited overshoot of concentration allows for meeting similar climate targets at lower costs. Emission reductions are achieved in various ways. One element is to increase energy efficiency, which reduces the total amount of energy use (a 20% reduction in 2050 compared to baseline) (see Fig. 4 ). The scenario also shows an increasing use of energy from non-fossil sources, which account for most of the growth in total energy use. Non-fossil energy use increases from about 15% of total primary energy use in 2010 to more than 30% in 2050 and is over 40% of the total by the end of the century. Most of this growth is due to an increase in bio-energy use. Carbon capture and storage is applied in most remaining stationary uses of fossil fuels. Finally, also non-carbon dioxide greenhouse gas emissions are reduced. As a result, global emissions peak around 2020, and reduce further with time. Emissions are reduced by more than 70% compared to the baseline in 2050 and more than 80% by 2100. The consequences of mitigation policies affect not only the energy sector, but also land use. Substantial additional land areas are used for afforestation and bio-energy (see Fig. 5 ). Model comparison studies show that the mitigation scenarios presented here are consistent with the current literature, although models show significant differences in the contribution of various reduction measures ( Clarke et al., 2010; Edenhofer et al., 2010 ). According to the IMAGE model calculations, the abatement costs of the emission reductions are in the order of 1–2% of GDP (i.e. the annual additional expenditures which can be compared to the current expenditure of around 1.5% of GDP on environmental policy in OECD countries) ( Fig. 6 ). The literature range of comparable scenarios is in the order 0.5–5.5% in 2100. Most studies agree that these additional expenditures would lead to a reduction of GDP. We discuss this further in Section 5.8 . 4.2 Climate change under the baseline and mitigation scenario The atmospheric GHG concentration and associated mean global temperature change resulting from the emissions of the two scenarios is shown in Fig. 7 (solid lines indicate best-guess values), based on the IMAGE model calculations. The IMAGE model uses the MAGICC model to calculate changes in global mean temperature. The MAGICC model was used earlier for similar IMAGE scenarios by van Vuuren et al. (2008b) to calculate trajectories for greenhouse gas concentration and temperature including uncertainty ranges. Here, the uncertainty ranges used for the MAGICC calculations were based on existing runs of more complex carbon cycle and climate models. We have used the implications for ranges in greenhouse gas concentration and temperature outcomes to also depict the uncertainty ranges here as is indicated by the shaded areas in this graph. For temperature, the wider shaded area indicates the uncertainty as result of uncertainty in the carbon cycle and climate sensitivity. For the baseline scenario, global mean temperature increases almost linearly to 2.1 °C above the pre-industrial levels in 2050 and to 3.7 °C in 2100 (uncertainty range 3–5 °C). In the mitigation scenario, the global mean temperature increase by 2100 is limited to 1.9 °C. Again, there is considerable uncertainty. Fig. 7 indicates that by the end of the century the mitigation case could also lead to a temperature increase of 2.6 °C compared to pre-industrial levels. As the mitigation scenario presented here is among the most stringent in the scientific literature (cf. Clarke et al., 2010; Edenhofer et al., 2010; Fisher et al., 2007 ), two important conclusions can be drawn. First, the analysis indicates that global warming can be moderated but not halted. Second, the observation that a stringent scenario could also lead to considerably greater climate change than 2 °C may imply that hedging adaptation policies against more warming might have considerable value. For example, such policies may be to ‘… aim for 2 °C, but prepare for 3 °C’. In the assessment of impacts below, we focus on the central climate change projections. Changes in mean monthly temperature and precipitation across the globe at the 0.5° × 0.5° scale, associated with the global average temperature changes, have been constructed by rescaling patterns derived from the HadCM2 climate model ( Fig. 8 ). These patterns show that the change in annual mean temperature is larger at high latitudes than at low latitudes, and show considerable spatial variation in change in rainfall. Considerable disagreement about the expected patterns of climate change exists, especially for precipitation: the impact results presented in this paper therefore represent only one possible outcome. 5 Results: impacts and adaptation in the different scenarios 5.1 Introduction IPCC's Fourth Assessment Report ( IPCC, 2007 ) gives an overview of climate impacts. Some of these impacts result from changes in average climate, but other impacts may result from changes in extreme events. Table 1 summarizes some of the impacts, for health, agriculture, water availability, coastal flooding, urban areas and energy system, and large-scale disruptions of the climate system (in contrast, biodiversity and ecosystem services have not been included). As noted earlier, most of the literature has treated climate change as “a gradual phenomena” ( Agrawala and Fankhauser, 2008 ). This is problematic for impacts characterized by low probabilities coupled with high impacts (see below). In this exploratory analysis, we sketch some of the impacts and adaptation requirements. We aimed to cover several key impacts mentioned in Table 1 , but the assessment was limited by the availability of models that could easily be coupled. Therefore, rather than intending to be exhaustive, the descriptions provide some indication of the magnitude of some impacts and key adaptation challenges. In presenting our results, we have used several new model runs based on the scenario discussed above (e.g. for malaria, water resources, sea-level rise, heating and cooling demand). We have, however, also assessed existing information from IPCC 4th Assessment Report in the context of the two scenarios presented here (temperature-related mortality, agriculture and extreme events). 5.2 Human health: temperature-related mortality and malaria Health impacts of climate change need to be seen in the context of other, more important drivers of human health, including lifestyle-related factors ( Hilderink et al., 2008 ). We focus here on temperature-related mortality and malaria. 5.2.1 Temperature-related mortality Temperature-related mortality impacts may occur via changes in extreme temperatures, changes in average temperatures, or in seasonal variation of temperatures, with the literature showing varying results. McMichael et al. (1996) made an estimation of temperature-related mortality using relative risk ratios, showing that there is an optimum temperature at which the death rate is lowest (also know as the U-shaped dose–response relation). If temperature increases, heat stress-related mortality increases, but cold-related mortality decreases. Tol (2002a) concluded that in monetary terms the reduction in cold-related mortality due to climate change outnumbers the increase in heat-related mortality. This conclusion is however, influenced by the approach used to value a life and also subject to the large uncertainties with respect to the relationships between average and regional temperatures and temperature and health. Adaptation may occur both by the adjustment of the human physiology to higher temperatures ( McMichael et al., 1996 ), changes in behavior and an increase of air conditioning use ( Kinney et al., 2008 ). Given the complexities in using dose–response relationships between temperature and mortality, we have not attempted to quantify these here. 5.2.2 Malaria Considerable attention has been paid to the relationship between malaria and climate change. In this paper, we also focus on climate-induced changes in malaria risks. Annually more than one million people, mostly African children, die from malaria, a vector-born infectious disease. The anopheles mosquitoes (the vector which spreads the malaria infection) can only survive in climates with high average temperatures, no frost and sufficient precipitation. The MARA/ARMA malaria suitability model ( Craig et al., 1999 ) incorporates these factors to determine climatically suitable areas. Mortality due to malaria is, however, also heavily influenced by factors such as access to preventative measures (including indoor spraying and insecticide-treated bed nets) and access to health care. In the MARA/ARMA model these factors are linked to income and urbanization. Fig. 9 shows the results of this model for the scenarios of this paper. The impact of autonomous adaptation (as function of rising income) reduces malaria deaths by around 50%, especially in Africa (mainly due to better provision of health care). In contrast, the impacts of climate – and especially the difference between the mitigation scenario and the baseline case are much smaller. Mitigation reduces malaria health risks by about 2% (2050). Adaptation, therefore, has a much more decisive influence on malaria control than mitigation (this finding seems to be robust with available literature). 5.3 Agriculture: impacts on yields Easterling et al. (2007) have synthesized a large amount of research on the impacts of climate change on crop growth, with and without adaptation. The results were summarized as a function of global mean temperature increase, although in reality changes in temperature and precipitation patterns and CO 2 fertilisation all play a role. For instance, the impacts of CO 2 fertilisation partly offset the impact of climate change. The results can be used to assess the climate impacts for our scenarios by using the best-fit polynomials from Easterling et al. (2007) , that indicate the impact on yield as a function of mean temperature change. 1 1 We have in each case taken the global mean temperature change for a scenario and used that as an indication of the average local temperature change to be expected. This means that our impact estimates are likely to be conservative, as temperature increase is likely to be stronger the global average over many land areas. We looked at the impacts for the baseline (4 °C) and mitigation (2 °C) scenario, with and without adaptation, for maize, wheat and rice (see Fig. 10 ; results are presented for tropical and temperate zones in 2100; these impacts are additional to the yield increases as a result of other factors than climate change). Although the results are very uncertain, some conclusions seem to be possible. First, the baseline scenario (no adaptation) causes a very substantial decrease in yields (relative to the situation without climate change) for all cases shown: Climate change impacts may reduce yields for the aggregated regions shown by 10–35% for the crops studied (2050). Second, engaging in either mitigation or adaptation limits the decrease in yields. In the tropics, however, impacts remain negative and typically in the order of a 10% loss. Third, the combination of mitigation and adaptation may result in an improvement from today's situation. Agricultural impacts may be more positive for temperate regions, but only if the advantages of higher temperature are not offset by impacts of extreme weather. These results underline the need to look at both mitigation and adaptation. The results presented are based on the IPCC assessment and represent a wide range of models. The results can also be illustrated by individual studies. Tubiello and Fischer (2007) , for instance, found that a mitigation scenario could reduce the global costs of climate change in agriculture significantly. Similarly, Fischer et al. (2007) illustrated the importance of adaptation for water irrigation requirements. They found that mitigation reduced agricultural water requirements by about 40%, leaving 60% of the impacts requiring adaptation. When dealing with impacts on agriculture both drought and heat wave stress play important roles. Fig. 11 shows, for Europe, the impact of drought and heat wave stress on crop yields for a 2 °C warming scenario, assuming various forms of adaptation ( Mechler et al., 2010; Moriondo et al., 2010 ). 2 2 Calculations were done using the Cropsyst model on the basis of the HADCM3 climate model for the 2030–2060 time slice. Winter and summer crop yields were simulated for spring wheat with today's and future crop management practices. Adaptation options considered comprised shifting the sowing date by a few days and using cultivars with a longer/shorter growth cycle. Results show that Southern Europe and parts of France are today already particularly exposed to drought and heat stress, and this situation is expected to worsen even under the 2 °C (mitigation) scenario ( Fig. 11 panel A). When considering the two adaptation strategies in combination with mitigation ( Fig. 11 panels B and C), many regions in Europe may actually benefit. Northern Europe, in particular, could exploit the advantage of higher precipitation by using crop varieties with a longer growing cycle. In contrast, in Southern Europe the same adaptation options would result in an added negative impact, since crop development would shift towards summer when longer dry spells and heat waves may significantly affect crop growth. Also, the results show that while there are some region-specific limits to adaptation, overall adaptation would effectively reduce impacts on the agricultural sector in Europe. 5.4 Water resources: potential water availability The effects of the two scenarios on exposure to changes in water resources stress are assessed using a global-scale water resources impact model ( Arnell, 2003 ). Fig. 12 shows the percentage change in average annual runoff by 2100 (relative to the 1961–1990 mean) under the baseline scenario and the mitigation scenario (with the HadCM2 climate model pattern). We define watersheds to be in a water-stressed condition if average annual runoff is less than 1000 m 3 /capita/year (other definitions are also used in the literature). The effect of climate change is indexed by summing (i) the populations living in water-stressed watersheds where runoff decreases (increases) significantly (typically by more than 5–10%) and (ii) the population living in watersheds that become water-stressed (cease to be water-stressed) due to climate change. The number of people exposed to an increase or decrease in water stress due to climate change have not been summed for two reasons: (i) the adverse effects of having less water are greater than the beneficial effects of having more water in a water-stressed catchment, and (ii) the regions with an increase and decrease in exposure to water resources stress are widely separated, and “surpluses” in one area do not offset “deficits” in another. The results show substantial differences in exposure to increased water resource stress in 2050, 2080 and 2100 between the mitigation and baseline scenarios. In 2020, there is little difference in runoff between the two scenarios. Fig. 13 shows the numbers of people exposed to an increase or decrease in water resource stress due to climate change under the two scenarios. In both the baseline and the mitigation scenario, the numbers of people living in water-stressed watersheds who apparently benefit from increased water availability is larger than the numbers exposed to a reduction in runoff, but – as outlined above – we do not focus on the net effect. The numbers of people exposed to change in water resources stresses are sensitive to the assumed pattern of climate change. Compared to the baseline, the mitigation scenario reduces the numbers exposed to an increase in water resources stress by 135 million (reducing impacts by 12%), 281 million (20% reduction) and 457 (30% reduction) million in 2050, 2080 and 2100 respectively. At the same time, however, there are also people benefiting from climate change. The relative size of the groups with positive and negative impacts depends on the climate model used (here only the Hadley pattern has been used). Clearly, mitigation also decreases the number of people benefiting from climate change. It is also clear that mitigation does not eliminate water supply impacts of climate change, and adaptation will be required for the remaining billion people exposed to increased water resource stress due to climate change. Adaptation may include measures to increase water storage, transport of water, or reduction of water demand by increasing efficiency. Underlying results show that the effects of mitigation vary significantly by region. In fact, in some regions mitigation may even increase the numbers of people exposed to increased stress. Specific uncertainty analysis shows that results are highly dependent on the uncertainty in the changes in the precipitation pattern due to climate change. 5.5 Sea level rise Another important impact of climate change is rising sea levels. Global mean sea-level rise has been projected for both scenarios using the MAGICC component of the IMAGE model. Due to the delayed response of sea-level to global warming, the projections mainly diverge in the second part of the century: sea level rise is 35 and 31 cm in 2050 in the 4 °C and 2 °C scenario, respectively and 71 and 49 cm in 2100. These projections do not include a potential accelerated contribution of the ice sheets of Greenland and Antarctica, which could lead to higher sea-level rises but the underlying processes are insufficiently understood and are currently not included in climate models ( Meehl et al., 2007; Nicholls et al., 2010; Vermeer and Rahmstorf, 2009 ). We use the DIVA model to assess both damage and adaptation costs of sea-level rise, associated storm surges and socio-economic development under the two scenarios taking into account coastal erosion (both direct and indirect), forced migration, coastal flooding (including rivers) and salinity intrusion into deltas and estuaries. For each scenario the model is run first without and then with adaptation in terms of raising dikes and nourishing beaches ( DINAS-COAST Consortium, 2006; Hinkel and Klein, 2009 ). Further impacts such as salinity intrusion in coastal aquifers, loss of coastal wetlands and biodiversity as well as further adaptation options such as salinity intrusion barriers, port upgrade, set-back zones and ecosystem-based protection could not be included due to the unavailability of global data and general models of these processes. Fig. 14 shows that independent of the level of mitigation, adaptation reduces global overall costs rather effectively, which illustrates the necessity for engaging in adaptation even under ambitious mitigation. At the aggregated scale more damages can be avoided through an adaptation-only strategy than through a mitigation-only strategy, although a combination of the two has the strongest positive impact. From the perspective of poorer and small island countries, however, stringent mitigation is necessary to keep risks at manageable levels. Even without sea-level rise, adaptation would be cost-effective in order to protect the assets situated in the floodplain, which increase due to socio-economic development alone. While this would involve substantial investment flows (tens of billions of US$ worldwide), they are a relatively small fraction of global GDP, even for sea level rise at the level of the baseline scenario. However, for individual countries or regions (particularly small island states) these costs can be a much larger fraction of GDP, including the risk of a complete loss. 5.6 Heating and cooling demand (settlements and society) Climate change is likely to influence the demand for space cooling and heating. Therefore, we have developed a set of simple relationships to describe heating and air conditioning demand in the residential sector and explored the impacts of climate change on this simulated energy demand ( Isaac and van Vuuren, 2009 ). Clearly, changes in population and income are projected to lead to a considerable growth in the energy demand for heating and air conditioning in the coming century (see Fig. 15 , no climate change case). Driven by climate, changes in cooling and heating practices are examples of autonomous adaptation (i.e. without policy intervention). Adaptation is not universal, however, since the population will not always be able to respond. Unfulfilled demand for heating and cooling can lead to health impacts (as described in Section 5.2 ) and to loss of labour productivity. In addition to these effects, there is reduced comfort when indoor temperatures rise above a given level. Fig. 15 shows that, globally, the autonomous increase in energy demand without taking climate change into account due to increasing income and wealth is much larger than the difference between the energy demand in the baseline scenario and the mitigation scenario ( Isaac and van Vuuren (2009) show this a robust result also for other baselines). The effect of climate change on combined energy demand is also smaller than the effect on heating and air conditioning separately, since increases in air conditioning compensate decreases in heating. On the regional and country level, impacts can be far more significant: for example, in India we project a large increase in energy demand due to increased cooling, while in Western Europe and the USA, we project a substantial decrease due to reduced heating. 5.7 Extreme events Climate change is expected to lead to changes in the frequency and intensity of some weather-related extreme events ( Parry et al., 2007 ). Extremes like floods, droughts, heat waves and storm surges could become more frequent and intense, while cold-extremes, such as cold spells, are likely to become less frequent and weaker. Assessing risks of climate change based on changes in average conditions-only runs the risk that changes in extreme event risks are averaged out. A more risk-based, geographically explicit method is therefore preferable. However, knowledge on disaster impacts is complex and contested. To date, there are only a limited number of national level studies taking a probabilistic approach to projecting future risk in the presence of climate change, mostly focusing on flood risk ( Mechler et al., 2010 ). One such study on the pan-European scale by Feyen et al. (2009) computed that expected annual damages would triple under a baseline scenario. A key constraint to quantitative risk approaches is the uncertainty in the climate projections. For precipitation, for instance, models often disagree on the sign of changes at the local scale. This is especially important for studies looking for instance flood risk. While the Mechler et al. (2010) study aimed to project future risk, they found future projection to be so uncertain that the authors refrained from projecting future flood risk based on an estimate of today's flood impacts. Current models and data, however, seem to be sufficient to assess the combined risk of drought and heat wave stress on agriculture with a relatively high level of certainty (slower phenomena). Some examples of work in the context of the 2 °C and 4 °C scenarios are provided here. Several studies looked into flood-affected people at the global scale ( Hirabayashi and Kanae, 2009; Kundzewicz et al., 2010 ). Regression of samples shows that the average global number of people affected by 100-year floods per year for the mitigation scenario (2 °C) is projected to be 211 million compared to 544 million for the baseline (4 °C). Mirza et al. (2003) showed that for Bangladesh, a flood-vulnerable country, even the 2 °C scenario is expected to increase the projected flooded area by at least 23–29%. It should be noted, however, that the uncertainties about exposure, vulnerability and adaptation still lead to a wide range of estimates for the costs of future flood damage. With respect to drought, the projections for the 2090s made by Burke et al. (2006) show that the number of extreme drought events per 100 years and mean drought duration are likely to increase by factors of two and six, respectively, for the baseline scenario by the 2090s. Evidence suggests that damage of weather and climate related impacts has already increased in the present-day, but these are mainly due to the wealth and population increases ( Bouwer, 2010 ). However, climate change is expected to increase over time, and is likely to become a more significant contributor to rising damages in the future. The most recent IPCC report indicates that the costs of major events are expected to range from several percent of annual regional GDP and income in very large regions with very strong economies, to more than 25% in smaller areas ( Parry et al., 2007 ). Disaster losses for highly exposed small island states in the past have in fact exceeded annual GDP ( Cummins and Mahul, 2009 ). 5.8 Economic evaluation of impacts Cost–benefit analysis (CBA) is used to express the costs and benefits of climate change of different strategies in terms of a common monetary unit. We use the CBA module of the FAIR model (see model Appendix A ) here to obtain some idea of impacts at a more aggregated scale. For mitigation costs, the FAIR model uses the information of the IMAGE model presented earlier. The climate damage and adaptation cost functions used in FAIR are derived from the AD-DICE model ( De Bruin et al., 2009a; Hof et al., 2009a ). In short, AD-DICE estimates adaptation costs based on the damage function of the DICE model ( Nordhaus and Boyer, 2000 ). The AD-DICE separates these functions into a damage cost function and residual damage function based on an assessment of each impact category described in the DICE model – agriculture, coastal zones, health, settlements, non-market time use, other vulnerable markets and catastrophic impacts. For this study, we assumed an optimal adaptation response to climate change (i.e. given a level of temperature change the model minimizes the sum of adaptation costs and residual impacts). The impact estimates used in DICE (and thus FAIR) include: (i) real, measurable, economic costs (so-called market costs); and (ii) other, intangible losses (non-market losses), which are monetized using the willingness-to-pay concept. The damage functions are not directly related to the physical or economic damages described earlier in this section, as they are derived from a separate source. It has been shown earlier that the FAIR results of adaptation costs are consistent with the range of values reported in the literature ( Hof et al., 2009a ). Under default settings of the FAIR model and a discount rate of 2.5%, the discounted costs as a share of global GDP due to climate change impacts for the period 2005–2200 amount to nearly 4.5% in the baseline ( Fig. 16 ). These costs may seem higher than suggested by the limited set of sectoral analyses presented above, but include more sectors and also the impacts of possible catastrophic events ( Nordhaus and Boyer, 2000 ). Annual costs rise sharply over time, reaching 17% in 2200 (note that impact estimates are very uncertain and both higher and lower values can be found in the literature ( Parry et al., 2007; Stern, 2006; Tol, 2002b )). Scenarios with only adaptation or mitigation reduce discounted costs substantially to around 2.5% ( Fig. 16 ). Hof et al. (2008) have shown that the results of CBA of climate change are very sensitive to model assumptions, with the discount rate playing the most important role. The discount rate is especially important due to the different costs function over time related to the adaptation only and mitigation only scenarios. 3 3 A discount rate of 5% leads to discounted costs of 0.8% and 1.9% for the adaptation-only scenario and mitigation-only scenario, respectively. If a discount rate of 1.4% is used (equal to the discount rate used by Stern (2006) ), the discounted costs are 3.2% and 2.5% for the adaptation-only scenario and mitigation-only scenario, respectively. With our discount rate of 2.5%, the combination of mitigation and adaptation leads to the lowest discounted costs, namely 2% of GDP. Consistent with literature, the adaptation investments are assessed to be smaller than mitigation investments and residual damages. However, they are very important in limiting residual damages. Some important caveats need to be mentioned. First, calculations cannot be regarded as reliable for the extreme tails of risks (i.e. low probability, high impact events). As a subjective assessment on how to handle such risks is involved, Weitzman (2008) questioned the usefulness of CBA for policymakers. Secondly, the value of the discount rate to account for time preference and risk is currently heavily debated, with arguments relating to subjective time preference and risk perception ( Nordhaus, 2008; Price, 2005; Stern, 2006 ). As mentioned above, the value of the discount rate can have a large effect on the results. Finally, non-market impacts need subjective quantification of damages; while it is difficult to monetize these impacts, in general, it is even more difficult for irreversible changes, for example a warming of the oceans leading to the loss of coral reefs ( Ackerman and Heinzerling, 2004 ). 5.9 Uncertainties in climate change, impacts and adaptation There are many sources of uncertainty in projections of future climate change and its impacts. Uncertainties are associated with every step in the causal chain: emissions, climatic drivers (e.g. the carbon cycle), climate (mainly climate sensitivity and pattern of climate change), and impacts (including adaptive capacity). As a result, different studies might give very different results for the same emission scenario. In fact, these differences are often larger than those arising in a particular model under different emission scenarios. For example, for precipitation changes at the end of the century, the multi-model ensemble mean exceeds the inter-model standard deviation only at high latitudes ( Kundzewicz et al., 2007 ). Uncertainties in climate change projections increase with the length of the time horizon. In the near term (e.g., the 2020s), climate model uncertainties play the most important role; while over longer time horizons (e.g. the 2090s), uncertainties due to the selection of emissions scenario become increasingly significant ( Jenkins and Lowe, 2003 ). The impact of future climate change on extreme events is particularly uncertain. This is partly due to a mismatch between the larger spatial and temporal scale of coarse-resolution climate models, and the local occurrence and short life of some weather extremes (e.g. cloudburst precipitation and flash floods). As impacts and adaptation take place at the local scale, detailed information is needed – which implies an increase in uncertainty. The large uncertainty ranges suggests that planning for adaptation should not be based on a single scenarios, but that a large range of projections need to be account for. 6 Conclusions In this paper, we have discussed how scenario analysis may contribute to the assessment of mitigation and adaptation strategies. We have also presented two integrated scenarios as a starting point for analysis. The scenarios have explicitly treated mitigation and adaptation action for several indicators – and cover several important linkages and feedbacks between socio-economic development and impacts (e.g. the impacts of climate change on land use and mitigation are accounted for). We specified impacts in those scenarios for a selected number of indicators, focusing mainly on mean climate changes. Based on our work, we draw the following conclusions: • By describing two contrasting sets of possible climate change trajectories for the world, we have created the basis for a more integrated analysis of the interaction between mitigation, adaptation and climate impacts. The first scenario (no mitigation) is expected to lead to a global mean temperature increase by the end of the century of around 4 °C (for the most likely values for climate parameters, and current economic trends). This scenario has high adaptation needs as has been shown in some of our analyses. The second scenario assumes stringent mitigation and limits global mean temperature change to 2 °C, with a probability of 50%. Even under this scenario, substantial adaptation measures will be needed. • Integrated scenario analysis as presented here can form a good basis for exploring the different consequences of policy choices (including uncertainties); it is not feasible, given uncertainties to determine an optimal mix between mitigation, adaptation and residual damages. As discussed in this paper, the weighing of the consequences of climate change and the various policy responses is complicated by large differences in scale, space and time; large uncertainties; and clear differences in interest between actors (whether they are perpetrators or victims of climate change, for instance). As a result, subjective interpretation of risks will always play an important role. Still, scenario analysis can provide a description of possible consequences and risks. At this stage, the monetary assessment of cost and benefits (Section 5.8 ) could not be linked to the description of physical change in the preceding sections. • Effective climate policy includes both adaptation and mitigation. Model calculations show that mitigation scenarios can be designed that lead to an increase of global mean temperature increase 2 °C for a best-guess climate sensitivity. However, even these stringent scenarios can still also result in a global mean temperature increase of more than 2.5 °C (and at best a temperature increase of 1.5 °C) and regional temperature change which is far greater. The need for a combination of mitigation and adaptation has been shown for most of the impacts explored in this paper. For example, adaptation can be more effective than mitigation in dealing with sea-level rise (at least during the 21st century), but mitigation still has a role to play in reducing damages and costs of adaptation. Agriculture presents an example where adaptation and mitigation are both clearly necessary. Crop yields in agriculture are projected to suffer negative impacts in many regions due to climate change in the absence of both adaptation and mitigation action. Without stringent mitigation, adaptation could limit negative impacts, but not remove them. An advantage of mitigation is that it affects all impact categories, while adaptation needs to be tailored to impacts and contexts. • While impacts of climate change can be severe and, depending on subjective choices, may warrant stringent climate policy, the impacts assessed in this study (given the state of the art) are likely to remain secondary influences of population change and economic growth at a global scale. Yet important caveats apply (see below). While climate change may have an impact on millions of people, other challenges are likely to influence people and governance more significantly. It should be noted, however, that we have covered only a limited set of impacts and focused mostly on mean estimates of gradual climate change and, for instance, not on catastrophic, very high-impact, extremely low-probability events ( Weitzman, 2008 ). Such events in fact may be so severe that the conclusion above no longer holds. If costs at a global scale remain relatively low, there is less need for global analysis to include all feedbacks on main drivers based on the consistency of the storylines. Clearly, at the local scale the situation is likely to be very different; impacts for individual countries can be far more substantial than at the global scale. For example, sea level rise is very important for some low-lying island states and countries that could be significantly affected by either large adaptation costs and/or damages (up to complete destruction). For agriculture, positive and negative impacts are projected to occur in different places and at different times – with low-income countries often experiencing relatively more negative impacts. Agriculture in temperate regions, where it is currently temperature-limited, could benefit. All in all, we believe that it useful to pursue further the development of integrated scenarios specifying these further on a regional scale. While this paper presents a useful first step, it also has left many feedbacks still unaccounted for. • The overall mitigation costs in this study are estimated to be in the order of 1–2% of GDP for the 2 °C scenario. The mitigation scenario reduces the risks of climate change. There are several types of benefits of investments in mitigation. First, climate-related damages and the costs of adaptation are reduced. Second, also uncertainty is reduced, which is important given the risks involved. While we argue there can be no optimal trade-off between mitigation and adaptation at a global level, we have shown that over the longer-run the costs and benefits of mitigation and adaptation are of an equivalent magnitude. • Important foci for further analysis include the linkages between assessment of physical changes and monetary impact analysis, variability and changes in extreme events, the potential role of large scale disruptions and governance. In our and other assessments, the focus has mostly been on changes in mean values, yet there is considerable concern about extreme events (resulting in natural disasters) associated with climate variability, but also in large scale disruptions (such as the disintegration of the West Antarctic Ice Shield), which are not accurately described by average values. Projections of changes in climate variability have been highly uncertain, and to date often hinder analyses from robustly predicting future extreme event risk. The role of different actors is another issue; some forms of adaptation require active governmental involvement; other forms are likely to be implemented by private investors, such as installation of space cooling systems. The differences between these two adaptation protagonists are relevant for future scenario development. Acknowledgements The research presented in this paper was performed as part of the EU-funded ADAM research project. An earlier version of this paper was published as part of the book “Making Climate Change work for us” edited by Hulme and Neufeld and published by Cambridge University Press in 2010. Appendix A Model descriptions A.1 IMAGE 2.4 The IMAGE 2.4 Integrated Assessment model ( Bouwman et al., 2006 ) consists of a set of linked and integrated models that together describe important elements of the long-term dynamics of global environmental change, such as air pollution, climate change, and land-use change. As part of IMAGE, the global energy model TIMER ( van Vuuren et al., 2006 ) describes the long-term dynamics of demand and production of primary and secondary energy and the related emissions of greenhouse gases and regional air pollutants. The model behavior is mainly determined by substitution processes of various technologies on the basis of long-term prices and fuel-preferences. The agricultural model of IMAGE models the productivity of 7 crop groups and 5 animal categories ( Leemans and Born, 1994 ). The regional production of agricultural goods is distributed spatially (at 0.5° × 0.5°) on the basis of a set of allocation rules ( Alcamo et al., 1998 ). Both the land use change maps and the agricultural activity data are used to model emissions from land use (change). The emissions of GHGs are used by the MAGICC model to calculate global mean temperature change ( Wigley and Raper, 2001 ). Patterns of temperature change are obtained by making a link to climate change patterns generated by a general circulation models (GCM). Limitations : IMAGE is provides a physically oriented description of human activities (use of tons of oil, production of tons of cereals, etc.). A fuller macro-economic description only emerges from cooperation with other models. The broad coverage of IMAGE as Integrated Assessment Model implies that many critical uncertainties influence the model outcomes. In this context, use of a single baseline (as in the ADAM project) does not do fully justice to the fundament uncertainties involved. A.2 FAIR The climate policy model FAIR ( Den Elzen et al., 2008 ) is used in conjunction with the IMAGE model to determine the reduction rates across different emission sources. Global climate calculations make use of the simple climate model, MAGICC 4.1 ( Wigley, 2003; Wigley and Raper, 2001 ). Required global emission reductions are derived by taking the difference between the baseline and a global emission pathway. The FAIR cost model distributes these between the regions following a least-cost approach using regional marginal abatement costs curves (MACs) for the different emissions sources. Recently, the FAIR model has been extended with damage and adaptation costs curves (based on the AD-DICE model ( De Bruin et al., 2009b ) and the ability to estimate macro-economic impacts on GDP growth ( Hof et al., 2008 )). This allows the model to explore the economic impacts of combined mitigation and adaptation strategies. Limitations : In its aim to be flexible, the FAIR model does not include a sectoral macro-economic model or an energy model. The model thus works from a partial equilibrium approach – and more underlying consequences of climate policy can only be studied by forwarding the FAIR results to other (linked) models. A.3 DIVA DIVA (Dynamic and Interactive Vulnerability Assessment) is an integrated model of coastal systems that was developed, together with its proper coastal database, within the EU-funded project DINAS-COAST 4 4 Dynamic and Interactive Assessment of National, Regional and Global Vulnerability of Coastal Zones to Sea-Level Rise; http://www.pik-potsdam.de/dinas-coast/ . ( DINAS-COAST Consortium, 2006; Hinkel and Klein, 2009 ). DIVA produces quantitative information on a range of ecological, social and economic coastal vulnerability indicators from sub-national to global scales, covering all coastal nations. The model consists of a number of modules developed by experts from various engineering, natural and social science disciplines. Based on climatic and socio-economic scenarios, the model assesses coastal erosion (both direct and indirect), coastal flooding (including rivers), wetland change and salinity intrusion into deltas and estuaries. DIVA also considers coastal adaptation in terms of raising dikes and nourishing beaches and includes several predefined adaption strategies such as no protection, full protection or optimal protection. Limitations : DIVA excludes the following processes that are likely to affect coastal impacts, but can currently not be modeled with confidence: changes in storm frequency and intensity, local distribution of GDP and population growth due to rapid coastal development and urbanization, and salinity intrusion into coastal aquifers. Further important uncertainties arise due to the coarse resolution and accuracy of elevation data. A.4 TIMER-cooling/heating energy demand The TIMER cooling/heating energy demand model ( Isaac and van Vuuren, 2009 ) describes the energy use for cooling and heating as a function of several factors, including population levels, changing income levels and climate. For both heating and cooling, empirical data is used to calibrate a set of system-dynamic demand functions. Climate (cooling and heating degree days) plays an important role. The model is able to account for the impacts of climate change. Limitations : The empirical basis on which the model is calibrated is relatively poor for developing countries. The model does not contain a description of different ways cooling and heating demand can be supplied and the costs involved in substituting one technology for the other. A.5 Water resources impact model The water resources impact model ( Arnell, 2003, 2004 ) has two components. The first simulates river runoff across the entire global land surface (at 0.5° × 0.5°) using the macro-scale hydrological model Mac-PDM, and the second determines indicators of water resources stress at the watershed level by calculating per capita water resource availability. A watershed is assumed to be exposed to water resources stress if it has an annual average runoff equivalent to less than 1000 m 3 /capita/year, a semi-arbitrary threshold widely used to identify water-stressed regions. Climate change leads to an increase in exposure to water resources stress if it causes runoff in a water-stressed watershed to decrease significantly, or causes the watershed to fall below the threshold. Climate change leads to an apparent reduction in exposure for the opposite trends. These changes cannot be directly compared; whilst a reduction in runoff (and an increase in exposure) is highly likely to be adverse, an increase in runoff (and apparent decrease in exposure) may not be beneficial if the additional water cannot be stored or if it occurs during high flow seasons as increased flooding. The number of people living in watersheds exposed to an increase in water resources stress can be used as an indicator of exposure to climate change. The actual impacts (in terms of real water shortages) will depend on water management structures in place. Limitations : The hydrological model does not simulate perfectly the volume of river runoff, and in particular tends to overestimate runoff in semi-arid regions. The water resources indicator is a measure of exposure to impact, not actual impact; it can be seen as a surrogate for the demand for adaptation. A.6 Malaria risks Malaria vectors, the mosquitoes spreading the infection, can only survive in suitable climates with high average temperatures, no frost and enough precipitation. The MARA/ARMA malaria suitability model ( Craig et al., 1999 ) incorporates these climatic factors to determine climatic suitable areas. The climatic levels required for the maximum suitability of 1, and for the minimum suitability of 0, are shown in Table A.1 . For indicators with levels between those required for 0 or 1 suitability a level is calculated using s simple function ( Craig et al., 1999 ). All these factors are calculated at half by half degree grid level, making use of the output from the IMAGE-model ( Bouwman et al., 2006 ). Total climatic malaria suitability for each grid cell is determined by the lowest of these three indices. Limitations : The MARA/ARMA model describes suitability for malaria vectors. It does not provide a process description of the spread of mosquitos, nor does it explicitly describe how people may react to increased risk levels. References Ackerman and Heinzerling, 2004 F. Ackerman L. Heinzerling Priceless: On Knowing the Price of Everything and the Value of Nothing 2004 The New Press New York Agrawala and Fankhauser, 2008 S. Agrawala S. Fankhauser Economic Aspects of Adaptation to Climate Change. Costs, Benefits and Policy Instruments 2008 OECD Paris Alcamo et al., 1998 J. Alcamo E. Kreileman M. Krol R. Leemans J. Bollen J.V. Minnen M. Schaeffer S. Toet B. de Vries Global modelling of environmental change: an overview of IMAGE 2.1 J. Alcamo R. Leemans E. Kreileman Global Change Scenarios of the 21st Century. Results from the IMAGE 2.1 Model 1998 Elsevier Science Ltd. Oxford 3 94 Arnell, 2003 N. Arnell Effects of IPCC SRES emissions scenarios on river runoff: a global perspective Hydrology and Earth System Sciences 7 5 2003 619 641 Arnell, 2004 N. Arnell Climate change and global water resources: SRES emissions and socio-economic scenarios Global Environmental Change 14 1 2004 31 52 Arnell et al., 2002 N.W. Arnell M.G.R. Cannell M. Hulme R.S. Kovats J.F.B. Mitchell R.J. Nicholls M.L. Parry M.J.L. Livermore A. White The consequences of CO 2 stabilisation for the impacts of climate change Climatic Change 53 4 2002 413 446 Bakkenes et al., 2006 M. Bakkenes B. Eickhout R. Alkemade Impacts of different climate stabilisation scenarios on plant species in Europe Global Environmental Change 16 1 2006 19 28 Barker, 2003 T. Barker Representing global climate change, adaptation and mitigation Global Environmental Change 13 2003 1 6 Barker et al., 2009 Barker, T., Kenber, M., Scrieciu, S., Ryan, D., 2009. Breaking the Climate Deadlock. Cutting the Cost: The Economic Benefits of Collaborative Climate Action. The Climate Group, The Office of Tony Blair, 4CMR – University of Cambridge and Cambridge Econometrics. Barker and Scrieciu, 2010 T. Barker S.S. Scrieciu Modelling low stabilisation with E3MG: towards a ‘New Economics’ approach to simulating energy-environment-economy system dynamics The Energy Journal 31 Special issue 1 2010 137 164 Barker et al., 2008 T. Barker S.S. Scrieciu T. Foxon Achieving the G8 50% target: modelling induced and accelerated technological change using the macro-econometric model E3MG Climate Policy 8 2008 S30 S45 Berkhout et al., 2002 F. Berkhout J. Hertin A. Jordan Socio-economic futures in climate change impact assessment: using scenarios as ‘learning machines’ Global Environmental Change 12 2 2002 83 95 Bouwer, 2010 L.M. Bouwer Have disaster losses increased due to anthropogenic climate change? Bulletin of the American Meteorological Society 2010 10.1175/2010BAMS3092.1 Bouwman et al., 2006 A.F. Bouwman T. Kram K. Klein Goldewijk Integrated Modelling of Global Environmental Change. An Overview of IMAGE 2.4 2006 Netherlands Environmental Assessment Agency Bilthoven 228 pp. (Publication 500110002/2006) Burke et al., 2006 E.J. Burke S.J. Brown N. Christidis Modelling the recent evolution of global drought and projections for the 21st century with the Hadley Centre climate model Journal of Hydrometeorology 7 2006 1113 1125 Clarke et al., 2010 L. Clarke J. Edmonds V. Krey R. Richels S. Rose M. Tavoni International climate policy architectures: overview of the EMF 22 international scenarios Energy Economics 31 Suppl. 2 2010 S64 S81 Copenhagen Accord, 2009 Copenhagen Accord, 2009. (Copenhagen Accord of 18 December 2009). United Nations Climate Change Conference 2009, Copenhagen. Craig et al., 1999 M.H. Craig R.W. Snow D. le Sueur A climate-based distribution model of malaria transmission in Africa Parasitology Today 15 3 1999 105 111 Cummins and Mahul, 2009 J.D. Cummins O. Mahul Catastrophe Risk Financing in Developing Countries: Principles for Public Intervention 2009 The World Bank Washington, DC De Bruin et al., 2009a K.C. De Bruin R.B. Dellink S. Agrawala Economic Aspects of Adaptation to Climate Change: Integrated Assessment Modelling of Adaptation Costs and Benefits 2009 OECD Paris De Bruin et al., 2009b K.C. De Bruin R.B. Dellink R.S.J. Tol AD-DICE: an implementation of adaptation in the DICE model Climatic Change 95 1–2 2009 63 81 Den Elzen et al., 2008 M.G.J. Den Elzen P.L. Lucas D.P. van Vuuren Regional abatement action and costs under allocation schemes for emission allowances for achieving low CO 2 -equivalent concentrations Climatic Change 90 3 2008 243 268 Den Elzen and van Vuuren, 2007 M.G.J. Den Elzen D.P. van Vuuren Peaking profiles for achieving long-term temperature targets with more likelihood at lower costs Proceedings of the National Academy of Sciences of the United States of America 104 46 2007 17931 17936 DINAS-COAST Consortium, 2006 DINAS-COAST Consortium, 2006. DIVA 1.5.5 CD-ROM, Potsdam Institute for Climate Impact Research, Potsdam, Germany. Easterling et al., 2007 W. Easterling P. Aggarwal P. Batima K. Brander L. Erda M. Howden A. Kirilenko J. Morton J.-F. Soussana J. Schmidhuber F.N. Tubiello Food, fibre and forest products M.L. Parry O.F. Canziani J.P. Palutikof P.J. van der Linden C.E. Hanson Climate Change 2007: Impacts, Adaptation and Vulnerability. Contribution of Working Group II to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge, UK EC, 2006 EC World Energy Technology Outlook 2050 (WETO H2) 2006 European Commission Brussels Edenhofer et al., 2010 O. Edenhofer B. Knopf T. Barker L. Baumstark E. Bellevrat B. Chateau P. Criqui M. Isaac A. Kitous S. Kypreos M. Leimbach K. Lessmann B. Magné S. Scrieciu H. Turton D.P. van Vuuren The economics of low stabilization: model comparison of mitigation strategies and costs The Energy Journal 31 SI-1 2010 11 48 Environmental Change Institute, 2009 Environmental Change Institute, 2009. International Climate Conference – 4 Degrees and Beyond. Environmental Change Institute, Oxford University, 28–30 September, Oxford, UK. Feyen et al., 2009 L. Feyen J.I. Barredo R. Dankers Implications of global warming and urban land use change on flooding in Europe J. Feyen K. Shannon M. Neville Water and Urban Development Paradigms. Towards an Integration of Engineering, Design and Management Approaches 2009 Taylor and Francis Group London Fischer et al., 2007 G. Fischer F.N. Tubiello H. van Velthuizen D.A. Wiberg Climate change impacts on irrigation water requirements: effects of mitigation, 1990–2080 Technological Forecasting and Social Change 74 7 2007 1083 1107 Fisher et al., 2007 B. Fisher N. Nakicenovic K. Alfsen J. Corfee Morlot F. de la Chesnaye J.-C. Hourcade K. Jiang M. Kainuma E. La Rovere A. Matysek A. Rana K. Riahi R. Richels S. Rose D. van Vuuren R. Warren P. Ambrosi F. Birol D. Bouille C. Clapp B. Eickhout T. Hanaoka M.D. Mastrandrea Y. Matsuoko B. O’Neill H. Pitcher S. Rao F. Toth Issues related to mitigation in the long-term context B. Metz O. Davidson P. Bosch R. Dave L. Meyer Climate Change 2007. Mitigation of Climate Change. Contribution of Working Group III to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press New York 169 250 Hallegatte, 2009 S. Hallegatte Strategies to adapt to an uncertain climate change Global Environmental Change 19 2009 240 247 Hayashi et al., 2010 A. Hayashi K. Akimoto F. Sano S. Mori T. Tomoda Evaluation of global warming impacts for different levels of stabilization as a step toward determination of the long-term stabilization target Climatic Change 98 2010 87 112 Hilderink et al., 2008 H. Hilderink P.L. Lucas A. ten Hove M. Kok M. de Vos P. Janssen J. Meijer A. Faber A. Ignaciuk A. Petersen H.J.M. de Vries Towards a Global Integrated Sustainability Model 2008 Netherlands Environmental Assessment Agency Bilthoven Hinkel and Klein, 2009 J. Hinkel R.J.T. Klein Integrating knowledge to assess coastal vulnerability to sea-level rise: the development of the DIVA tool Global Environmental Change 19 3 2009 384 395 Hirabayashi and Kanae, 2009 Y. Hirabayashi S. Kanae First estimate of the future global population at risk of flooding Hydrological Research Letters 3 2009 6 9 Hof et al., 2009a A.F. Hof K.C. de Bruin R.B. Dellink M.G.J. den Elzen D.P. van Vuuren The effect of different mitigation strategies on international financing of adaptation Environmental Science and Policy 12 7 2009 832 843 Hof et al., 2009b A.F. Hof K. de Bruin R. Dellink M.G.J. den Elzen D.P. van Vuuren Costs, benefits and inter-linkages between adaptation and mitigation F. Biermann P. Pattberg F. Zelli Global Climate Governance After 2012: Architecture, Agency and Adaptation 2009 Cambridge University Press Cambridge Hof et al., 2008 A.F. Hof M.G.J. den Elzen D.P. van Vuuren Analysing the costs and benefits of climate policy: value judgements and scientific uncertainties Global Environmental Change 18 3 2008 412 424 IMAGE-team, 2001 IMAGE-team, 2001. The IMAGE 2.2 implementation of the IPCC SRES scenarios. A comprehensive analysis of emissions, climate change and impacts in the 21st century. RIVM CD-ROM publication 481508018, National Institute for Public Health and the Environment, Bilthoven, the Netherlands. IPCC, 2007 IPCC (Ed.), 2007. Climate Change 2007: Synthesis Report. Contribution of Working Groups I, II and III to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change. IPCC, Geneva, 104 pp. Isaac and van Vuuren, 2009 M. Isaac D.P. van Vuuren Modeling global residential sector energy demand for heating and air conditioning in the context of climate change Energy Policy 37 2 2009 507 521 Jenkins and Lowe, 2003 Jenkins, G., Lowe, J., 2003. Handling uncertainties in the UKCIP02 scenarios of climate change. Hadley Centre Technical Note 44, Met Office, Exeter. Kinney et al., 2008 P.L. Kinney M.S. O’Neill M.L. Bell J. Schwartz Approaches for estimating effects of climate change on heat-related deaths: challenges and opportunities Environmental Science and Policy 11 87 2008 Klein et al., 2007 R.J.T. Klein S. Huq F. Denton T.E. Downing R.G. Richels J.B. Robinson F.L. Toth Inter-relationships between Adaptation and Mitigation. Climate Change 2007. Impacts, Adaptation and Vulnerability. Contribution of Working Group II. Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge 745 777 Krol et al., 1997 M. Krol J. Alcamo R. Leemans Global and regional impacts of stabilizing atmospheric CO 2 Mitigation and Adaptation Strategies for Global Change 1 1997 341 361 Kundzewicz et al., 2010 Z.W. Kundzewicz Y. Hirabayashi S. Kanae River floods in the changing climate – observations and projections Water Resources Management 2010 10.1007/s11269-009-9571-6 Kundzewicz et al., 2007 Z.W. Kundzewicz L.J. Mata N. Arnell P. Döll P. Kabat B. Jiménez K. Miller T. Oki Z. Şen I. Shiklomanov Freshwater resources and their management M.L. Parry O.F. Canziani J.P. Palutikof C.E. Hanson P.J. van der Linden Climate Change 2007: Impacts, Adaptation and Vulnerability. Contribution of Working Group II to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge, UK Leemans and Born, 1994 R. Leemans G.J.v.d. Born Determining the potential global distribution of natural vegetation, crops and agricultural productivity Water, Air and Soil Pollution 76 1994 133 161 Lenton et al., 2008 T.M. Lenton H. Held E. Kriegler J.W. Hall W. Lucht S. Rahmstorf H.J. Schellnhuber Tipping elements in the Earth's climate system Proceedings of the National Academy of Sciences of the United States of America 105 6 2008 1786 1793 Manne and Richels, 2005 A.S. Manne R.G. Richels Merge: an integrated assessment model for global climate change R. Loulou J.-P. Waaub G. Zaccour Energy and Environment 2005 Springer USA McMichael et al., 1996 A. McMichael A. Haines R. Slooff S. Kovats Climate Change and Human Health 1996 World Health Organization Geneva Mechler et al., 2010 R. Mechler S. Hochrainer A. Aaheim Z. Kundzewicz N. Lugeri M. Moriondo H. Salen M. Bindi I. Banaszak A. Chorynski E. Genovese H. Kalirai J. Linnerooth-Bayer C. Lavalle D. McEvoy P. Matczak M. Radziejewski D. Rübbelke M.-J. Schelhaas M. Szwed A. Wreford Risk management approach for assessing adaptation to changing flood and drought risks in Europe M. Hulme H. Neufeldt Making Climate Change Work for Us: European Perspectives on Adaptation and Mitigation Strategies 2010 Cambridge University Cambridge, UK Meehl et al., 2007 G.A. Meehl T.F. Stocker W.D. Collins P. Friedlingstein A.T. Gaye J.M. Gregory A. Kitoh R. Knutti J.M. Murphy A. Noda S.C.B. Raper I.G. Watterson A.J. Weaver Z.-C. Zhao Global climate projections S. Solomon Climate Change 2007: The Physical Science Basis. Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge Metz et al., 2007 B. Metz O.R. Davidson P.R. Bosch R. Dave L.A. Meyer Climate Change. Mitigation of Climate Change. Contribution of Working Group III to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge, United Kingdom Mirza et al., 2003 M.M.Q. Mirza R.A. Warrick N.J. Ericksen The implications of climate change on floods of the Ganges, Brahmaputra and Meghna Rrivers in Bangladesh Climatic Change 57 2003 287 318 Moriondo et al., 2010 M. Moriondo M. Bindi Z.W. Kundzewicz M. Szwed A. Chorynski P. Matczak M. Radziejewski D. McEvoy A. Wreford Impact and adaptation opportunities for European agriculture in response to climatic change and variability Mitigation and Adaptation Strategies for Global Change 15 7 2010 657 679 Moss et al., 2010 R.H. Moss J.A. Edmonds K.A. Hibbard M.R. Manning S.K. Rose D.P. van Vuuren T.R. Carter S. Emori M. Kainuma T. Kram G.A. Meehl J.F.B. Mitchell N. Nakicenovic K. Riahi S.J. Smith R.J. Stouffer A.M. Thomson J.P. Weyant T.J. Wilbanks The next generation of scenarios for climate change research and assessment Nature 2010 10.1038/nature08823 Nakicenovic et al., 2000 N. Nakicenovic Special Report on Emissions Scenarios (SRES) 2000 Cambridge University Press Cambridge, UK Nakicenovic et al., 2006 N. Nakicenovic P. Kolp K. Riahi M. Kainuma T. Hanaoka Assessment of Emissions Scenarios Revisited Environmental Economics and Policy Studies 7 3 2006 137 173 Nicholls and Lowe, 2004 R.J. Nicholls J.A. Lowe Benefits of mitigation of climate change for coastal areas Global Environmental Change 14 3 2004 229 244 Nicholls et al., 2010 R.J. Nicholls N. Marinova J.A. Lowe S. Brown P. Vellinga D. de Gusmao J. Hinkel R.S.J. Tol Sea-level rise and its possible impacts given a “beyond 4 degree world” in the 21st century Philosophical Transactions of the Royal Society 2010 10.1098/rsta.2010.029 Nordhaus and Boyer, 2000 W.D. Nordhaus J. Boyer Warming the World: Economic Models for Global Warming 2000 MIT Press Cambridge, MA pp. 315–328 Nordhaus, 2008 W.D. Nordhaus A Question of Balance Weighing the Options on Global Warming Policies 2008 Yale University Press New Haven and London Parry et al., 2007 M.L. Parry O.F. Canziani J.P. Palutikof P.J. van der Linden C.E. Hanson Climate Change 2007: Impacts, Adaptation and Vulnerability. Contribution of Working Group II to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change 2007 Cambridge University Press Cambridge Patt et al., 2010 A.G. Patt D.P. van Vuuren F. Berkhout A. Aaheim A.F. Hof M. Isaac R. Mechler Adaptation in integrated assessment modeling: Where do we stand? Climatic Change 99 3 2010 383 402 Piani et al., 2005 C. Piani D.J. Frame D.A. Stainforth M.R. Allen Constraints on climate change from a multi-thousand member ensemble of simulations Geophysical Research Letters 32 2005 L23825 Price, 2005 C. Price An intergenerational perspective on effects of environmental changes: discounting the future's viewpoint J.L. Innes G.M. Hickey H.F. Hoen Forestry and Environmental Change: Socioeconomic and Political Dimensions 2005 International Union on Forestry Research Organisations (IUFRO) Vienna Rose et al., 2007 Rose, S., Ahammad, H., Eickhout, B., Fisher, B., Kurosawa, A., Rao, S., Riahi, K., van Vuuren, D. 2007. Land in climate stabilization modeling: initial observations Energy Modeling Forum Report. Stanford University. Schneider and Kuntz-Duriseti, 2002 S.H. Schneider K. Kuntz-Duriseti Uncertainty and climate change policy S.H. Schneider A. Rosencranz O. Niles Climate Change Policy: A Survey 2002 Island Press Washington, DC Stern, 2006 N. Stern Stern Review on the Economics of Climate Change 2006 Cambridge University Press Cambridge Swart et al., 2009 R. Swart L. Bernstein M. Ha-Duong A. Petersen Agreeing to disagree: uncertainty management in assessing climate change, impacts and responses by the IPCC Climatic Change 92 2009 1 29 Swart and Raes, 2007 R. Swart F. Raes Making integration of adaptation and mitigation work: mainstreaming into sustainable development policies? Climate Policy 7 4 2007 288 303 Tol, 2002a R. Tol Estimates of the damage costs of climate change. Part II. Dynamic estimates Environmental and Resource Economics 21 2 2002 135 160 Tol, 2002b R.S.J. Tol Estimates of the damage costs of climate change. Part 1. Benchmark estimates Environmental and Resource Economics 21 1 2002 47 73 Tol, 2002c R.S.J. Tol Welfare specifications and optimal control of climate change: an application of fund Energy Economics 24 4 2002 367 376 Tubiello and Fischer, 2007 F.N. Tubiello G. Fischer Reducing climate change impacts on agriculture: global and regional effects of mitigation, 2000–2080 Technological Forecasting and Social Change 74 7 2007 1030 1056 UN, 2005 UN, 2005. World Population Prospects: The 2004 Revision. CD-ROM Edition – Extended Dataset. United Nations publications, Sales No. E.05.XIII.12, United Nations, Department of Economic and Social Affairs, Population Division. van Vliet et al., 2009 J. van Vliet M.G.J. den Elzen D.P. van Vuuren Meeting radiative forcing targets under delayed participation Energy Economics 31 Suppl. 2 2009 S152 S162 van Vuuren et al., 2006 D.P. van Vuuren B. van Ruijven M. Hoogwijk M. Isaac B. De Vries TIMER 2: model description and application L. Bouwman T. Kram K. Klein-Goldewijk Integrated Modelling of Global Environmental Change. An overview of IMAGE 2.4 2006 MNP - Netherlands Environmental Assessment Agency Bilthoven van Vuuren et al., 2007 D.P. van Vuuren M.G.J. Den Elzen P.L. Lucas B. Eickhout B.J. Strengers B. Van Ruijven S. Wonink R. Van Houdt Stabilizing greenhouse gas concentrations at low levels: an assessment of reduction strategies and costs Climatic Change 81 2 2007 119 159 van Vuuren et al., 2008a D.P. van Vuuren B. De Vries A. Beusen P.S.C. Heuberger Conditional probabilistic estimates of 21st century greenhouse gas emissions based on the storylines of the IPCC-SRES scenarios Global Environmental Change 18 4 2008 635 654 van Vuuren et al., 2008b D.P. van Vuuren M. Meinshausen G.K. Plattner F. Joos K.M. Strassmann S.J. Smith T.M.L. Wigley S.C.B. Raper K. Riahi F. De La Chesnaye M.G.J. Den Elzen J. Fujino K. Jiang N. Nakicenovic S. Paltsev J.M. Reilly Temperature increase of 21st century mitigation scenarios Proceedings of the National Academy of Sciences of the United States of America 105 40 2008 15258 15262 van Vuuren et al., 2009 D.P. van Vuuren M.G.J. den Elzen J. van Vliet T. Kram P. Lucas M. Isaac Comparison of different climate regimes: the impact of broadening participation Energy Policy 37 12 2009 5351 5362 van Vuuren et al., 2010 D.P. van Vuuren E. Stehfest M.G.J. den Elzen J. Van Vliet M. Isaac Exploring scenarios that keep greenhouse gas radiative forcing below 3 W/m 2 in 2100 Energy Economics 31 Special Issue 1 2010 165 192 Vermeer and Rahmstorf, 2009 M. Vermeer S. Rahmstorf Global sea level linked to global temperature Proceedings of the National Academy of Sciences of the United States of America 106 2009 21527 21532 Weitzman, 2008 Weitzman, M.L., 2008. On modeling and interpreting the economics of catastrophic climate change. Wigley, 2003 T.M.L. Wigley MAGICC/SCENGEN 4.1: Technical Manual 2003 UCAR - Climate and Global Dynamics Division Boulder, CO Wigley and Raper, 2001 T.M.L. Wigley S.C.B. Raper Interpretation of high projections for global-mean warming Science 293 2001 451 454|设想情景用于探讨不确定情况下不同适应和缓解战略的后果。在本文中，我们使用了两种情景来探讨发展: (1)没有缓解措施导致全球平均气温到2100年上升4摄氏度; (2)一个雄心勃勃的缓解策略导致到2100年上升2摄氏度。就第二种情况而言，气候系统的不确定性意味着不能排除全球平均气温上升3摄氏度或更多的可能性。我们的分析表明，在许多情况下，适应和减缓不是权衡，而是补充。例如，在缓解设想方案中，因气候变化而面临更大水资源压力的人数可以大幅度减少，但仍然需要对面临更大压力的其余大量人口进行适应。另一个例子是海平面上升，从全球和纯货币的角度来看，适应(直到2100年)似乎比缓解更有效。然而，从较贫穷和小岛屿国家的角度来看，严格的缓解措施对于将风险保持在可控水平是必要的。就农业而言，只有基于适应和缓解相结合的设想方案才能避免严重的气候变化影响。关键词情景综合评估气候变化缓解适应气候影响1引言情景分析是评估气候变化和气候变化政策的一个非常重要的工具，它使分析人员能够探索经济发展、温室气体排放、气候和生态系统等因素之间复杂而不确定的未来相互作用。这些因素共同决定了缓解和适应政策的必要性和可能性。设想情景还可以作为一种手段，协调参与气候研究领域的各种不同研究群体的假设，从而更好地比较其结果。因此，情景在缓解和适应研究中得到了广泛的应用(参见 Metz 等，2007; Parry 等，2007)(特别是来自排放情景特别报告(SRES)的情景(Nakicenovic 等，2000))。Moss 等人(2010)指出，由于 SRES 对场景分析的信息需求正在发生变化。首先，人们对探索适应与缓解之间的关系越来越感兴趣。正如 Moss 等人(2010)所指出的，这将需要进一步整合气候研究中涉及的不同分析传统的信息。第二，除了迄今为止探讨的无气候政策情景之外，人们对明确探讨气候政策影响的情景也越来越感兴趣。具体而言，在没有气候政策的情况下，能够评估长期气候目标的“成本”和“收益”是非常有意义的。在本文中，我们遵循这一思路，探讨情景分析如何能够促进对未来适应和缓解战略的联合评估。这样的联合评估有以下几个原因: (1)首选的缓解策略取决于预期的气候影响和适应成本; (2)考虑到适应气候变化的局限性; (3)一些适应和缓解策略可能相互作用; (4)最后，气候变化的影响可能有需要考虑的重要反馈。这种分析在战略层面上是最有用的，而不是针对个人的适应(或缓解)决策。鉴于这一目的，我们在本文中讨论了两个主要的情景，其中包括适应和减缓战略的要素(见本文的进一步内容) ，导致本世纪末全球平均气温上升4摄氏度和2摄氏度。这两个温度水平已经开始成为标志性的数字，代表着在没有减缓政策(4摄氏度)和国际气候谈判的温度目标(2摄氏度)(2009年哥本哈根协议)的情况下的潜在结果。可以说，如果政治领导人要在减缓、适应和气候影响之间做出明智的选择，了解这两个温度水平的影响是至关重要的(环境变化研究所，2009)。缓解和适应战略的综合评估由于方法上的差异而受到阻碍。考虑到当地环境的重要性，综合评估模型很难描述适应过程(Patt et al。 ，2010)。一个实际问题是，迄今为止，影响文献的相当一部分集中在非政策情景下的影响(例外包括 Arnell 等，2002; Bakkenes 等，2006; Hayashi 等，2010; Krol 等，1997; Nicholls 和 Lowe，2004)。因此，本文提出了一个基于耦合信息的广义情景评估——但没有假装是完整的或完全集成的。作为一项边做边学的活动，本文件打算说明4摄氏度和2摄氏度世界之间的重要区别，但也要确定进行综合情景分析所涉及的一些实际问题。这意味着，与现有文献相比，最重要的进步是我们提出了一个基于一致情景的多部门分析。鉴于目前综合评估模型的先进水平，已经使用几个松散耦合模型进行了试验。因此，一些重要的联系无法得到解决，如农业的适应性反应，这可能涉及灌溉(见第5.3节)和水需求(第5.4节)。事实上，本文提出的一个重要问题是，是否需要进行全面综合分析，或者部分综合是否足够。本文的内容安排如下: 我们首先讨论在开发能够为适应和缓解政策决策提供信息的设想方案时所遇到的一些方法上的复杂问题。接下来，我们讨论两种主要情景在社会经济驱动因素方面的差异(第3和第4部分)。在第5节中，我们探讨了适应和减缓战略对气候变化各种影响的潜在后果。2评估气候战略和情景发展(理论和方法)2.1应对气候变化的不同战略气候变化及其响应可能导致三种形式的成本(不一定是货币) : (1)气候影响的(剩余)成本，(2)适应的成本和(3)缓解的成本。至少在理论上，这对应于三种不同的策略: (1)“自由放任”(接受气候变化) ，(2)关注适应，(3)关注缓解，如图1所示(另见 Klein 等，2007)。虽然图1表明，缓解、适应和剩余损害的成本和收益可以相互交换，但存在一些概念和分析问题，使这种办法复杂化。这些与空间和时间尺度、风险和不确定性有关(SwartandRaes，2007)。缓解和适应是在不同空间尺度上发生的过程。虽然缓解行动通常是在国家或地方范围内采取的，但好处是全球共享的。因此，气候政策成功和成本的关键因素是国际合作的程度(Barker 等，2009; Clarke 等，2010; van Vliet 等，2009; van Vuuren 等，2009)。相比之下，对于适应而言，成本和收益在从地方到国家乃至国际的多个尺度上都存在。较大规模的扶持性环境仍然可以在较小规模上加强适应(例如，由国际融资机制资助的地方能力建设)。由于这些原因，缓解评估往往集中在全球一级，而相比之下，适应研究大多集中在地方一级。随着时间的推移，缓解和适应的动态也是一个重要因素。严格的缓解方案通常需要强有力的早期减排。然而，由于气候系统内部的巨大惯性，这些假设情景的气候变化影响在短期(前几十年)与没有气候变化政策的假设情景几乎没有差别。相比之下，一些相关的影响(例如减少当地空气污染的共同利益)可以以更快的速度实现。适应措施可能在短期内产生私人和社会效益。例如，空气调节等简单的适应措施可以带来明显的短期效益。一些重要的例外存在，可能需要几十年的实施，如空间规划的变化或大规模的工程工程防洪(见哈勒盖特，2009年)。其他重要因素是风险和不确定性。我们对气候变化的理解面临许多不确定性。要确定的关键不确定性包括认知、数据、模型和实体不确定性(施奈德和 Kuntz-Duriseti，2002; van Vuuren 等，2008a)。涉及不确定因素的例子有: (i)未来的排放量，(ii)气候系统，(iii)未来的脆弱性和对气候风险的暴露，以及(iv)缓解成本。采取缓解行动减少了一些不确定性，因为它减少了气候变化的源头，并揭示了实际的缓解成本(Barker，2003; Piani 等，2005)。然而，缓解措施也可能增加风险。例如，如果以不可持续的方式实施生物能源，可能会抵消一组风险(气候变化) ，同时产生另一组不同的风险(生物多样性丧失和粮食安全下降)。处理风险的一种方法是包括概率评估。这通常是使用过去的证据，推断以涵盖特定的未来情况。其他不确定性(例如不可知的冲击和意外)在量化意义上更难处理，但它们证明了承认无知的合理性。情景可以用来探索极端事件的可能性和各种政策组合的稳健性，但这并不常见(Berkhout et al。 ，2002)。传统上，涉及缓解研究和适应研究的学科对不确定性有不同的描述方式。虽然缓解研究往往使用定量方法并侧重于平均估计，但适应研究往往更侧重于对不确定性的定性描述，并侧重于危险事件的风险，即使这些事件发生的概率很低。这些不同的不确定性感知可能会使不同策略的综合评估复杂化(Swartet al。 ，2009)。2.2场景的类型我们可以根据缓解和适应的考虑将场景分为不同的类别。首先，我们将基线情景定义为一个事件的轨迹，假设没有来自气候变化的重大反馈，也没有关于缓解或适应的具体政策努力(这种情景可能仍然包括许多间接影响缓解或适应气候变化能力的行动; 例如，可以预期收入水平的增加与对减少疟疾等气候相关疾病风险的卫生服务的更大投资相一致)。这种类型的场景的主要目的是进行分析，作为其他场景的参考点。其次，适应情景描述了一个社会正在应对气候变化影响的世界。其目的是探讨适应气候变化所需的技术和政策类型、避免的损害和相关费用。适应包括所谓的自主适应(即在没有特定政府行动的情况下发生的行动)和有计划的适应。第三，缓解方案描述了一个包括旨在限制气候变化的政策的世界。其目的是探讨最大限度地减少气候变化及相关成本所需的技术和政策类型。由于总是存在剩余的影响，第四组，适应和缓解情景综合了两种类型的气候变化应对措施。可能的话，这第四类情景可以根据适应和缓解备选办法之间可能存在的协同作用，例如对于一些重新造林备选办法，重新排列政策备选办法。每一种情况都与更广泛的社会、政治和文化背景有关，在这种背景下，它们被认为会出现。在探索缓解、适应和残余损害的优选组合时，存在两种主要方法: (i)将潜在影响描述为全球平均气温上升(从而缓解)的功能的影响和基于风险的方法，以及(ii)成本效益分析，其中确定货币成本和收益，以最大限度地提高福利(例如，参见 Nordhaus，2008; Tol，2002c)。在这两种情况下，我们认为，描述不同应对战略之间的关系比寻求确定最佳办法更有用，也更能反映问题。鉴于第2.1节所列出的复杂性和不确定性，我们认为在现实中不可能采取任何最佳的缓解、适应或联合策略。2.3综合分析缓解和适应的综合分析可以通过不同方式实现: 例如，使用单一的所谓综合评估模型，或在不同模型和学科之间交流信息，评估现有文献并使结果具有可比性。这两种方法都是围绕气候变化的因果链进行组织的，即描述经济活动(收入、能源使用、农业等)、排放、气候变化和影响之间的关系——以及相关的反馈(图2)。实际上，该方案也构成了 IPCC 报告情景信息流的主干(Moss et al。 ，2010)。情景首先由综合评估和排放模型制定(侧重于经济驱动力、能源和土地使用以及温室气体排放(IPCC“工作组 III”))。随后，排放轨迹在气候模型中被用来评估气候变化的影响(IPCC“工作组 I”)。最后，这些情景被用于影响、适应和脆弱性分析(IPCC“工作组 II”)。不同研究学科和工作组的参与意味着很难说明不同领域之间的反馈意见。综合评估模型只能获取有限数量的可能反馈(经常被忽略的反馈包括粮食和水安全对人口和经济驱动因素的影响; 水资源短缺与粮食生产之间的关系; 气候变化对能源使用的影响等)。如果这些反馈不足以对系统产生重大影响，忽略(其中一些)可能是合理的。出于分析原因，在学科领域内组织场景开发并考虑有限数量的反馈有很大的优势。它使研究人员能够专注于他们很好地理解的链条要素，并增加所需的细节数量，而不必面对相互联系的复杂性。然而，在更加注重对缓解和适应战略进行综合分析的情况下，这种情况可能会改变。关于为什么需要采取综合办法的一些例子是: 一、气候影响，例如极端事件引发的影响，可能非常严重，破坏了原先设想的经济假设; 二。气候影响可能对农业产生重大影响，因此，对未考虑影响的土地使用相关排放量的估计可能是错误的，生物能源的缓解潜力可能受到影响;。对于对缓解和适应都有吸引力的土地面积，可能存在相互竞争的权利主张。因此，一个有趣的问题是，是否需要更加集成的分析是如此迫切，以至于需要更加复杂的集成模式(模型的交互耦合; 一个复杂的模型) ，或者是否可以单独处理影响，简化分析框架。时间范围和决策焦点在这里可能也很重要，例如是否考虑到潜在的临界点(Lenton et al。 ，2008)。研究这个问题的少数现有研究似乎表明，在大多数部门，任何缓解项目的适应影响都很小，大多数适应活动产生的排放量也很小(Klein et al。 ，2007)。迄今为止，最完整的分析来自以成本效益为导向的综合评估模型，如基金、 DICE 和 MERGE (Manne and Richels，2005; Nordhaus，2008; Tol，2002c) ，但这些模型通常将气候影响聚合为有限数量的相当抽象的损害函数。我们认为，随着时间的推移，随着许多部门减缓和适应措施的力度不断加大，将更加需要进行足够详细的联合评估。这里提供的场景基于建模和场景开发的当前技术状态，迈出了第一步。缓解和影响评估的一项评估使用了相同的设想方案，我们明确提出了缓解和适应战略(作为设想方案的一部分或在用于不同影响的模型中)。然而，许多反馈没有被考虑进去。在本文的最后，我们回到了更加集成(但也更加复杂)的场景的作用。2.4本文件使用的方法如上所述，可以确定几种情景: 基线情景、缓解情景、适应情景和适应-缓解情景。本文还介绍了这些场景类型。对于基准/适应情景，我们假设大多数社会经济驱动因素的中间假设。情景假设在第3和第4节中描述。这些设想方案不包括缓解措施，导致到2100年全球平均气温比工业化前水平上升4摄氏度。虽然我们描述了在这些情况下可能产生的影响和适应，但我们没有包括对原始驱动因素的反馈。在缓解设想方案中，包括严格的缓解努力，导致全球平均气温上升2摄氏度。使用 IPCC 给出的3 ° C 的气候敏感性中值(Meehl 等，2007) ，这意味着稳定水平约为450 ppm CO2当量(CO2当量).气候政策对经济驱动因素的影响没有被考虑在内——但是其他几个关系是耦合的(例如土地使用)。因此，在大多数文章中，我们忽略了气候变化和气候政策对经济假设的潜在影响。然而，在第5.8节中，我们用一个简单的经济模型(FAIR)来讨论它们的影响，以提供对全球范围内经济后果的可能规模的一些见解。使用了几个模型工具。这些情景主要是使用 IMAGE 综合评估模型开发的(Bouwman 等，2006)。IMAGE 模型根据对人口和世界经济的假设，结合对技术发展和消费模式的假设，描述了21世纪能源和土地使用的发展情况。该模型预测了全球范围内的气候变化(以全球平均温度变化和海平面上升为指标) ，并通过模式缩放缩小的气候模式模式构建了0.5 ° × 0.5 ° 网格上月气温和降雨量变化的空间场景。IMAGE 的产出用于描述海平面上升的 DIVA 模型; 用于估计水资源紧张后果的 Mac-PDM 全球水文模型; 用于估计加热和降温需求影响的 TIMER 能源模型; 用于疟疾对疟疾影响的 MARA/ARMA 适宜性模型和用于货币成本效益分析的 FAIR 模型。此外，我们更一般地讨论对农业的影响(基于 IPCC AR4)和极端事件。附录 A 提供了所有使用模型的简要描述。在我们的描述中，我们关注于全局级别(考虑到空间有限)。显然，这导致了我们讨论适应的局限性。实验取决于每个模型的设计，因此，不同影响之间可以提出的假设情景的数量是不同的。这意味着研究报告应被解释为综合评估的第一个例证，而不是关于适应及其限制的全面研究。3结果: 基线情景中的社会经济趋势3.1人口发展和经济增长我们假设人口遵循2004年世界人口预测修订版(UN，2005)到2050年的中等生育率变量，以及联合国到2100年的长期中等预测(图3)。这意味着，到2050年，全球人口将稳步增至近91亿，并在随后的50年中稳定在约92亿人，直至2100年。这种情况在人口预测范围内采取中间立场(见图3)。对于直到2050年的经济增长，这种情况遵循与剑桥模型 E3MG 相关的预测(巴克和 Scrieciu，2010; 巴克等人，2008)。利用基于 SRES 的 B2情景(IMAGE-team，2001)的经济增长预测，该情景延伸到2050年以后。从数量上看，这是一个中高速经济增长的情景，主要是对中国和印度经济增长的乐观假设的结果。按人均计算，经合组织经济体预计仍将是世界上最富有的经济体，但就经济活动总量而言，发展中区域的重要性迅速增加。在非洲、中东和拉丁美洲，人均国内生产总值的年增长率在0% 到2% 之间。在亚洲，到2050年，这一比例将从目前的高水平降至每年3% 。3.2基线情景下的能源使用和温室气体排放基线情景下的能源使用与欧盟委员会(EC，2006)公布的基线保持一致。尽管能源强度进一步降低，世界能源消费在2000-2050年期间增加了一倍以上，在2050-2100年期间又增加了25% (图4)。整个世纪以来，能源供应仍然以化石燃料为主。当石油和天然气的生产在本世纪达到高峰和下降时，煤炭的使用在整个情景期间增加。此外，非化石能源的产量也在迅速增长。在截至2100年的这段时间里，核能使用量增加了2到3倍，达到76 EJ，生物质使用量强劲增加，而水力发电产量增加了大约60% 到80% 。相对增长最大的是风能和太阳能; 到2050年，风能和太阳能在所有非化石能源中的比例将从不到1% 上升到10% 至14% 。2050年可再生能源总使用量为120-140 EJ，2100年为190 EJ。上述趋势表明，能源活动的二氧化碳排放量在2050年之前增加了一倍以上，在2050年至2100年之间又增加了三分之一(见图3)。因此，该方案在文献范围内形成了一个中间基线方案(Fisher et al。 ，2007)。非二氧化碳温室气体(尤其是甲烷)在2000-2050年期间稳步增长，但增长速度低于二氧化碳(作为其驱动因素，农业的增长速度预计将低于能源部门)。本世纪上半叶，土地利用产生的二氧化碳排放量回落至零。农业用地面积位于最近公布的类似情景的范围内，尽管处于该范围的低端(Rose et al。 ，2007)。4缓解方案和气候方案的结果4.1能源使用和温室气体排放缓解方案旨在将温室气体稳定在大约450 ppm CO2-当量。(参见 van Vuuren 等人，2007,2010)。这种情况允许初始浓度超调至大约510 ppm 二氧化碳当量。DenElzen 和 van Vuuren (2007)早些时候已经表明，有限的过度集中可以以较低的成本达到类似的气候目标。减少排放的方法有很多种。一个要素是提高能源效率，从而减少能源使用总量(2050年与基准相比减少20%)(见图4)。该设想方案还显示，非化石能源的使用日益增加，占能源使用总量增长的大部分。非化石能源的使用从2010年占一次能源使用总量的15% 增加到2050年的30% 以上，到本世纪末占总量的40% 以上。这种增长主要是由于生物能源使用的增加。碳捕获和储存应用于化石燃料的大多数固定用途。最后，还减少了非二氧化碳温室气体的排放。其结果是，全球排放量在2020年左右达到峰值，并随着时间的推移进一步减少。与2050年的基准相比，排放量减少了70% 以上，到2100年减少了80% 以上。减缓政策的后果不仅影响能源部门，而且影响土地使用。大量额外的土地用于造林和生物能源(见图5)。模型比较研究表明，这里提出的缓解方案与目前的文献是一致的，尽管模型显示各种减排措施的贡献有显着差异(Clarke 等，2010; Edenhofer 等，2010)。根据 IMAGE 模型计算，减排成本约为 GDP 的1-2% (即每年的额外支出，可与经合组织国家目前约占 GDP 1.5% 的环境政策支出相比较)(图6)。可比情景的文献范围在2100年的0.5.5% 左右。大多数研究都认为，这些额外的支出将导致国内生产总值的减少。我们将在第5.8节进一步讨论这个问题。4.2基线和缓解情景下的气候变化基于 IMAGE 模型计算，大气温室气体浓度和由这两种情景排放引起的相关平均全球温度变化如图7所示(实线表示最佳猜测值)。IMAGE 模型使用 MAGICC 模型来计算全球平均温度的变化。早期，van Vuuren 等人(2008b)将 MAGICC 模型用于类似的 IMAGE 场景，以计算温室气体浓度和温度(包括不确定范围)的轨迹。在这里，用于 MAGICC 计算的不确定性范围是基于现有的更复杂的碳循环和气候模型。我们使用了温室气体浓度范围和温度结果的含义来描述这里的不确定性范围，如图中阴影区域所示。对于温度，较宽的阴影区域表示由于碳循环和气候敏感性的不确定性而产生的不确定性。对于基线情景，全球平均气温在2050年几乎与工业化前水平线性上升至2.1摄氏度，在2100年上升至3.7摄氏度(不确定性范围为3-5摄氏度)。在缓解方案中，到2100年全球平均气温上升幅度限制在1.9摄氏度。同样，存在相当大的不确定性。图7表明，到本世纪末，与工业化前水平相比，减缓气候变化的情况也可能导致气温上升2.6摄氏度。由于这里提出的缓解方案是科学文献中最严格的方案之一(参考 Clarke 等，2010; Edenhofer 等，2010; Fisher 等，2007) ，可以得出两个重要的结论。首先，分析表明，全球变暖可以得到缓解，但不能停止。第二，严格的设想也可能导致气候变化大大超过2摄氏度这一观察结果可能意味着，对冲适应政策以防止气候变暖加剧可能具有相当大的价值。例如，这些政策可能是“ ... ... 目标是2摄氏度，但准备3摄氏度”。在下面的影响评估中，我们关注的是中央气候变化预测。通过源自 HadCM2气候模型的重新尺度模式(图8)构建了与全球平均温度变化相关的全球0.5 ° × 0.5 ° 尺度上的月平均温度和降水量的变化。结果表明，高纬度地区年平均气温变化大于低纬度地区，降水量变化具有明显的空间变化特征。关于气候变化的预期模式，特别是降水的模式，存在着相当大的分歧: 因此，本文件提出的影响结果只代表一种可能的结果。5结果: 不同情景下的影响和适应5.1导言 IPCC 的第四次评估报告(IPCC，2007)概述了气候影响。其中一些影响来自平均气候的变化，但其他影响可能来自极端事件的变化。表1总结了一些影响，包括健康、农业、水资源供应、沿海洪水、城市地区和能源系统，以及气候系统的大规模破坏(相比之下，生物多样性和生态系统服务没有包括在内)。如前所述，大多数文献都将气候变化视为“渐进现象”(Agrawala and Fankhauser，2008)。这对于低概率和高拥有属性的影响是有问题的(见下文)。在这个探索性的分析中，我们描绘了一些影响和适应需求。我们的目标是涵盖表1中提到的几个关键影响，但是评估受到可以很容易耦合的模型的限制。因此，这些描述并不打算详尽无遗，而是在一定程度上说明了一些影响的严重程度和关键的适应挑战。在介绍我们的结果时，我们使用了几个基于上述情景的新模型运行(例如疟疾、水资源、海平面上升、加热和降温需求)。然而，我们也根据这里提出的两种情景(与温度相关的死亡率、农业和极端事件)评估了 IPCC 第四次评估报告中的现有信息。5.2人类健康: 与温度相关的死亡率和疟疾气候变化的健康影响需要在其他更重要的人类健康驱动因素的背景下看待，包括与生活方式相关的因素(Hilderink 等，2008)。我们在这里关注与温度有关的死亡率和疟疾。5.2.1与温度有关的死亡率与温度有关的死亡率影响可能通过极端温度的变化、平均温度的变化或温度的季节性变化而发生，文献显示的结果各不相同。McMichael 等(1996)使用相对风险比对温度相关死亡率进行了估计，表明存在死亡率最低的最佳温度(也称为 U 形剂量-反应关系)。如果温度升高，热应激相关的死亡率增加，但寒冷相关的死亡率下降。Tol (2002a)的结论是，从货币角度来看，由于气候变化导致的与寒冷有关的死亡率的下降超过与热有关的死亡率的增加。然而，这一结论受到用于评估生命价值的方法的影响，也受到平均和区域温度以及温度和健康之间关系的巨大不确定性的影响。适应可能发生在人体生理学适应更高的温度(麦克迈克尔等人，1996) ，行为的改变和空气调节使用的增加(Kinney 等人，2008)。考虑到使用温度和死亡率之间的剂量-反应关系的复杂性，我们还没有尝试在这里量化这些。5.2.2疟疾疟疾与气候变化之间的关系引起了人们的极大关注。在本文中，我们还重点讨论了气候引起的疟疾风险变化。每年有100多万人死于疟疾，其中大多数是非洲儿童。疟疾是一种由病媒传播的传染病。按蚊(传播疟疾感染的媒介)只能在平均温度高、没有霜冻和降水充足的气候中生存。MARA/ARMA 疟疾适宜性模型(Craig et al。 ，1999)综合了这些因素来确定气候适宜的地区。然而，疟疾造成的死亡率也受到诸如获得预防措施(包括室内喷洒和经杀虫剂处理的蚊帐)和获得医疗保健等因素的严重影响。在 MARA/ARMA 模型中，这些因素与收入和城市化有关。图9显示了这个模型在本文情景下的结果。自主适应的影响(作为收入增加的功能)减少了约50% 的疟疾死亡，特别是在非洲(主要是由于更好地提供卫生保健)。相比之下，气候的影响——特别是缓解设想方案与基准设想方案之间的差异要小得多。减缓措施将疟疾健康风险降低约2% (2050年)。因此，适应对疟疾控制的影响比缓解更具决定性(这一发现在现有文献中似乎很有说服力)。5.3农业: 对产量的影响伊斯特林等人(2007)综合了大量关于气候变化对作物生长的影响的研究，包括适应和不适应。结果总结为全球平均气温升高的函数，尽管实际上气温和降水模式的变化以及 CO2施肥都起作用。例如，二氧化碳施肥的影响部分抵消了气候变化的影响。结果可以用来评估我们的情景的气候影响使用最佳拟合多项式从伊斯特林等人(2007年) ，这表明产量的影响作为平均温度变化的函数。我们在每种情况下都采用全球平均气温变化作为一种假设，并将其作为预期的局部平均气温变化的指示。这意味着我们的影响估计可能是保守的，因为在许多陆地地区，气温上升可能比全球平均水平更强。我们研究了基线(4摄氏度)和缓解(2摄氏度)情景对玉米、小麦和水稻的影响，包括适应和不适应(见图10; 2100年热带和温带地区的结果; 这些影响是由于气候变化以外的其他因素导致的产量增加的额外影响)。虽然结果很不确定，但有些结论似乎是可能的。首先，基准情景(无适应性)导致所有情况下的产量(相对于没有气候变化的情况)大幅度下降: 气候变化影响可能会使所研究作物(2050年)的总体产量下降10-35% 。其次，无论是采取缓解还是适应措施，都会限制产量的下降。然而，在热带地区，影响仍然是负面的，通常在10% 左右的损失。第三，缓解和适应相结合可能会导致从今天的情况得到改善。对温带地区的农业影响可能更为积极，但前提是气温较高的优势不被极端天气的影响所抵消。这些结果突出表明，需要同时考虑缓解和适应问题。所提出的结果以气专委的评估为基础，代表了范围广泛的模型。结果也可以通过个别研究来说明。比如，Tubiello 和 Fischer (2007)发现，缓解方案可以显著降低全球农业气候变化的成本。同样，Fischer 等人(2007)阐述了适应水灌溉需求的重要性。他们发现，缓解措施减少了约40% 的农业用水需求，剩下60% 的影响需要适应。在处理对农业的影响时，干旱和热浪胁迫都起着重要作用。图11显示了在欧洲，假设各种形式的适应(Mechler 等，2010; Moriondo 等，2010) ，干旱和热浪胁迫对2 °C 升温情景下作物产量的影响。22在 HADCM3气候模式的基础上，利用 Cropsyst 模式对2030-2060年的时间片进行了计算。利用当前和未来的作物管理措施，模拟了春小麦的冬夏季作物产量。所考虑的适应选择包括将播种日期推迟几天和使用生长周期较长/较短的品种。结果显示，南欧和法国部分地区如今已经特别容易受到干旱和热应激的影响，即使在2摄氏度(缓解)的情况下，这种情况预计也会恶化(图11图 A)。当考虑两种适应策略与缓解措施相结合时(图11图 B 和 C) ，欧洲的许多地区实际上可能会受益。尤其是北欧，可以利用降水量较高的优势，使用生长周期较长的作物品种。相比之下，在南欧，同样的适应办法将产生额外的负面影响，因为作物发展将转向夏季，而夏季较长的干旱期和热浪可能会严重影响作物生长。此外，结果表明，虽然适应存在一些区域特有的限制，但整体适应将有效减少对欧洲农业部门的影响。5.4水资源: 潜在的水资源可利用性使用全球范围的水资源影响模型(Arnell，2003)评估了这两种情景对水资源压力变化的影响。图12显示了在基线情景和缓解情景(使用 HadCM2气候模型模式)下，到2100年(相对于1961-1990年的平均值)平均年径流量的百分比变化。如果年平均径流量小于1000 m3/人均/年，我们将流域定义为处于水资源紧张状态(文献中也使用了其他定义)。气候变化的影响是通过总结(i)生活在径流显着减少(增加)的水资源紧张流域的人口(通常超过5-10%)和(ii)生活在由于气候变化而变得水资源紧张(不再水资源紧张)的流域的人口。因气候变化而面对水资源压力上升或下降的人数没有计算在内，原因有二: (i)水资源压力下降的负面影响大于水资源压力下降的有利影响; (ii)面对水资源压力上升或下降的地区分布广泛，一个地区的“盈余”不能抵消另一个地区的“亏损”。结果显示，在2050年、2080年和2100年，缓解和基线假设情景之间，水资源紧张程度增加的风险暴露存在显著差异。到2020年，这两种情况的径流量几乎没有差别。图13显示了在这两种情景下，由于气候变化而面临水资源压力增加或减少的人数。在基线和缓解方案中，生活在缺水流域的人们显然从增加的可用水量中受益，这个数字大于暴露于径流减少的人数，但是，如上所述，我们并不关注净效应。面临水资源压力变化的人数对假定的气候变化模式很敏感。与基线相比，缓解方案在2050年、2080年和2100年分别减少了1.35亿(减少12% 的影响)、2.81亿(减少20%)和4.57亿(减少30%)面临水资源压力增加的人口。然而，与此同时，也有人从气候变化中受益。具有积极和消极影响的群体的相对规模取决于所使用的气候模型(这里只使用了哈德利模式)。显然，减缓气候变化也减少了从气候变化中受益的人数。同样显而易见的是，缓解并不能消除气候变化对供水的影响，因此需要对气候变化而面临更大水资源压力的其余10亿人进行适应。适应措施可以包括增加水的储存、水的运输或通过提高效率来减少水的需求。基本结果表明，缓解效果因地区而异。事实上，在一些地区，缓解甚至可能增加暴露于压力增加的人数。具体的不确定性分析表明，结果高度依赖于气候变化引起的降水模式变化的不确定性。5.5海平面上升气候变化的另一个重要影响是海平面上升。利用 IMAGE 模型的 MAGICC 组成部分，预测了这两种情况下的全球平均海平面上升。由于海平面对全球变暖的反应迟缓，预测主要在本世纪下半叶出现分歧: 在4摄氏度和2摄氏度的情况下，2050年海平面上升分别为35厘米和31厘米，在2100年分别为71厘米和49厘米。这些预测不包括格陵兰岛和南极洲冰盖的潜在加速贡献，这可能导致更高的海平面上升，但潜在的过程没有得到充分的理解，目前不包括在气候模型中(Meehl 等，2007; Nicholls 等，2010; Vermeer 和 Rahmstorf，2009)。我们使用 DIVA 模型来评估海平面上升、相关风暴潮和社会经济发展在这两种情况下的损害和适应成本，同时考虑到海岸侵蚀(直接和间接)、强迫迁移、沿海洪水(包括河流)以及盐度入侵三角洲和河口。对于每种情况，模型首先在没有堤坝的情况下运行，然后根据提高堤坝和滋养海滩的情况进行适应(DINAS-COAST Consortium，2006; Hinkel and Klein，2009)。由于缺乏全球数据和这些过程的一般模型，无法列入诸如沿海含水层盐度入侵、沿海湿地和生物多样性丧失等进一步影响，以及诸如盐度入侵屏障、港口升级、倒退区和基于生态系统的保护等进一步适应办法。图14显示，与缓解水平无关的是，适应相当有效地降低了全球总体成本，这说明即使在目标远大的缓解情况下也必须进行适应。在总体规模上，仅通过适应战略比仅通过缓解战略可以避免更多的损害，尽管两者结合起来产生的积极影响最大。然而，从较贫穷和小岛屿国家的角度来看，严格的缓解措施对于将风险保持在可控水平是必要的。即使没有海平面上升，为了保护仅由于社会经济发展而增加的泛滥平原资产，适应措施也是具有成本效益的。虽然这将涉及大量的投资流动(全球数百亿美元) ，但它们在全球 GDP 中所占的比例相对较小，即使对于基线情景下的海平面上升而言也是如此。然而，对于个别国家或地区(特别是小岛屿国家)来说，这些成本可能占 GDP 的比例要大得多，包括完全丧失的风险。5.6供热和供冷需求(居住区和社会)气候变化可能会影响空间供冷和供热的需求。因此，我们建立了一套简单的关系来描述住宅部门的供暖和空气调节需求，并探索了气候变化对这种模拟能源需求的影响(Isaac and van Vuuren，2009)。显然，人口和收入的变化预计将导致下个世纪取暖和空气调节的能源需求大幅增长(见图15，没有气候变化的例子)。在气候的驱动下，制冷和制热实践的变化是自主适应的例子(即没有政策干预)。然而，适应并不是普遍的，因为人们并不总是能够做出反应。供暖和制冷需求得不到满足可能导致健康影响(如第5.2节所述)和劳动生产率的损失。除了这些影响，当室内温度超过一定水平时，舒适度也会降低。图15显示，在全球范围内，由于收入和财富的增加而不考虑气候变化的能源需求的自主增长远远大于基线情景和缓解情景中的能源需求之间的差异(Isaac 和 van Vuuren (2009)显示，这对其他基线也是一个强有力的结果)。气候变化对综合能源需求的影响也小于单独对供暖和空气调节的影响，因为空气调节的增加弥补了供暖的减少。在区域和国家层面，影响可能更为显著: 例如，在印度，我们预计由于冷却增加，能源需求将大幅增加，而在西欧和美国，我们预计由于供暖减少，能源需求将大幅减少。5.7极端事件气候变化预计将导致一些与天气有关的极端事件的频率和强度发生变化(Parry et al。 ，2007)。像洪水、干旱、热浪和风暴潮这样的极端天气可能会变得更加频繁和强烈，而寒冷的极端天气，如寒潮，可能会变得不那么频繁和弱化。根据平均条件的变化来评估气候变化的风险——只有极端事件风险的变化被平均化的风险。因此，一种更基于风险、更明确地理位置的方法更可取。然而，关于灾害影响的知识是复杂和有争议的。迄今为止，只有有限的国家级研究采用概率方法预测气候变化存在的未来风险，主要集中在洪水风险(Mechler et al。 ，2010)。Feyen 等人(2009)在泛欧范围内进行的一项研究计算出，在基线情景下，预期的年度损失将增加三倍。定量风险方法的一个关键制约因素是气候预测的不确定性。对于降水，例如，模型往往不同意在局部尺度的变化迹象。这对于寻找例如洪水风险的研究尤其重要。虽然 Mechler 等人(2010)的研究旨在预测未来的风险，但他们发现未来的预测是如此的不确定，以至于作者没有根据对当今洪水影响的估计来预测未来的洪水风险。然而，目前的模型和数据似乎足以以相对较高的确定性(较慢的现象)评估干旱和热浪对农业造成的综合风险。这里提供了在2 °C 和4 °C 情景下的一些工作实例。一些研究调查了全球范围内受洪水影响的人们(Hirabayashi 和 Kanae，2009; Kundzewicz 等，2010)。样本回归分析显示，在缓解方案(2摄氏度)中，全球每年受100年洪灾影响的人口平均为2.11亿人，而基线(4摄氏度)为5.44亿人。Mirza 等人(2003)指出，对于孟加拉国这样一个易受水灾影响的国家来说，即使是2摄氏度的情况，预计也会使预计的洪水泛滥面积至少增加23-29% 。然而，应当指出的是，由于暴露、脆弱性和适应方面的不确定性，对未来洪灾损失的估计范围仍然很广。关于干旱，Burke 等人(2006)对2090年代的预测表明，对于2090年代的基线情景，每100年的极端干旱事件数量和平均干旱持续时间可能分别增加2倍和6倍。有证据表明，天气和气候相关影响造成的损害在当今已经增加，但这主要是由于财富和人口的增加(Bouwer，2010)。然而，气候变化预计将随着时间的推移而增加，并可能成为未来损害增加的一个更重要的因素。IPCC 最近的报告指出，重大事件的成本预计从占地区年 GDP 和收入的几个百分点到经济强劲的大地区的25% 不等(Parry et al。 ，2007)。过去受灾严重的小岛屿国家的灾害损失实际上已经超过了年度 GDP (Cummins and Mahul，2009)。5.8影响的经济评估成本-收益分析(CBA)是用一个共同的货币单位来表示不同战略的气候变化的成本和收益。我们在这里使用 FAIR 模型的 CBA 模块(参见模型附录 A)来获得一些关于更加聚合规模的影响的概念。对于缓解成本，FAIR 模型使用了前面介绍的 IMAGE 模型的信息。FAIR 中使用的气候损害和适应成本函数是从 AD-DICE 模型中推导出来的(De Bbu 等，2009a; Hof 等，2009a)。简而言之，AD-DICE 根据 DICE 模型的损伤函数估计适应成本(Nordhaus and Boyer，2000)。AD-DICE 将这些功能分为损害成本函数和残余损害函数，基于 DICE 模型中描述的每个影响类别的评估-农业，沿海地区，健康，住区，非市场时间使用，其他脆弱市场和灾难性影响。在这项研究中，我们假设了对气候变化的最佳适应响应(即给定一个温度变化水平，模型将适应成本和剩余影响的总和最小化)。DICE (因此 FAIR)中使用的影响估计包括: (i)实际的，可测量的经济成本(所谓的市场成本) ; 和(ii)其他的，无形的损失(非市场损失) ，使用支付意愿概念货币化。损害函数与本节前面所述的物理或经济损害没有直接关系，因为它们来自单独的来源。早先已经表明，适应成本的 FAIR 结果与文献中报道的值范围一致(Hof et al。 ，2009a)。在 FAIR 模型的默认设置和2.5% 的贴现率下，2005-2200年期间气候变化影响造成的贴现成本占全球 GDP 的比例在基线水平上接近4.5% (图16)。这些成本可能看起来高于上面提到的有限的部门分析，但是包括更多的部门和可能的灾难性事件的影响(Nordhaus and Boyer，2000)。年度成本随着时间的推移急剧上升，到2200年达到17% (注意，影响估计非常不确定，文献中可以找到更高和更低的值(Parry 等，2007; Stern，2006; Tol，2002b))。只有适应或缓解的情况下，折扣成本大幅降低到2.5% 左右(图16)。Hof 等人(2008)的研究表明，气候变化的 CBA 结果对模型假设非常敏感，其中折现率起着最重要的作用。贴现率特别重要，因为随着时间的推移，与仅适应和仅缓解设想相关的成本函数不同。3.5% 的贴现率导致仅适应情景和仅缓解情景的贴现成本分别为0.8% 和1.9% 。如果使用1.4% 的贴现率(相当于 Stern (2006)使用的贴现率) ，仅适应情景和仅缓解情景的贴现成本分别为3.2% 和2.5% 。在贴现率为2.5% 的情况下，缓解与适应相结合，贴现成本最低，即占 GDP 的2% 。与文献资料一致，适应性投资被评估为小于缓解投资和剩余损害。然而，它们在限制残余损伤方面是非常重要的。需要提及一些重要的警告。首先，对于风险的极端尾部(即低概率、高影响事件) ，计算不能被视为可靠。作为对如何处理这些风险的主观评估，Weitzman (2008)质疑 CBA 对决策者的有用性。其次，考虑到时间偏好和风险的贴现率的价值目前正在激烈争论，争论涉及主观时间偏好和风险感知(Nordhaus，2008; Price，2005; Stern，2006)。如上所述，贴现率的价值可以对结果产生很大的影响。最后，非市场影响需要对损害进行主观量化; 虽然这些影响很难货币化，但一般来说，不可逆转的变化更加困难，例如导致珊瑚礁丧失的海洋变暖(Ackerman and Heinzerling，2004)。5.9气候变化、影响和适应方面的不确定性对未来气候变化及其影响的预测有许多不确定性的来源。不确定性与因果链中的每一步都有关联: 排放、气候驱动因素(如碳循环)、气候(主要是气候敏感性和气候变化模式)以及影响(包括适应能力)。因此，对于同样的排放情景，不同的研究可能会给出非常不同的结果。事实上，这些差异往往大于不同排放情景下某一特定模型产生的差异。例如，对于本世纪末的降水变化，多模式集合平均值仅在高纬度地区超过模式间的标准差(Kundzewicz et al。 ，2007)。气候变化预测的不确定性随着时间的推移而增加。在近期(例如2020年代) ，气候模型的不确定性起着最重要的作用; 而在较长的时期(例如2090年代) ，由于排放情景的选择而产生的不确定性变得越来越重要(Jenkins and Lowe，2003)。未来气候变化对极端事件的影响尤其不确定。这部分是由于粗分辨率气候模型的较大空间和时间尺度与某些极端天气(如暴雨降水和山洪)的局部发生和短期生命之间的不匹配。由于影响和适应是在当地范围内发生的，因此需要详细的信息——这意味着不确定性的增加。较大的不确定性范围表明，适应规划不应基于单一的假设情景，而是需要考虑到大范围的预测。6结论在本文中，我们讨论了情景分析如何有助于评估缓解和适应战略。我们还提出了两个集成的场景作为分析的起点。这些设想方案明确地将缓解和适应行动纳入了几个指标，并涵盖了社会经济发展与影响之间的几个重要联系和反馈(例如，考虑了气候变化对土地利用和缓解的影响)。我们为选定的一些指标确定了这些设想方案的影响，主要侧重于平均气候变化。基于我们的工作，我们得出以下结论: •通过描述两套对比鲜明的世界可能的气候变化轨迹，我们为对减缓、适应和气候影响之间的相互作用进行更加综合的分析奠定了基础。第一种情况(不采取缓解措施)预计将导致全球平均气温在本世纪末上升4摄氏度左右(气候参数和当前经济趋势的最有可能值)。正如我们的一些分析所显示的，这种情况有很高的适应需求。第二种设想假设有严格的缓解措施，并将全球平均气温变化限制在2 °C，概率为50% 。即使在这种情况下，也需要大量的适应措施。•这里提出的综合情景分析可以为探讨政策选择的不同后果(包括不确定性)奠定良好基础; 鉴于不确定性，确定缓解、适应和剩余损害之间的最佳组合是不可行的。正如本文所讨论的那样，衡量气候变化的后果和各种政策回应的复杂性在于规模、空间和时间上的巨大差异; 巨大的不确定性; 以及行为者之间利益的明显差异(例如，他们是气候变化的肇事者还是受害者)。因此，对风险的主观解释将始终发挥重要作用。尽管如此，情景分析可以提供对可能结果和风险的描述。在这个阶段，成本和收益的货币评估(第5.8节)不能与前面章节中对物理变化的描述联系起来。有效的气候政策包括适应和减缓。模型计算表明，可以设计缓解设想方案，使全球平均气温上升2摄氏度，从而达到对气候敏感性的最佳猜测。然而，即使是这些严格的情况也可能导致全球平均气温上升超过2.5摄氏度(最多上升1.5摄氏度)和区域气温变化更大。本文件探讨的大多数影响都表明需要将缓解和适应结合起来。例如，在应对海平面上升(至少在21世纪)方面，适应措施可能比缓解措施更有效，但缓解措施在减少损害和降低适应成本方面仍然可以发挥作用。农业提供了一个明显需要适应和缓解的例子。如果不采取适应和减缓行动，预计许多区域的农作物产量将因气候变化而受到不利影响。如果没有严格的缓解措施，适应只能限制负面影响，而不能消除它们。缓解的一个好处是，它影响到所有影响类别，而适应需要根据影响和环境进行调整。•尽管气候变化的影响可能很严重，而且根据主观选择，可能需要制定严格的气候政策，但本研究评估的影响(鉴于目前的技术水平)可能仍然是全球范围内人口变化和经济增长的次要影响。然而，需要注意的是(见下文)。虽然气候变化可能对数百万人产生影响，但其他挑战可能对人民和治理产生更大的影响。然而，应该指出的是，我们只涉及了有限的一组影响，并且主要集中在对逐渐变化的气候的平均估计上，例如，没有涉及灾难性的、影响非常大的、极低概率的事件(Weitzman，2008)。这些事件实际上可能非常严重，以至于上述结论不再成立。如果全球范围的成本仍然相对较低，就不太需要进行全球分析，以便根据故事情节的一致性纳入对主要驱动因素的所有反馈。显然，在地方一级，情况可能大不相同; 对个别国家的影响可能远远大于全球一级的影响。例如，海平面上升对一些低洼岛国和国家非常重要，这些国家可能会受到巨大的适应成本和/或损害(直至完全毁灭)的显著影响。对农业而言，预计积极和消极影响将在不同地方和不同时间发生，低收入国家往往受到相对较为负面的影响。目前温度受到限制的温带地区的农业可能会受益。总之，我们认为，进一步制定在区域范围内进一步具体说明这些情况的综合设想是有益的。虽然本文提出了一个有用的第一步，它也留下了许多反馈意见仍然没有说明。本研究中的总体缓解成本估计为2 °C 情景下国内生产总值的1-2% 左右。缓解方案降低了气候变化的风险。在缓解方面的投资有几种类型的好处。首先，与气候相关的损害和适应成本得到降低。其次，不确定性也会减少，考虑到所涉及的风险，这一点很重要。虽然我们认为，在全球一级，缓解和适应之间不可能存在最佳的平衡，但我们已经表明，从长远来看，缓解和适应的成本和收益是相当的。进一步分析的重点包括评估实际变化与货币影响分析之间的联系、极端事件的可变性和变化、大规模干扰和治理的潜在作用。在我们和其他评估中，主要关注的是平均值的变化，然而，人们对与气候变化有关的极端事件(导致自然灾害) ，以及大规模的破坏(如西南极冰盾的解体)有相当大的关注，这些破坏并没有被平均值准确地描述。对气候变异性变化的预测高度不确定，迄今为止常常妨碍分析有力地预测未来的极端事件风险。不同行为者的作用是另一个问题; 某些形式的适应需要政府的积极参与; 其他形式的适应可能由私人投资者实施，例如安装空间冷却系统。这两个适应主体之间的差异与未来的情景发展有关。本文中提出的研究是作为欧盟资助的 ADAM 研究项目的一部分进行的。这篇论文的早期版本是作为《让气候变化为我们服务》一书的一部分出版的，该书由休姆和纽菲尔德编辑，剑桥大学出版社于2010年出版。附录 A.模型描述 A.1 IMAGE 2.4 IMAGE 2.4综合评估模型(Bouwman et al。 ，2006)由一系列相互关联的综合模型组成，这些模型共同描述了全球环境变化长期动态的重要因素，如空气污染、气候变化和土地使用变化。作为 IMAGE 的一部分，全球能源模型 TIMER (van Vuuren et al。 ，2006)描述了一次能源和二次能源需求和生产的长期动态以及温室气体和区域空气污染物的相关排放。模型行为主要是由各种技术的替代过程决定的，基于长期价格和燃料偏好。IMAGE 的农业模型模拟了7个作物类别和5个动物类别的生产力(Leemans 和 Born，1994)。根据一套分配规则，农产品的区域生产在空间上(0.5 ° × 0.5 °)分布(Alcamo et al。土地利用变化图和农业活动数据都被用来模拟土地利用(变化)产生的排放。MAGICC 模型利用温室气体排放量来计算全球平均气温变化(Wigley and Raper，2001)。温度变化模式是通过与大气环流模式(GCM)产生的气候变化模式联系而获得的。局限性: IMAGE 提供了对人类活动的物理描述(使用成吨的石油，生产成吨的谷物等)。更全面的宏观经济描述只能通过与其他模型的合作得到。IMAGE 作为综合评估模型的广泛覆盖范围意味着许多关键的不确定性影响模型的结果。在这种情况下，使用单一的基线(如在 ADAM 项目中)并不能完全满足所涉及的基本不确定性。A. 2 FAIR 气候政策模型 FAIR (Den Elzen 等，2008)与 IMAGE 模型一起用于确定不同排放源的减排率。全球气候计算利用了简单的气候模型 MAGICC 4.1(Wigley，2003; Wigley and Raper，2001)。所要求的全球减排量是通过计算基准和全球排放路径之间的差额得出的。FAIR 成本模型使用不同排放源的区域边际减排成本曲线(MAC) ，采用最小成本方法，在各区域之间进行分配。最近，FAIR 模型已经扩展到损害和适应成本曲线(基于 AD-DICE 模型(De Bbu 等，2009b)和估计宏观经济对 GDP 增长的影响的能力(Hof 等，2008))。这使模型能够探讨减缓和适应综合战略的经济影响。限制: 为了灵活起见，公平竞争模式不包括部门宏观经济模式或能源模式。因此，该模型从一个局部均衡的方法工作-和更深层次的后果气候政策只能通过转发公平的结果到其他(相关)模型研究。A.3 DIVA DIVA (动态和互动脆弱性评估)是在欧盟资助的项目 DINAS-COAST 44中开发的一个沿海系统的综合模型，连同其适当的沿海数据库，对沿海地区对海平面上升的国家、区域和全球脆弱性进行动态和互动评估;  http://www.pik-potsdam.de/DINAS-COAST/。(DINAS-COAST Consortium，2006; Hinkel and Klein，2009).《综合发展战略》提供了一系列生态、社会和经济沿海脆弱性指标的定量信息，从国家以下各级到全球各级，涵盖所有沿海国家。该模型由来自各种工程、自然和社会科学学科的专家开发的若干模块组成。根据气候和社会经济情景，该模型评估了海岸侵蚀(直接和间接)、海岸洪水(包括河流)、湿地变化和盐度入侵三角洲和河口。家庭影响评估还从提高堤坝和滋养海滩的角度考虑沿海适应问题，并包括一些预先确定的适应战略，例如不予保护、充分保护或最佳保护。限制因素: 《综合可持续发展战略》排除了可能影响沿海影响的下列进程，但目前无法有把握地建立模型: 风暴频率和强度的变化、沿海快速发展和城市化导致的国内生产总值的地方分布和人口增长，以及盐度侵入沿海含水层。由于高程数据的粗分辨率和精度，还会产生更多的重要不确定性。A. 4 TIMER ——冷却/加热能源需求 TIMER 冷却/加热能源需求模型(Isaac and van Vuuren，2009)描述了冷却和加热能源的使用是几个因素的函数，包括人口水平、不断变化的收入水平和气候。对于加热和冷却，经验数据被用来校准一组系统-动态需求函数。气候(降温和升温度日)起着重要作用。该模型能够解释气候变化的影响。局限性: 对发展中国家而言，校准模型的经验基础相对较差。该模型没有描述可以提供冷却和加热需求的不同方式以及用一种技术替代另一种技术所涉及的成本。水资源影响模型水资源影响模型(Arnell，2003,2004)有两个组成部分。第一阶段采用宏观尺度水文模型 Mac-PDM 模拟全球地表(0.5 ° × 0.5 °)的径流，第二阶段通过计算人均水资源可利用率确定流域水资源压力指标。如果一个流域的年平均径流量低于每人每年1000立方米，则假定该流域面临水资源压力，这是一个半任意的阈值，广泛用于确定水资源压力区域。如果气候变化导致缺水流域的径流量显著减少，或者导致流域降低到阈值以下，那么气候变化将导致水资源压力暴露的增加。气候变化导致对相反趋势的暴露明显减少。这些变化不能直接比较; 虽然径流量的减少(和暴露量的增加)极有可能是不利的，但如果额外的水不能储存，或者在洪水增加的高流量季节发生，则径流量的增加(和暴露量的明显减少)可能不是有益的。生活在水资源压力增加的流域的人口数量可以作为暴露于气候变化的一个指标。实际影响(就真正的水资源短缺而言)将取决于现有的水资源管理结构。局限性: 水文模型不能完全模拟河流径流量，特别是在半干旱地区倾向于高估径流量。水资源指标是衡量受影响程度的指标，而不是实际影响; 它可以被视为适应需求的替代指标。答.6疟疾的危险疟疾媒介，蚊子传播感染，只能生存在适当的气候与高平均温度，没有霜冻和足够的降水。MARA/ARMA 疟疾适宜性模型(Craig et al。 ，1999)综合了这些气候因素来确定气候适宜区域。最大适合度为1和最小适合度为0所需的气候水平见表 A.1。对于水平介于0或1适合性所需水平之间的指标，使用简单函数计算水平(Craig et al。 ，1999)。利用 IMAGE 模型的输出结果，所有这些因子都是在半乘以半度的网格水平上计算的(Bouwman et al。 ，2006)。每个栅格细胞的总气候疟疾适应性是由这三个指数中的最低值决定的。局限性: MARA/ARMA 模型描述了对疟疾病媒的适用性。它没有提供蚊子传播的过程说明，也没有明确说明人们可能对增加的风险水平作出何种反应。参考文献 Ackerman and Heinzerling，2004 F。 Ackerman L。 Heinzerling 无价之宝: 关于了解一切事物的价格和一无所有的价值2004 The New Press 纽约 Agrawala and Fankhauser，2008 S。 Agrawala S。 Fankhauser 适应气候变化的经济方面。成本、收益和政策工具2008经合组织巴黎 Alcamo 等人，1998年 J。 Alcamo E。 Krol R。 Leemans J。 Bolen J。 Minnen M。 Schaeffer S。 Toet B。 Vries 全球环境变化模型: IMAGE 2.1 J。IMAGE 2.1型号1998爱思唯尔科技有限公司测试结果。Arnell，2004 N.Arnell 气候变化与全球水资源: 气候变化与社会经济情景全球环境变化14120043152 Arnell 等，2002 N.W。Arnell M.G.R.Cannell M. Hulme R.S.Kovats J.F.B.Mitchell R.J.Nicholls M.L.Parry M.J.L.利弗莫尔 · A · 怀特二氧化碳稳定化对气候变化影响的后果气候变化5342002413446 Bakkenes 等，2006 M。 Bakkenes B. Eickhout R. Alkemade 不同气候稳定化情景对欧洲植物物种的影响全球环境变化16120061928,2003年代表全球气候变化、适应和减缓全球环境变化132003年16巴克等人，2009年巴克，t。 ，肯伯，M。 ，Scrieciu，S。打破气候僵局。降低成本: 合作气候行动的经济效益。气候小组，托尼 · 布莱尔办公室，4CMR-剑桥大学和 Cambridge Econometrics。Barker and Scrieciu，2010 T Barker SS.Scrieciu 用 E3MG 模拟低稳定性: 走向模拟能源-环境-经济系统动力学的“新经济学”方法能源期刊31特刊12010137164 Barker 等，2008 T。Scrieciu T. Foxon 实现八国集团50% 的目标: 使用宏观经济计量模型 E3MG 气候政策82008 S30 S45 Berkhout 等,2002 F. Berkhout J. Hertin A. Jordan 气候变化影响评估中的社会经济未来: 使用情景作为“学习机器”全球环境变化12220028395 Bouwer，2010 L.M。人为气候变化造成的灾害损失增加了吗？2010年美国气象学会简报10.1175/2010BAMS 3092.1 Bouwman 等人，2006年 A.F. Bouwman T. Kram K. Klein Goldewijk 全球环境变化综合模拟。2006年荷兰环境评估机构 Bilthoven 228页。(出版物500110002/2006) Burke 等人，2006 E.J。 Burke S.J。 Brown N. Christidis 利用哈德利中心气候模型对21世纪全球干旱的近期演变和预测进行建模。《水文气象学杂志》2006年7月1113日1125 Clarke 等人，2010年 L。 Clarke J. Edmonds V. Krey R. Richels S. Rose M. Tavoni 国际气候政策架构: EMF 22国际情景的概述能源经济学31号补编。2010年 S64/s81哥本哈根协议，2009年哥本哈根协议，2009年。(2009年12月18日哥本哈根协议)。2009年联合国气候变化会议，哥本哈根。Craig 等人，1999 M.H。Craig R.W.基于气候的疟疾在非洲传播的分布模型寄生虫学今天1531999105111康明斯和马胡尔，2009 J.D。发展中国家巨灾风险融资: 公共干预原则，2009年世界银行华盛顿，DC De Bbu 等，2009a K.C。德布鲁姆 R.B。Dellink S. Agrawala 适应气候变化的经济方面: 适应成本和效益综合评估模型，2009年经合组织巴黎德布鲁恩等，2009 b。德布鲁姆 R.B。Dellink R.S.J.Tol AD-DICE: DICE 模型气候变化951-220096381 Den Elzen 等，2008 M.G。J.登伊尔森私人侦探社。Lucas D.P.为实现低二氧化碳当量浓度而采取的区域减排行动和分配办法下的排放限额费用气候变化9032008243268 Den Elzen 和 van Vuuren，2007 M.G。J.Den Elzen D.P.104-46/2007/17931/17936 DINAS-COAST 财团，2006年 DINAS-COAST 财团，2006年，更有可能以更低的成本实现长期温度目标的范维伦峰值美国国家科学院院刊。DIVA 1.5.5光盘，德国波茨坦气候影响研究所。伊斯特林等，2007 W。伊斯特林 P。阿加瓦尔 P。巴蒂玛 K。布兰德 L。埃尔达 M。霍登 A。基里连科 J。莫顿 J。Soussana J. Schmidhuber F.N. Tubiello 食品、纤维和森林产品 M.L. Parry O.F. Canziani J.P. Palutikof P.J. van der Linden C.E. Hanson 气候变化2007: 影响、适应和脆弱性。第二工作组对政府间气候变化专门委员会2007年剑桥大学出版社第四次评估报告的贡献剑桥，英国欧共体，2006年欧共体2050年世界能源技术展望(WETO H2)2006年欧盟委员会布鲁塞尔埃登霍费尔等人,2010 O。 Edenhofer B。 Knopf T。 Barker L。 Baumstart E。 Bellevrat B。 Chateau P。 Criqui M。 Isaac A。 Kitous S。 Kypreos M。 Leimbach K。 Lessmann B。 Magné S。 Scrieciu H。 Turton D。Van Vuuren 低稳定性的经济学: 减缓战略和成本的模型比较能源杂志31 SI-120101148环境变化研究所，2009环境变化研究所，2009。国际气候会议-4度及以上。英国牛津大学环境变化研究所，9月28日至30日。Feyen J.I. Barredo R. Dankers 全球变暖和城市土地利用变化对欧洲洪水的影响。迈向工程、设计和管理方法的一体化2009 Taylor and Francis Group London Fischer et al。 ，2007 G. Fischer F.N. Tubiello H. van Velthuizen D.Wiberg 气候变化对灌溉用水需求的影响: 缓解的影响，1990-2080技术预测和社会变化747200710831107 Fisher et al。时间序列: K.Jiang M.Kainuma E. La Rovere A. Matysek A. Rana K. Riahi R. Richels S. Rose D. van Vuuren R. Warren P. Ambrosi F. Birol D. Bouille C. Clapp B. Eickhout T. Hanaoka M.D. Mastrandrea Y.Matsuoko B.O’Neill H. Pitcher S. Rao F. Toth 与长期减缓有关的问题:。减缓气候变化。第三工作组对2007年政府间气候变化专门委员会剑桥大学出版社第四次评估报告的贡献,2010年 A. Hayashi K. Akimoto F. Sano S. Mori T. Tomoda 评估全球变暖对不同稳定水平的影响，作为确定长期稳定目标气候变化的一个步骤98201087112 Hilderink et al。 ，2008 H. Hilderink P.L。卢卡斯 · A · 滕霍夫 · 科克 · M · 德沃斯 · P · 詹森 · J · 梅耶尔 · A · 费伯尔 · A · 伊格纳修克 · A · 彼得森 · H · J。M.2008年荷兰环境评估机构 Bilthoven Hinkel 和 Klein，2009年 J。T.Klein 整合知识以评估海平面上升对沿海脆弱性的影响: DIVA 工具全球环境变化的发展1932009384395 Hirabayashi and Kanae，2009 Y. Hirabayashi S. Kanae 第一次估计未来全球人口面临洪水风险水文研究快报3200969 Hof 等，2009a A.F。霍夫 K.C。德布鲁姆 R.B。Dellink M.G.J.Elzen D.P.不同减缓战略对国际适应融资的影响环境科学和政策1272009832843 Hof 等，2009b A.F。霍夫・德・布鲁姆・ R ・德林克・ M.G。J.Elzen D.P.2012年后的全球气候治理: 架构、机构和适应2009年剑桥大学出版社剑桥霍夫等人，2008年 A.F。Hof M.G.J.Elzen D.P.范维伦分析气候政策的成本和收益: 价值判断和科学不确定性全球环境变化1832008412424 IMAGE-team，2001 IMAGE-team，2001。IPCC SRES 情景的图像2.2实现。全面分析21世纪的排放、气候变化和影响。RIVM 光盘出版物481508018，比尔特霍芬国家公共卫生与环境研究所。政府间气候变化专门委员会，2007年，2007年。2007年气候变化: 综合报告。第一、第二和第三工作组对政府间气候变化专门委员会第四次评估报告的贡献。政府间气候变化专门委员会，日内瓦，104页。能源政策背景下的全球住宅部门供暖和空气调节的能源需求建模。处理 UKCIP02气候变化情景中的不确定性。埃克塞特气象局哈德利中心技术说明44。Kinney et al。 ，2008 P.L。 Kinney M.S. O’Neill M.L。 Bell J. Schwartz 用于估计气候变化对与热有关的死亡的影响的方法: 挑战和机会环境科学和政策11872008 Klein et al。 ，2007 R.J.T. Klein S. Huq F. Denton T.E. Downing R.G. Richels J.B. Robinson F.L. 适应和缓解之间的相互关系。2007年气候变化。影响、适应和脆弱性。第二工作组的贡献。2007年政府间气候变化专门委员会报告剑桥大学出版社剑桥745777 Krol 等人，1997年 M. Krol J. Alcamo R. Leemans 稳定大气中二氧化碳的全球和区域影响全球变化的缓解和适应策略11997年341361 Kundzewicz 等人，2010 Z.W。Kundzewicz y. Hirabayashi 气候变化中的 S. Kanae River 洪水——水资源管理的观察和预测2010年10.1007/s11269-009-9571-6 Kundzewicz 等，2007 z.W。Kundzewicz L.J.Mata N. Arnell P. Döll P. Kabat B. Jiménez K. Miller T. Oki Z. en I. Shiklomanov 淡水资源及其管理。招架。坎齐亚尼 J.P。普鲁提克。Hanson P.J.2007年范德林登气候变化: 影响、适应和脆弱性。第二工作组对2007年政府间气候变化专门委员会剑桥大学出版社第四次评估报告的贡献。翻译。D.确定自然植被、作物和农业生产力的潜在全球分布水、空气和土壤污染761994133161 Lenton 等人，2008 T.M。Lenton H 拘留了 E. Kriegler J.W。鲁赫特 · S · 拉姆斯托夫 · H · J 大厅。Schellnhuber 地球气候系统中的倾斜元素美国国家科学院院刊1056200817861793。Manne R.G.Richels Merge: 全球气候变化综合评估模型。Zaccour Energy and Environment 2005 Springer USA McMichael et al。 ，1996 A. McMichael A. Haines R. Sloff S. Kovats Climate Change and Human Health 1996 World Health Organization Geneva Mechler et al。 ，2010 R. Mechler S. Hochrainer A. Aaheim Z. Kundzewicz N. Lugeri M. Moriondo H. Salen M. Bindi I. Banaszak A. Chorynski E. Genovese H. Kalirai J. Linnerooth-Bayer C. Lavalle D. McEvoy P. Matczak M. RadzieJewski D. Rübbelke M.-J。评估欧洲适应不断变化的洪水和干旱风险的风险管理方法 M.Hulme H. Neufeldt 让气候变化为我们服务: 关于适应和减缓战略的欧洲观点2010年剑桥大学，英国 Meehl 等人，2007年 G.A. Meehl T.F. Stocker W.D. Collins Friedlingstein A.T. Gaye J.M. Gregory A. Kitoh R. Knutti J.M. Murphy A. Noda S.C.B. Raper I.G. Watterson A.J. Weaver Z- C。赵全球气候预测2007所罗门气候变化: 物理科学基础。第一工作组对2007年剑桥大学出版社第四次评估报告的政府间气候变化专门委员会。减缓气候变化。第三工作组对2007年政府间气候变化专门委员会剑桥大学出版社第四次评估报告的贡献剑桥，United Kingdom Mirza 等，2003 M.M。问:。Mirza R.A.Warrick N.J.气候变化对恒河、 Brahmaputra 和梅格纳河洪水的影响孟加拉国气候变化572003287318 Moriondo et al。欧洲农业应对气候变化和可变性的影响和适应机会全球变化的缓解和适应战略1572010657679 Moss 等，2010 R.H。Moss J.A.Edmonds K.A.Hibbard M.R.Manning S.K.Rose D.P.Van Vuuren T.R.Kainuma T. Kram G.A.Meehl J.F.B.Mitchell N. Nakicenovic K. Riahi S.J.史密斯 R.J。早上吃饱。汤姆森 J.P。Weyant T.J.Nakicenovic 等，2000 N。 Nakicenovic 排放情景特别报告(SRES)2000剑桥大学出版社剑桥，英国 Nakicenovic 等，2000,nakicenovic P. Kolp K. Riahi M. Kainuma T. Hanaoka 排放情景评估重新审视环境经济学和政策研究732006137173 Nicholls and Lowe，2004 R.J。Nicholls J.A.全球环境变化1432004229244 Nicholls 等，2010 R.J。Nicholls N. Marinova J.A.劳 · S · 布朗 · P · 维林加 · D · 德 · 古斯芒 · J · 欣克尔。J.21世纪英国皇家学会哲学汇刊2010年10月10日。2010.029 Nordhaus and Boyer，2000 W.D.诺德豪斯 · J · 博耶(Nordhaus J. Boyer)《全球变暖: 2000年全球变暖的经济模型》麻省理工学院出版社，剑桥，马萨诸塞州，第10页。315-328 Nordhaus，2008 W.D.平衡的问题衡量全球变暖政策的选择2008年纽黑文和伦敦帕里等人的耶鲁大学出版社，2007年。招架。坎齐亚尼 J.P。PJ 的 Palutikof。Van der Linden C.E.2007年汉森气候变化: 影响、适应和脆弱性。返回文章页面第二工作组对2007年剑桥大学出版社第四次评估报告的贡献政府间气候变化专门委员会:？气候变化9932010383402 Piani 等人，2005 C. Piani D.J。陷害地方检察官。Stainforth M.R.艾伦对气候变化的约束来自数千名成员的模拟地球物理研究通讯322005 L23825 Price，2005 c 价格关于环境变化影响的代际视角: 折现未来的观点 J.L。Innes 通用汽车。Hickey H.F.2005年国际林业研究组织联合会(国际林研组织)维也纳罗斯等人，2007年，Ahammad，h,a，Rao，S. ，Riahi，K. ，van Vuuren，D. 2007.土地在气候稳定模拟中的作用: 初步观测能源模拟论坛报告。斯坦福大学。Schneider 和 Kuntz-Duriseti，2002 S.H。不确定性与气候变化政策。奈尔斯气候变化政策: 一项调查2002年岛屿出版社华盛顿特区斯特恩，2006年 N。斯特恩气候变化经济学评论2006年剑桥大学出版社剑桥斯沃特等人,斯瓦特 · L · 伯恩斯坦 · M · Ha-Duong A. Petersen 同意不同意: 政府间气候变化专门委员会评估气候变化、影响和应对措施的不确定性管理922009129斯瓦特和雷斯，2007 R · 斯瓦特 · F · 雷斯将适应和缓解工作结合起来: 纳入可持续发展政策的主流？气候政策742007288303 Tol，2002a。第二部分。环境和资源经济学2122002135160托尔，2002年 b。第一部分。基准估计环境和资源经济学21120024773 Tol，2002 c R.S.J. Tol 福利规格和气候变化的最佳控制: 基金的应用能源经济学2442002367376 Tubiello and Fischer，2007 F.N. Tubiello G. Fischer 减少气候变化对农业的影响: 减缓的全球和区域影响，2000-2080年技术预测和社会变化747200710301056联合国，2005年。世界人口前景: 2004年修订本。光盘版-扩展数据集。联合国出版物。E.05.十三.12，联合国，经济和社会事务部，人口司。Van Vliet et al。 ，2009 J.van Vliet M.G.J.den Elzen D.P. van Vuuren 会议根据延迟参与的能源经济31号补充辐射效应制定了目标。22009 S152 S162 van Vuuren et al。 ，2006 D.P. van Vuuren B。 van Ruijven M。 Hoogwijk M。 Isaac B。 De Vries TIMER 2: 模型描述和应用 L。 Bouwman T。 Kram K。 Klein-Goldewijk 全球环境变化综合模型。IMAGE 2.42006 MNP 概述-荷兰环境评估机构 Bilthoven van Vuuren 等，2007 D.P。Van Vuuren M.G.J.登伊尔森私人侦探社。Lucas B. Eickhout B.J.稳定低水平的温室气体浓度: 减少战略和成本的评估气候变化8122007119159 Van Vuuren 等，2008 a D.P。Van Vuuren B. De Vries A. Beusen P.S.C.21世纪温室气体排放的有条件概率估计基于 IPCC-SRES 设想的全球环境变化1842008635654 van Vuuren 等，2008b D.P。Van Vuuren M Meinshausen G.K.普拉特纳 · F · 乔斯 · K.M。斯特拉斯曼 S.J。Smith T.M.L.Wigley S.C.B.Raper K. Riahi F. De La Chesnaye M.G.J.登藤野 K。江 N。 Nakicenovic S。 Paltsev J.M。21世纪减缓美国国家科学院院刊的温度上升。Van Vuuren M.G.J.艾萨克不同气候制度的比较: 扩大参与的影响能源政策3712200953515362 van Vuuren 等，2010 D.P。Van Vuuren E. Stehfest M.G.J.艾萨克探索将温室气体辐射效应控制在3瓦/平方米以下的情景能源经济学31特刊12010165192维梅尔和拉姆斯托夫,2009年 M. Vermeer S. Rahmstorf 与全球气温美国国家科学院院刊相关的全球海平面10620092152721532 Weitzman，2008 Weitzman，M.L。，2008年。灾难性气候变化的经济学模型与解释。Wigley，2003 T.M.L. Wigley MAGICC/SCENGEN 4.1: 技术手册2003 UCAR-气候和全球动力学分部博尔德，CO Wigley and Raper，2001 T.M.L. Wigley S.C.B. Raper 全球平均变暖科学高预测的解释2932001451454|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+in+Graph+Machine+Learning:+Recent+Advances+and+Future+Prospectives)|0|
|[Socially Responsible Machine Learning: A Causal Perspective](https://doi.org/10.1145/3580305.3599571)|Raha Moraffah, AmirHossein Karimi, Adrienne Raglin, Huan Liu|Pukyong Natl Univ, Grad Sch Management Technol, Busan, South Korea; Shanghai Lixin Univ Accounting & Finance, Shanghai, Peoples R China|The underlying assumption of using investor sentiment to predict stock prices, stock market returns, and liquidity is that of synergy between stock prices and investor sentiment. However, this synergistic relationship has received little attention in the literature. This paper investigates the synergistic pattern between stock prices and investor sentiment using social media messages from stock market investors and natural language processing techniques. At the macro level, we reveal extremely significant positive synergy between investor sentiment and stock prices. That is, when a stock price rises, investor sentiment rises, and when a stock price falls, investor sentiment falls. However, this synergy may be reversed or even disappear over a specific time period. Through a segmented measurement of the synergy between stock prices and investor sentiment over the course of a day, we also find that investor sentiment on social media is forward looking. This provides theoretical support for using investor sentiment in stock price prediction. We also examine the effect of lockdowns, the most draconian response to COVID-19, on synergy between stock prices and investor sentiment through causal inference machine learning. Our analysis shows that external anxiety can significantly affect synergy between stock prices and investor sentiment, but this effect can promote either positive or negative synergy. This paper offers a new perspective on stock price forecasting, investor sentiment, behavioral finance, and the impact of COVID-19 on the stock markets. Copyright (c) 2022 Borsa Istanbul Anonim S, irketi. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).|利用投资者情绪来预测股价、股市回报和流动性的基本假设是股价和投资者情绪之间的协同作用。然而，这种协同关系在文献中很少受到关注。本文利用来自股市投资者的社交媒体信息和自然语言处理技术，研究了股票价格与投资者情绪之间的协同关系。在宏观层面，我们发现投资者情绪与股价之间存在极其显著的创建力量。也就是说，当股价上涨时，投资者情绪上升，当股价下跌时，投资者情绪下降。然而，这种协同作用可能会逆转，甚至在特定的时间段内消失。通过对股票价格和投资者情绪在一天内的协同效应进行分段测量，我们还发现，社交媒体上的投资者情绪具有前瞻性。这为利用投资者情绪进行股价预测提供了理论支持。我们亦会透过因果推理机器学习，研究封锁对股价与投资者情绪之间的协同效应的影响。封锁是对2019冠状病毒疾病最严厉的回应。我们的分析表明，外部焦虑可以显著影响股票价格和投资者情绪之间的协同效应，但这种效应可以促进正面或负面的协同效应。本文提供了一个新的视角股票价格预测，投资者情绪，行为金融学，以及2019冠状病毒疾病对股票市场的影响。版权所有(c)2022伊斯坦布尔博尔萨 Anonim S，irketi。由 Elsevier B.V 出版。这是 CC BY-NC-nd 许可证下的一篇开放存取文章( http://creativecommons.org/licenses/BY-NC-ND/4.0/)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Socially+Responsible+Machine+Learning:+A+Causal+Perspective)|0|
|[Training Large-scale Foundation Models on Emerging AI Chips](https://doi.org/10.1145/3580305.3599573)|Aashiq Muhamed, Christian Bock, Rahul Solanki, Youngsuk Park, Yida Wang, Jun Huan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Training+Large-scale+Foundation+Models+on+Emerging+AI+Chips)|0|
|[How to DP-fy ML: A Practical Tutorial to Machine Learning with Differential Privacy](https://doi.org/10.1145/3580305.3599561)|Natalia Ponomareva, Sergei Vassilvitskii, Zheng Xu, Brendan McMahan, Alexey Kurakin, Chiyaun Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+to+DP-fy+ML:+A+Practical+Tutorial+to+Machine+Learning+with+Differential+Privacy)|0|
|[Trustworthy Machine Learning: Robustness, Generalization, and Interpretability](https://doi.org/10.1145/3580305.3599574)|Jindong Wang, Haoliang Li, Haohan Wang, Sinno Jialin Pan, Xing Xie|Univ Wisconsin, Madison, WI 53706 USA; Google, Mountain View, CA USA|An emerging problem in trustworthy machine learning is to train models that produce robust interpretations for their predictions. We take a step towards solving this problem through the lens of axiomatic attribution of neural networks. Our theory is grounded in the recent work, Integrated Gradients (IG) [STY17], in axiomatically attributing a neural network's output change to its input change. We propose training objectives in classic robust optimization models to achieve robust IG attributions. Our objectives give principled generalizations of previous objectives designed for robust predictions, and they naturally degenerate to classic soft-margin training for one-layer neural networks. We also generalize previous theory and prove that the objectives for different robust optimization models are closely related. Experiments demonstrate the effectiveness of our method, and also point to intriguing problems which hint at the need for better optimization techniques or better neural network architectures for robust attribution training.|值得信赖的机器学习中一个新出现的问题是训练模型，为它们的预测产生可靠的解释。我们通过神经网络的公理属性透镜来解决这个问题。我们的理论是基于最近的工作，综合梯度(IG)[ STY17] ，在公理归因于一个神经网络的输出变化的输入变化。在经典的鲁棒优化模型中，我们提出训练目标来获得鲁棒 IG 属性。我们的目标提供了原则性的概括以前的目标设计的稳健预测，他们自然退化到经典的软边际训练的一层神经网络。我们还推广了以往的理论，证明了不同鲁棒优化模型的目标是密切相关的。实验证明了我们的方法的有效性，也指出了有趣的问题，暗示需要更好的优化技术或更好的神经网络架构的鲁棒性归因训练。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trustworthy+Machine+Learning:+Robustness,+Generalization,+and+Interpretability)|0|
|[Large-Scale Graph Neural Networks: The Past and New Frontiers](https://doi.org/10.1145/3580305.3599565)|Rui Xue, Haoyu Han, Tong Zhao, Neil Shah, Jiliang Tang, Xiaorui Liu|College of Biosystems Engineering and Food Science, Zhejiang University, Huajiachi Campus, Hangzhou, PR China; Graduate School of Agricultural and Life Sciences, The University of Tokyo, Yayoi 1-1-1, Bunkyo-ku, Tokyo 113-8657, Japan; Maekawa Manufacturing Co., Ltd., Tatsuzawa 2000, Moriya-shi, Ibaraki-Prefecture 302-0118, Japan|Highlights ► A real-time detection method by UV–Vis spectroscopy was developed for monitoring of ATP and viable cells on meat surface. ► The linear relationship was observed between the ATP amount and plate count with the determination coefficient of 0.95. ► The 2nd derivative of reflectance spectra gave a high correlation for the first 48 h with both ATP amount and viable cell count at 318 nm. Abstract Cleanliness monitoring at slaughterhouses depend on traditional methods, e.g. , visual inspection or swabbing. The visual inspection is not always accurate. Swabbing requires skilled workers and further plate count or ATP bioluminescence technique. To solve these problems, a rapid technique based on non-destructive UV–Vis reflectance was developed to monitor the ATP and viable cells. Samples were lean part of pork loin. The samples stored at 15 °C were analyzed at 0, 24, 48, 72, 84 and 96 h for ATP, plate count and UV–Vis reflectance. The reflectance spectra were measured from 240 to 540 nm at 20 °C, and then the area of 40 × 40 mm 2 of the sample surface was swabbed for the determination of plate count and ATP amount. The plate count on the sample surface increased from the initial count of 29 to 3.2 × 10 7 CFU/cm 2 after 84 h. The ATP amount also increased with time from the initial amount of 9.2 × 10 −15 to 2.8 × 10 −10 mol/cm 2 after 84 h. The linear relationship was observed between the ATP amount and plate count with the determination coefficient of 0.95. The 2nd derivative of raw spectra gave a high correlation for the first 48 h with both ATP amount and viable cell count showing the determination coefficient of 0.89 and 0.83, respectively at 318 nm. The results strongly suggested that the UV–Vis reflectance spectrum analysis could be used as the real-time monitoring of ATP and/or plate count on meat surface with the optimal wavelength. Keywords ATP Sanitation Real-time monitoring Non-destructive detection Pork Spectroscopy Plate count Absorbance Reflectance Meat Quality 1 Introduction Muscle foods that include both meat and poultry are an integral part of the human diet and have been so for several thousand years. However, within the past two decades public concern, as well as awareness, has been raised due to high profile food safety issues such as the BSE and foot and mouth epidemics ( Fox, 2001; Pickrell and Enserink, 2001 ). These outbreaks, along with concerns over specific pathogenic bacteria within meats have illustrated the requirement for a rapid and accurate detection system for microbial spoilage of meats within what is a large-scale production industry whose turn over is billions of £ and $ per annum ( Ellis and Goodacre, 2001 ). The major role of microorganisms in the spoilage of food and the role of food as a vector for the transmission of microbes responsible for food-borne disease are well recognized. At a slaughterhouse of poultry, pork and beef, monitoring of cleanliness depends mainly on traditional methods of visual inspection, swabbing and subsequent viable cell count or ATP bioluminescence technique ( Hawronskyj and Holah, 1997 ). This is especially important for microbial hazards associated with food process. In the case of poultry, pork and beef processing, verification of the efficacy of preventive measures to reduce or eliminate microbial hazards may be achieved by routine carcass analysis using cultural method, i.e., classical Standard Plate Count ( Bautista et al., 1997 ). However, the development of more rapid methods on a ‘real time basis’ for microbiological quality control has been in the interest of scientists ever since routine microbiological analysis was applied to foods. Rapid detection methods based on the detection of whole cells or their metabolites can be divided into two main classes: direct methods are based on the detection of cells with or without incubation and indirect methods are based on the measurement of metabolic products or other changes caused by the cell growth ( Vanne et al., 1996 ). Although rapid detection methods have been under development, conventional methods for microbial monitoring are used on the job site of slaughterhouse. However, such methods usually require operator’s skill, long analysis time and high expenses. Moreover, visual inspection is not always accurate, the swabbing requires skilled worker and further plate count analysis which usually requires 24–48 h. The conventional microbiological approach to food sampling has changed over the last half century and it has been estimated that there are currently in excess of 40 methods to measure and detect bacterial spoilage in meats ( Jay, 2005; Nychas et al., 1988 ). The development of rapid microbiological test procedures over the last two decades can be divided into two main groups: enumeration and presence–absence tests. Several commercial presence–absence (P-A) test kits area available and were evaluated over a 6-month period in 1990 by using the Ontario Ministry of the Environment P-A test for comparison by Clark and El-Shaarawi (1993) . Current rapid enumeration methods are generally based on microscopy, ATP bioluminescence or the measurement of electrical phenomena ( Ellis and Goodacre, 2001 ). The use of ATP bioluminescence assay is a logical approach and relies on the fact that all living cells contain adenosine 5’-triphosphate (ATP), which is a universal energy donor for metabolism ( Bautista et al., 1997 ). Detection of the high-energy molecule adenosine triphosphate (ATP) extracted from cells is a widely used indirect assay method. The ATP amount is measured as the light energy released by the luciferin–luciferase system in the presence of magnesium ions ( Stanley, 1989 ). The assay is rapid, only a few seconds in hygiene monitoring applications and less than an hour of most other samples. Previously, it was thought that this technology has limitations – because of the fact that ATP is present in all viable cells. Therefore, intrinsic ATP originating from the target cells must be removed enzymatically before the assay ( Vanne et al., 1996 ). Siragusa et al. (1996) stated that the major challenge in using microbial ATP as a means of determining total microbial populations in food samples is the separation of nonmicrobial ATP from microbial ATP. The basis of their described Rapid-microbial ATP assay was the use of a filtration device in which somatic ATP was extracted: then within the same device, extraction of bacterial ATP was followed by its quantification. In the case of microscopic methods sophisticated techniques have been developed where microorganisms are stained with fluorescent dyes and viewed with an epifluorescent microscope. ATP bioluminescence acts by measuring ATP levels in bacterial cells in culture in order to calculate the number of cells present in that culture ( Champiat et al., 2001; de Boer and Beurmer, 1999; D’Souza, 2001; Siragusa et al., 1996 ). The problem with this method is that ATP is the primary energy source of all living cells and the food samples themselves will also contain large amounts of this chemical which have to be destroyed before microbial ATP can be measured. Consequently, the measurement of ATP bioluminescence is probably the best suited to detection of contaminated surfaces on equipment and machinery associated with food production and preparation ( Ellis and Goodacre, 2001 ). In the case that the viable cells should only be detected, the above-mentioned limitation of ATP bioluminescence technology and drawback of microscopic method have to be taken into account. However, the total amount of ATP originating from both meat and viable cell has sufficient importance in the cleanliness evaluation, because the ATP of meat origin acts as a nutrient source for the bacteria leading to bacterial spoilage. Due to the advantages of nondestructive, free of chemical preparation and fast inspection speed, spectroscopy has been studied extensively for determining properties of agricultural products, but less for meat products as compared to plant materials ( Chan et al., 2002 ).According to the literature, VIS/NIRS technology has been used in pork to determine intramuscular fat ( Hoving-Bolink et al., 2005; Savenije et al., 2006 ), fatty acid composition ( Fernandez-Cabanas et al., 2007; Gonzalez-Martin et al., 2003, 2005 ), color ( Cozzoliono et al., 2003 ), water-holding capacity ( Brondum et al., 2000 ), presence of RN − genetic allele ( Josell et al., 2000 ), and Doroc and Iberian porl neural network classification ( del Moral et al., 2009 ), but it has not been applied for the direct qualitative classification of meats of varied quality and price ( del Moral et al., 2009 ). Moreover, only a few reports are available for determination of quality of food products by using reflectance data. From these current conditions, the objective of this study was to develop a real-time detection method for monitoring of ATP and viable cells on meat surface by using reflectance spectra that could be used for sanitation management. 2 Materials and methods 2.1 Meat samples The lean part of pork loin samples sliced in 5-mm thick was obtained from a retailer. It was slaughtered 3 days ago and kept in the marketing conditions at the retailer shop. A total of 24-sliced samples were cut into pieces of about 6 × 6 cm 2 , and were individually placed in sterilized Petri dishes. 2.2 Experimental setup The samples were separated into six groups with four samples of each and were stored in a constant temperature chamber at 15 °C. The storage temperature was selected as the highest temperature in a working room of a slaughterhouse, where the temperature is usually controlled from 10 to 15 °C in consideration of worker’s health, according to our conversation with the slaughterhouse management. Measurements were conducted after 0, 24, 48, 72, 84 and 96 h of storage. The each value shown is a mean of four pieces. The experiment was repeated thrice to validate the results. Similar results were obtained in all the repeated experiments. Here, for simplicity, results of only one experiment are shown. 2.3 UV–Vis reflectance spectrum A dual beam spectrometer (UV-3600, Shimadzu Co., Kyoto, Japan) equipped with an integrating sphere setup was used for recording reflectance spectrum from a surface of meat sample (9 × 20 mm 2 ). Measured range of wave length was 240–1200 nm with the resolution of 2 nm; however, the results from 240 to 540 nm are only shown in the Section 3 . In order to confirm the maximum absorption wavelength of ATP, the transmittance of serial dilutions of ATP standard solution (LL-100-1, TOYO B-Net Co., Tokyo, Japan) was obtained with 10 mm quartz cells. 2.4 Spectral data pre-treatment Spectral data are often pre-processed to reduce undesirable systemtic noise, such as baseline variation, light scattering, path length differences and so on, and enhance the contribution of the chemical composition (Tigabu and Oden, 2002). In this study, two types of pre-processing were employed: Savitzky–Golay 1st and 2nd derivative. In our case, the possible sources of systematic variation could be due to the path length slight difference arising from the positioning of individual meat samples with slight different sizes during scanning. 2.5 Sampling protocol and microbiological analysis 2.5.1 Sampling protocol Sampling of materials on pork meat surface (40 × 40 mm 2 ) covering the area for spectroscopic measurement was carried out using a swab technique. To ensure adequate sampling, the sample was swabbed in a horizontal pattern and again in a vertical pattern being rotated between the index finger and the thumb in a back and forth motion according to Bautista et al. (1997) . The end of cotton bud used for swabbing was cut into 9 ml of sterilized water and then the swab sample was stirred well for the further examination for plate count and ATP determination. 2.5.2 Plate count Serial dilutions of the swab sample were prepared from the phosphate buffer solution in which the swab was immersed and 1 ml of the dilution was dispensed onto Petrifilms™ (AC plate, Sumitomo 3M Ltd., Tokyo, Japan) for total aerobic counts. The Petrifilms™ were incubated for 48 h at 35 °C. 2.5.3 ATP bioluminescence assay One hundred microliters of the swab sample (phosphate buffer solution in which the swab was immersed) was injected into a fresh cuvette placed in a luminometer (Luminescenser MCA, Atto Corporation, Tokyo, Japan), and then, 100 μl of Extractant (LL-100-2, Toyo B-Net Co. Ltd., Tokyo, Japan) was added into it. After 10 s, 100 μl of Luciferin–luciferase complex (LL-100-1, Toyo B-Net Co. Ltd., Tokyo, Japan) was added, and the light output was measured. From each swab, two measurements were taken and means were calculated to determine relative light units (RLU). The RLU was then converted into the amount of ATP by a standard curve constructed with ATP standard solution (LL-100-1, Toyo B-Net Co. Ltd., Tokyo, Japan) in the range of 10 −16 –10 −11 mole/100 microliters. 2.6 Statistical analysis The samples of four pieces of pork meat were selected at random for the storage time period. The regression analysis was carried out to know the relationship between ATP contents and plate count. The raw data had background information; therefore, it was converted by using the 1st and the 2nd derivatives, and the best one was selected. 3 Results 3.1 Plate count The plate count on the sample meat surface increased with the storage time period. At the outset of the experiment, the initial count was 29 CFU/cm 2 and 84 h after storage it was 3.2 × 10 7 CFU/cm 2 . 3.2 ATP content The amount of ATP increased with storage time period from the initial amount of 9.2 × 10 −5 to 2.8 × 10 −10 mol/cm 2 (84 h after storage). A linear relationship was observed between the amount of ATP and the plate count with the determination coefficient (R 2 ) of 0.95 as shown in Fig. 1 . 3.3 Absorption maximum of pure ATP The transmittance of ATP solutions of different concentration from 1 × 10 −4 to 5.85 × 10 −6 M are shown in Fig. 2 . It shows that the transmittance decreased with an increase in the ATP concentration, and spectra taken for all samples of different ATP concentrations showed that the maximum absorbance related to the decrease in transmittance was at 260 nm ( Fig. 2 ). 3.4 Estimation of ATP and plate count from reflectance The reflectance spectra obtained at 0–84 h of storage are shown in Fig. 3 in the UV–Vis range (from 240 to 540 nm). There was a very little difference between the reflectance at 0 h and that at 24 h. The reflectance of samples taken at 48, 72 and 84 h, however, showed a decreasing trend with increase in the storage time period. The 2nd derivative of reflectance data selected as the best between 1st and 2nd derivatives of reflectance is shown in Fig. 4 . Many upward and downward peaks were observed and the analysis of correlation of peaks at 298, 318, 344 and 374 nm in UV range was conducted. Fig. 5 shows the correlation coefficient between the 2nd derivative of reflectance and log (ATP). This gave a high correlation between the values of the 2nd derivative and log (ATP). Considering bathochromic shift, any of these four wave lengths could be taken as the maximum absorption of ATP. 4 Discussion Spectroscopic methods have gained importance in the evaluation of food quality attributes during the last decades (Nadai, 1983; Nadai and Mihalyi-Kengyel, 1984). Although NIR spectra reflect several parameters relating to complex quality of food (Williams and Norris, 2001), the information on ATP and/or microorganisms can not be detected in the range of NIR. Therefore, in the present study, UV–Vis was applied ranging from 240 to 540 nm. 4.1 Plate count In this study, samples were evaluated as fresh until that time when bacterial counts crossed the boundary line of 107 CFU/g and no putrid odor could be perceived. After 72 h, the plate count reached the order of 107 CFU/g and samples gave off a faint putrid odor. These samples were in the initial stage of spoilage and would be regarded as unacceptable. Plate count is a fundamental index of meat spoilage, and count of 10 7 CFU/g in meat is regarded as unacceptable ( Brown, 1982 ). Detection of the order of 10 6 CFU/g is important as this is achieved just before the meat reaches the unacceptable stage. Fresh meats generally have a pH range between 5.5 and 5.9 and contain sufficient glucose and other simple carbohydrates to support approximately 10 9 CFU/cm 2 . The organisms that grow the fastest and utilize glucose at refrigeration temperatures are the pseudomonas ( Gill and Newton, 1977; Jay, 2005; Seymour et al., 1994 ). At levels of 10 7 CFU/cm 2 off-odors may become evident in the form of a faint ‘dairy’ type aroma and once the surface population of bacteria has reached 10 8 CFU/cm 2 the supply of simple carbohydrates has been exhausted and recognizable off-odors develop leading to what is known as ‘sensory’ spoilage ( Jackson et al., 1997; Jay, 2005; Stanbridge and Davies, 1998 ). The development of off-odors is dependent upon the extent to which free amino acid utilization has occurred and these odors have been variously described as dairy/buttery/fatty/cheesy at 10 7 CFU/cm 2 through to a sickly sweet/fruity aroma at 10 8 CFU/cm 2 and finally putrid odor at 10 9 CFU/cm 2 ( Adams and Moss, 2007; Dainty et al., 1985 ). 4.2 ATP content Fig. 1 shows the linear relationship between log 10 ATP and log 10 plate count. From this figure both the ATP analysis and the plate count methods were able to assess the hygienence of pork meat samples. The ATP analysis provides only an estimation of the total bacterial count, and cannot differentiate between bacteria ( Baumgart, 1993 ). Theoretically, ATP amounts as low as 100 fg (10 −13 g) can be measured, corresponding to about 100 bacterial cells. Under practical conditions the sensitivity is about 1000 fg (10 −12 g), which corresponds to about 1000 bacterial cells or one to two yeast cells ( Heeschen et al., 1991 ). Stressed cells and cells in the stationary growth phase contain less ATP, which also affects the results ( Bulte and Reuter, 1985 ). On the other hand, however, the amount of ATP in a sample provides an estimate of the active microbial population, which is important when considering the shelf life of the product. Stressed cells can also be allowed to resuscitate before the ATP assay ( Graumlich, 1985 ). The enzyme, luciferase, converts the chemical energy provided by ATP into light by a stochiometric reaction. Thus, the amount of light produced is proportional to the concentration of ATP present, which in turn, is directly related to the number of cells in the sample ( Bautista et al., 1997 ). ATP bioluminescence is also useful for monitoring microbial contamination in scalding and chilling tanks within a meat processing operation. In the ATP bioluminescence assays for carcass contamination and process water quality, microbial cells are removed by filtration before they are lysed to release intracellular ATP. To simplify the method, it would be desirable if the step could be eliminated to allow direct detection of ATP on swabs of the carcass surface, in much the same way as for the ATP bioluminescence hygiene monitoring tests ( Griffiths, 1996 ). However, there would be no way of differentiating ATP from microbial and non-microbial sources using a swab assay, but results would be obtained within 2 min, as opposed to the 10–15 min required when a filtration step is incorporated ( Bautista et al., 1997 ). Siragusa et al. (1996) developed segmented-model statistical approach to determine the lower limits of assay sensitivity and by using this model analyzed in-plant data. According to them, the rapid microbial-ATP test responded in a linear fashion to levels of microbial contamination of >log 10 3.2 aerobic CFU/cm 2 for pork carcasses. 4.3 Absorption maximum of pure ATP As shown in Fig. 2 , the transmittance decreased with an increase in ATP concentration, and different spectra showed the minimum absorbance at 260 nm. The wave length of 260 nm was in accordance with the maximum absorbance of ATP (259 nm) as previously reported by Bagshaw (2001) . 4.4 Estimation of ATP and plate count from reflectance Reflectance ( Fig. 3 ) showed a decreasing trend with time in the UV–Vis range, although there was a very little difference between the reflectance at 0 h and that at 24 h. To remove the background effect the raw data was transformed by using the 1st and the 2nd derivates. However, the 2nd derivative was chosen because the effect was more clearer in it. The 2nd derivative technique is often used to process NIR data. It helps to separate overlapping absorption bands, remove baseline shifts and increase apparent spectral resolution (Lin et al., 2004), although the derivatives are notoriously sensitive to noise (Tsai and Philpot, 1998). Many upward and downward peaks were observed when the 2nd derivative of raw reflectance spectra for all storage time periods (from 0 to 96 h) was taken ( Fig. 4 ). The analysis of correlation of peaks at 298, 318, 344 and 374 nm was conducted. These selected wavelengths were in the UV range, i.e., less than 400 nm. The greatest differences were obtained for all selected wavelengths, between time 0 and 96 h. The maximum differences between time 0 and 96 h were in the range of 318 nm. This wavelength range could mainly differentiate between samples at 0 and 96 h. Fig. 5 shows the correlation coefficient between the 2nd derivative of reflectance and log (ATP). This gave a high correlation between the value of the 2nd derivative and log (ATP). Considering bathochromic shift, any of these four wave lengths could be taken as the maximum absorption of ATP. On the other hand, it is widely known that the spectral absorption by ATP is usually masked by protein absorbance and cannot be exploited in spectroscopic studies ( Bagshaw, 2001 ). However, the graph of correlation coefficient between the 2nd derivative of reflectance and log (plate count) shown in Fig. 6 became very similar in shape to Fig. 5 . This indicated that the 2nd derivative of reflectance involved the information of ATP in viable cells. The understanding of this is also supported by the result that the amount of ATP corresponded to the plate count ( Fig. 1 ). From these considerations, the wave length of 318 nm showing the highest correlation coefficient was selected. The linear relationship between the value of the 2nd derivative and log (ATP) for the first 48 h at 318 nm is shown in Fig. 7 with the determination coefficient of 0.89. The similar relationship was also observed between the value of the 2nd derivative and log (plate counts) for the first 48 h at 318 nm with the determination coefficient of 0.83. The duration of the first 48 h chosen here means that pork meat samples were fresh. From these results, it is expected that the selection of appropriate wave length could give the real-time monitoring of ATP and/or viable cell count on meat surface by the use of reflectance information. The plate count gives an estimate of microbial contamination whereas the ATP bioluminescence method used in this study measures total ATP, from both microbial and non-microbial sources, and may be a better measure of the overall cleanliness of the carcass. Therefore, an exact relationship between the two methods should not be expected and results obtained from the two assay systems should be interpreted separately. Multiple linear regression analysis using more than one reflectance at different wave length is a powerful tool in estimating ATP and/or viable cell count on meat surface, and can lead to higher predictive power. However, such paradigm may lead to overfitting. Accordingly, in this study, only one wave length (i.e., 318 nm) was selected for the prediction of ATP. 5 Conclusions A real-time detection method for monitoring of ATP and viable cells on meat surface by using reflectance spectra was developed. The data showed that the plate count on the sample meat surface increased and it corresponded exactly to the increase in the amount of ATP during 84 h storage at 15 °C. The linear relationship between the amount of ATP and plate count was supported by its determination coefficient of 0.95. Reflectance showed a decreasing trend with time in UV–Vis range and at the peak of 318 nm, 2nd derivative of reflectance gave a high correlation with log (ATP). As a similar high correlation was also observed between the 2nd derivative of reflectance and log (plate count), it is suggested that the 2nd derivative of reflectance involved the information of ATP in viable cells. From these observations, a linear relationship was given for the estimation of the amount of microbialy-derived ATP on the basis of reflectance analysis of meat surface. Hence, the developed technique can give a powerful way for monitoring of cleanness at a slaughterhouse. Acknowledgements This research was partly funded by the Japan Society for the Promotion of Science (JSPS) Grant No. 19:07178 . References Adams and Moss, 2007 Adams, M.R., Moss, M.O., 2007. Food Microbiology, third ed. The Royal Society of Chemistry, Cambridge, pp. 138. Bagshaw, 2001 C.R. Bagshaw ATP analogues at a glance Journal of Cell Science 114 2001 459 460 Baumgart, 1993 J. Baumgart Lebensmitteluberwachung und–qualitatssicherung Mikrobiologisch- hygienische Schnellverfahren Fleischwirtschaft 73 1993 292 396 Bautista et al., 1997 D.A. Bautista D.W. Sprung S. Barbut M.W. Griffiths A sampling regime based on an ATP bioluminescence assay to assess the quality of poultry carcasses at critical control points during processing Food Research International 30 1997 803 809 Brondum et al., 2000 J. Brondum L. Munck P. Henckel A. Karlsson E. Tornberg S.B. Engelsen Prediction and water-holding capacity and composition of porcine meat by comparative spectroscopy Meat Science 55 2000 177 185 Brown, 1982 Brown, M. H. (1982). Meat microbiology. (p. 410). New York: Applied Science Publications. Bulte and Reuter, 1985 M. Bulte G. Reuter The bioluminescence as a rapid method for the determination of the microflora of meat International Journal of Food Microbiology 2 1985 371 381 Champiat et al., 2001 D. Champiat N. Matas B. Monofort H. Fraass Applications of bioluminescence to HACCP Luminescence 16 2001 193 198 Chan et al., 2002 D.E. Chan P.N. Walker E.W. Mills Prediction of pork quality characteristics using visible and NIR spectroscopy Transactions of ASAE 45 2002 1519 1527 Clark and El-Shaarawi, 1993 J.A. Clark A.H. El-Shaarawi Evaluation of commercial presence-absence test kits for detection of total coliforms, Escherichia coli, and other indicator bacteria Applied and Environmental Microbiology 59 2 1993 380 388 Cozzoliono et al., 2003 D. Cozzoliono N. Barlocco A. Vadell F. Ballesteros G. Gallieta The use of visible and near-infrared reflectance spectroscopy to predict colour on both intact and homogenized pork muscle LWT-Food Science and Technology 36 2003 195 202 de Boer and Beurmer, 1999 E. de Boer R.R. Beurmer Methodology for detection and typing of food borne microorganisms International Journal of Food Microbiology 50 1999 119 130 del Moral et al., 2009 F.G. del Moral A. Guillen K.G. del Moral F. O’Valle L. Martinez R.G. del Moral Duroc and Iberian porl neural network classification by visible and near infrared reflectance spectroscopy Journal of Food Engineering 90 2009 540 547 Dainty et al., 1985 R.H. Dainty R.A. Edwards C.M. Hibbard Time course of volatile compound formation during refrigerated storage of naturally contaminated beef in air Journal of Applied Bacteriology 59 1985 303 309 D’Souza, 2001 S.F. D’Souza Microbial biosensors Biosensors and Bioelectronics 16 2001 337 353 Ellis and Goodacre, 2001 D.I. Ellis R. Goodacre Rapid and quantitative detection of the microbial spoilage of muscle foods: Current status and future trends Trends in Food Science & Technology. 12 2001 414 424 Fernandez-Cabanas et al., 2007 V.M. Fernandez-Cabanas A. Garrido-Varo J. Gracia-Olmo E. De Pedro P. Dardenne Optimisation of the spectral pre-treatments used for Iberian pig fat NIR calibration Chemometrics and Intelligent Laboratory System 87 2007 104 112 Fox, 2001 S. Fox WHO to convene on worldwide risk of BSE and CJD Infections in Medicine. 18 2001 69 Gonzalez-Martin et al., 2003 I. Gonzalez-Martin C. Gonzalez-Perez J. Hernandez-Menderz N. Alvarez-Gracia Determination of fatty acids in the subcutaneous fat of Iberian breed swine by near infrared spectroscopy (NIRS) with a fiber-optic probe Meat Science 65 2003 713 719 Gonzalez-Martin et al., 2005 I. Gonzalez-Martin C. Gonzalez-Perez N. Alvarez-Gracia J.M. Gonzalez-Cabrera On-line determination of fatty acids composition in intramuscular fat of Iberian pork loin by NIRS with a remote reflectance fibre optic probe Meat Science 65 2005 713 719 Gill and Newton, 1977 C.O. Gill K.G. Newton The development of aerobic spoilage flora on meat stored at chill temperatures Journal of Applied Bacteriology 43 1977 189 195 Graumlich, 1985 T.R. Graumlich Estimation of microbial populations in orange juice by bioluminescence Journal of Food Science 50 1985 116 117, 124 Griffiths, 1996 M.W. Griffiths The role of ATP bioluminescence in the food industry: new light on old problems Food Technology 50 6 1996 62 72 Hawronskyj and Holah, 1997 J.M. Hawronskyj J. Holah ATP: A universal hygiene monitor Trends in Food Science & Technology 8 1997 79 84 Heeschen et al., 1991 W.H. Heeschen G. Suhren G. Hahn Rapid methods in the dairy industry A. Vaheri R.C. Tilton A. Balows Rapid methods and Automation in Microbiology and Immunology 1991 Springer-Verlag Berlin Heidelberg 520 532 Hoving-Bolink et al., 2005 A.H. Hoving-Bolink H.W. Vedder J.W.M. Merks W.J.H. de Klein H.G.M. Reimert R. Frankhuizen W.H.A.M. van den Broek enE. Lambooji Perspective of NIRS measurements early post mortem for prediction of pork quality Meat Science 69 2005 417 423 Jackson et al., 1997 T.C. Jackson G.R. Acuff J.S. Dickson Meat, poultry, and seafood M.P. Doyle L.R. Beuchat T.J. Montville Food microbiology: fundamentals and frontiers 1997 ASM Press Washington DC 83 100 Jay, 2005 J.M. Jay Modern food microbiology sixth ed. 2005 Aspen Publishers Maryland Josell et al., 2000 A. Josell L. Martinsson C. Borggaard J.R. Anderson E. Tornberg Determination of RN - phenotype in pigs at slaughter-line using visual and near-infrared spectroscopy Meat Science 55 2000 273 278 Nychas et al., 1988 G.J. Nychas V.M. Dillon R.G. Board Glucose, the key substrate in the microbiological changes occurring in meat and certain meat products Biotechnology and applied biochemistry 10 1988 203 231 Pickrell and Enserink, 2001 J. Pickrell M. Enserink Foot-and-mouth disease – UK outbreak is latest in global epidemic Science 291 2001 1677 Savenije et al., 2006 B. Savenije G.H. Geesink J.G.P. van der Palen G. Hemke Prediction of pork quality using visible/near infrared reflectance spectroscopy Meat Science 73 2006 181 184 Seymour et al., 1994 I.J. Seymour M.B. Cole P.J. Coote A substrate-mediated assay of bacterial proton efflux/influx to predict the degree of spoilage of beef mince stored at chill temperatures Journal of Applied Bacteriology 76 1994 608 615 Siragusa et al., 1996 G.R. Siragusa W.J. Dorsa C.N. Cutter Perino L.J. Kooh-maraie Use of a newly developed rapid microbial ATP bioluminescence assay to detect microbial contamination on poultry carcasses Journal of Bioluminescence and Chemilumoscence 11 1996 297 301 Stanbridge and Davies, 1998 L.H. Stanbridge A.R. Davies The microbiology of chill-stored meat Davies R. Board The microbiology of meat and poultry 1998 Blackie Academic & Professional London 174 219 Stanley, 1989 P.E. Stanley A review of bioluminescent ATP techniques in rapid microbiology Journal of Bioluminescence and Chemiluminescence 4 1989 375 380 Vanne et al., 1996 L. Vanne M. Karwoski S. Karppinen A.M. Sjoberg HACCP-based food quality control and rapid detection methods for microorganisms Food Control 7 1996 263 276|建立了一种紫外-可见光谱实时检测肉表面 ATP 和活细胞的方法。ATP 含量与平板计数呈线性关系，测定系数为0.95。反射光谱的二阶导数与前48h 的 ATP 含量和318nm 的活细胞计数均有较高的相关性。摘要屠宰场的清洁度监测依赖于传统的方法，例如目视检查或擦拭。目视检查并不总是准确的。采样需要熟练的工人和进一步的平板计数或 ATP 生物发光技术。为了解决这些问题，开发了一种基于无损紫外-可见光反射的快速检测技术来监测 ATP 和活细胞。样本为猪腰瘦肉。分别在0、24、48、72、84和96h 分析15 °C 保存的样品的 ATP、平板计数和紫外-可见光反射率。测定了样品在20 °C 下240 ~ 540nm 范围内的反射光谱，然后采集样品表面40 × 40mm2的面积，测定平板计数和 ATP 含量。84h 后，样品表面的平板计数由最初的29个增加到3.2 × 107CFU/cm2。84h 后，ATP 含量从最初的9.2 × 10 ~ (-15) mol/cm2增加到2.8 × 10 ~ (-10) mol/cm2。ATP 含量与平板计数呈线性关系，测定系数为0.95。原始光谱的二阶导数与 ATP 含量和活细胞计数在318nm 处的相关系数分别为0.89和0.83。实验结果表明，紫外-可见光反射光谱分析可以用于实时监测肉品表面 ATP 和/或平板计数，并且具有最佳波长。关键词 ATP 卫生实时监测无损检测猪肉光谱板计数吸光度反射率肉类质量1简介包括肉类和家禽的肌肉食品是人类饮食的一个组成部分，并已经存在了几千年。然而，在过去的二十年中，由于诸如疯牛病和口蹄疫等引人注目的食品安全问题(Fox，2001; Pickrell and Enserink，2001) ，公众的关注和意识已经提高。这些疾病的爆发，以及对肉类中特定病原菌的担忧，说明了在这个大规模生产行业中，对肉类微生物腐败的快速和准确检测系统的要求，这个行业每年的营业额达数十亿美元(Ellis and Goodacre，2001)。微生物在食物变质中的主要作用以及食物作为传播导致食源性疾病的微生物的媒介的作用已得到公认。在家禽、猪肉和牛肉的屠宰场，清洁度的监测主要依赖于传统的目视检查、拭子和随后的活细胞计数或 ATP 生物发光技术(Hawronskyj and Holah，1997)。这对于与食物加工过程有关的微生物危害尤其重要。就家禽、猪肉和牛肉加工而言，可以通过使用培养方法(即经典的标准盘计数法)进行常规屠体分析，来验证预防措施减少或消除微生物危害的有效性(Bautista et al。 ，1997)。然而，自从常规微生物分析应用于食品以来，科学家一直对发展更快速的“实时”微生物质量控制方法感兴趣。基于检测整个细胞或其代谢物的快速检测方法可以分为两大类: 直接方法基于检测有或没有孵育的细胞，间接方法基于测量代谢产物或由细胞生长引起的其他变化(Vanne 等，1996)。虽然快速检测方法正在发展中，传统的微生物监测方法在屠宰场的工作现场使用。然而，这些方法往往需要操作人员的技能，分析时间长，费用高。此外，目测检查并不总是准确的，擦拭需要熟练的工人和进一步的板计数分析，通常需要24-48小时。在过去的半个世纪中，传统的食品采样微生物学方法已经发生了变化，据估计，目前有超过40种方法来测量和检测肉类中的细菌变质(Jay，2005; Nychas 等，1988)。近二十年来微生物快速检测技术的发展可分为两大类: 计数检测和存在-缺失检测。1990年，通过使用安大略省环境部的 P-A 测试，Clark 和 El-Shaarawi (1993)比较了几种商业存在缺失(P-A)测试试剂盒区域，并在6个月内进行了评估。目前的快速计数方法一般基于显微镜、 ATP 生物发光或电现象的测量(Ellis and Goodacre，2001)。ATP 生物发光测定的使用是一种合乎逻辑的方法，并且依赖于所有活细胞都含有腺苷5’-三磷酸(ATP)的事实，ATP 是代谢的通用能量供体(Bautista 等，1997)。检测从细胞中提取的高能分子三磷酸腺苷(ATP)是一种广泛使用的间接测定方法。三磷酸腺苷(ATP)量是以镁离子存在下荧光素-荧光素酶系统释放的光能量来测量的(Stanley，1989)。该分析是快速的，只有几秒钟的卫生监测应用和不到一个小时的大多数其他样品。以前，人们认为这种技术有局限性，因为 ATP 存在于所有活细胞中。因此，来自靶细胞的内源性 ATP 必须在测定前被酶去除(Vanne 等，1996)。Siragusa 等(1996)指出，使用微生物 ATP 作为确定食品样品中微生物总数的手段的主要挑战是从微生物 ATP 中分离非微生物 ATP。他们描述的快速微生物 ATP 测定的基础是使用过滤装置提取体细胞 ATP: 然后在同一装置内，提取细菌 ATP 后进行定量。在显微镜方法的情况下，复杂的技术已经开发出来，其中微生物被荧光染料染色，并用荧光显微镜观察。ATP 生物发光通过测量培养细菌细胞中的 ATP 水平来计算该培养物中存在的细胞数量(Champiat 等，2001; de Boer 和 Beurmer，1999; D’Souza，2001; Siragusa 等，1996)。这种方法的问题在于，ATP 是所有活细胞的主要能量来源，而且食品样本本身也含有大量这种化学物质，在测量微生物 ATP 之前，必须将其销毁。因此，ATP 生物发光的测量可能最适合于检测与食品生产和制备相关的设备和机械的污染表面(Ellis and Goodacre，2001)。在只检测活细胞的情况下，必须考虑 ATP 生物发光技术的上述局限性和显微技术的缺陷。然而，来源于肉类和活细胞的 ATP 总量在清洁度评估中具有足够的重要性，因为来源于肉类的 ATP 是导致细菌腐败的营养来源。由于无损、无化学制备和快速检测的优点，光谱学已被广泛研究用于确定农产品的性质，但与植物材料相比，肉类产品的性质较少(Chan et al。 ，2002)。根据文献，VIS/NIRS 技术已应用于猪肉中以测定肌间脂肪(Hoving-Bolink 等，2005; Savenije 等，2006) ，脂肪酸组成(Fernandez-Cabanas 等，2007; Gonzalez-Martin 等，2003,2005) ，颜色(Cozzoliono 等，2003) ，持水能力(Brondum 等，2000)(Josell et al。 ，2000) ，以及 Doroc 和伊比利亚 porl 神经网络分类(del Moral。 ，2009) ，但是它还没有被应用于不同质量和价格的肉类的直接定性分类(del Moral。 ，2009)。此外，利用反射率数据测定食品质量的报告屈指可数。从目前的情况来看，这项研究的目的是开发一种实时检测方法，以监测 ATP 和活细胞在肉表面的反射光谱，可用于卫生管理。2材料及方法2.1肉类样本2.1从零售商取得切成5毫米厚的猪腰样本的瘦肉部分。3天前被宰杀，在零售商店保存在市场条件下。共有24个切片的样品被切成约6 × 6cm2的块，并分别放置在消毒的培养皿中。2.2实验装置样品分为6组，每组4个样品，在15 °C 恒温箱中保存。根据我们与屠宰场管理人员的谈话，我们选择了屠宰场工作室的最高温度作为储存温度，考虑到工人的健康，通常将温度控制在10 °C 至15 °C 之间。在储存0,24,48,72,84和96小时后进行测量。显示的每个值是四个部分的平均值。实验重复三次以验证结果。在所有的重复实验中都得到了相似的结果。在这里，为了简单起见，只显示了一个实验的结果。2.3 UV-Vis 反射光谱日本京都岛津公司的 UV-3600双光束光谱仪配备了积分球装置，用于记录肉样表面(9 × 20mm2)的反射光谱。测量的波长范围为240-1200纳米，分辨率为2纳米; 然而，从240-540纳米的结果只显示在第3节。为了确定 ATP 的最大吸收波长，用10mm 石英电池测定了 ATP 标准溶液(LL-100-1，日本东京东洋 B-Net 公司)系列稀释液的透过率。2.4光谱数据预处理光谱数据经常被预处理，以减少不良的系统噪声，如基线变化、光散射、路径长度差异等，并增强化学成份的贡献(Tigabu 和 Oden，2002)。在这项研究中，两种类型的预处理采用: 萨维茨基-戈雷一阶和二阶导数。在我们的案例中，系统变化的可能来源可能是由于路径长度的微小差异产生的定位个别肉类样本与轻微不同的大小在扫描过程中。2.5采样方案和微生物分析2.5.1采样方案采用拭子技术对覆盖光谱测量区域的猪肉表面(40 × 40mm2)材料进行采样。为了确保足够的采样，根据 Bautista 等人(1997) ，以水平模式擦拭样品，并在食指和拇指之间来回旋转的垂直模式中再次擦拭样品。将棉签末端切入9ml 无菌水中，搅拌均匀，进一步检测棉签平板计数和 ATP 含量。2.5.2平板计数从浸入拭子的磷酸盐缓冲溶液中制备拭子样品的连续稀释液，并将1ml 稀释液分配到 Petri ilmsTM (AC 板，Sumitomo 3M Ltd. ，Tokyo，Japan)上以进行总有氧计数。在35 °C 下培养48小时。2.5.3 ATP 生物发光测定将100微升拭子样品(将拭子浸入其中的磷酸盐缓冲溶液)注射到置于发光计(Luminescenser MCA，Atto Corporation，Tokyo，Japan)中的新鲜试管中，然后将100μl 萃取剂(LL-100-2，Toyo B-Net Co. Ltd. ，Tokyo，Japan)加入其中。10秒后，加入100μl 萤光素-荧光素酶复合物(LL-100-1，日本东京东洋 B-Net Co. Ltd. ，Tokyo，Japan) ，并测量光输出。从每个拭子，采取两个测量和手段计算确定相对光单位(RLU)。然后通过用 ATP 标准溶液(LL-100-1，Toyo B-Net Co. Ltd. ，Tokyo，Japan)在10-16-10-11摩尔/100微升范围内构建的标准曲线将 RLU 转化为 ATP 的量。2.6统计分析在贮存期间，随机抽取四块猪肉样本作统计分析。研究人员进行回归分析测试，以了解三磷酸腺苷(ATP)含量与盘子数量的关系。由于原始数据具有背景信息，因此采用一阶导数和二阶导数对原始数据进行转换，从中选出最优的一个。3结果3.1平板计数样品肉表面平板计数随贮藏时间延长而增加。试验开始时，初始计数为29CFU/cm2，贮藏84h 后为3.2 × 107CFU/cm2。3.2 ATP 含量随着贮藏时间的延长，ATP 含量由最初的9.2 × 10-5增加到2.8 × 10-10mol/cm2(贮藏后84h)。ATP 含量与平板计数呈线性关系，测定系数(R2)为0.95，如图1所示。3.3纯 ATP 的最大吸收量不同浓度(1 × 10-4 ~ 5.85 × 10-6M)的 ATP 溶液的透过率如图2所示。它表明，透过率随着 ATP 浓度的增加而降低，并且对不同 ATP 浓度的所有样品进行的光谱显示与透过率降低相关的最大吸光度在260nm 处(图2)。3.4根据反射率估计 ATP 和平板计数储存0-84小时获得的反射光谱如图3所示，在 UV-Vis 范围内(从240到540nm)。0小时和24小时的反射率差别很小。在48、72和84小时采集的样品的反射率随着储存时间的延长呈下降趋势。反射率数据的二阶导数被选为反射率一阶导数和二阶导数之间的最佳值，如图4所示。在紫外光谱范围内，观察到多个向上和向下的峰，并对298,318,344和374nm 的峰进行了相关性分析。图5显示了反射率二阶导数与对数(ATP)之间的相关系数。这给出了二阶导数和对数(ATP)之间的高度相关性。考虑到暗色移动，这四个波长中的任何一个都可以作为 ATP 的最大吸收。4在过去的几十年中，光谱方法在食品质量属性的评价中获得了重要性(Nadai，1983; Nadai 和 Mihalyi-Kengyel，1984)。尽管近红外光谱反映了与食品复杂质量有关的几个参数(Williams 和 Norris，2001) ，但是在近红外光谱范围内不能检测到 ATP 和/或微生物的信息。因此，在本研究中，UV-Vis 的应用范围从240到540纳米。4.1平板计数在这项研究中，样品被评估为新鲜，直到当细菌计数超过107CFU/g 的边界线，没有腐臭气味可以察觉。72小时后，平板计数达到107 CFU/g，样品散发出微弱的腐臭气味。这些样品处于变质的初始阶段，将被视为不可接受。盘子计数是肉类腐败的一个基本指标，肉类中107CFU/g 的计数被认为是不可接受的(Brown，1982)。检测106 CFU/g 的量级很重要，因为这是在肉类达到不可接受阶段之前完成的。新鲜肉类的 pH 值一般在5.5至5.9之间，并含有足够的葡萄糖和其他简单碳水化合物，以支持大约109 CFU/cm2。在冷藏温度下生长最快并利用葡萄糖的生物是假单胞菌(Gill and Newton，1977; Jay，2005; Seymour et al。 ，1994)。在107 CFU/cm2的水平下，异味可能会以淡淡的“奶制品”香味的形式变得明显，一旦表面细菌数量达到108 CFU/cm2，简单碳水化合物的供应已经耗尽，可识别的异味发展导致所谓的“感官”腐败(Jackson 等，1997; Jay，2005; Stanbridge and Davies，1998)。异味的发展取决于游离氨基酸利用发生的程度，这些气味已经被不同地描述为107CFU/cm2的乳制品/黄油/脂肪/奶酪，直到108CFU/cm2的病态甜味/水果香味，最后是109CFU/cm2的腐臭气味(Adams 和 Moss，2007; Dainty 等，1985)。4.2 ATP 含量图1显示了对数10 ATP 和对数10平板计数之间的线性关系。根据这个数字，ATP 分析法和平板计数法都能够评估猪肉样本的卫生情况。ATP 分析只能提供总细菌计数的估计，不能区分细菌(Baumgart，1993)。理论上，ATP 含量低至100fg (10-13g) ，相当于约100个细菌细胞。在实际条件下，灵敏度约为1000fg (10-12g) ，相当于约1000个细菌细胞或一至两个酵母细胞(Heeschen 等，1991)。应激细胞和处于静止生长阶段的细胞含有较少的 ATP，这也影响了结果(Bulte 和 Reuter，1985)。然而，另一方面，样品中 ATP 的含量提供了对活性微生物种群的估计，这在考虑产品的货架期时是很重要的。应激细胞也可以在 ATP 测定前复苏(Graumlich，1985)。这种酶，萤光素酶，通过化学计量反应将 ATP 提供的化学能转化为光。因此，产生的光量与存在的 ATP 浓度成正比，而 ATP 浓度又与样品中细胞的数量直接相关(Bautista 等，1997)。ATP 生物发光也可用于监测肉类加工过程中烫伤和冷却罐中的微生物污染。在对屠体污染和处理水质的 ATP 生物发光测定中，微生物细胞在裂解释放细胞内 ATP 之前通过过滤除去。为了简化这种方法，如果能够消除这一步骤以允许在屠体表面的拭子上直接检测 ATP，就像 ATP 生物发光卫生监测测试(Griffiths，1996)一样是可取的。然而，使用拭子测定将无法区分 ATP 与微生物和非微生物来源，但是结果将在2分钟内获得，而不是当纳入过滤步骤时所需的10-15分钟(Bautista 等，1997)。Siragusa 等(1996)开发了分段模型统计方法来确定检测灵敏度的下限，并使用该模型分析植物内数据。根据他们的研究，快速微生物 ATP 测试以线性方式响应猪肉胴体 > log 103.2需氧 CFU/cm2的微生物污染水平。4.3纯 ATP 的吸收峰如图2所示，透过率随 ATP 浓度的增加而降低，不同的光谱在260nm 处表现出最小的吸光度。260nm 的波长与之前 Bagshaw (2001)报道的 ATP (259nm)的最大吸光度一致。4.4从反射反射率估计 ATP 和平板计数(图3)在 UV-Vis 范围内随着时间的推移显示出减少的趋势，尽管在0小时和24小时的反射率之间差异很小。为了消除背景效应原始数据通过使用第一和第二导数进行转换。然而，选择二阶导数是因为它的效果更加明显。二阶导数技术是近红外数据处理的常用方法。它有助于分离重叠吸收带，消除基线偏移和增加明显的光谱分辨率(Lin et al。 ，2004) ，尽管衍生物是众所周知的对噪声敏感(Tsai 和 Philpot，1998)。在所有贮存时间段(0-96小时)的原始反射光谱二阶导数测量时，观察到许多向上和向下的峰值(图4)。对298、318、344和374nm 波段的峰进行了相关性分析。这些选定的波长在紫外线范围内，即小于400纳米。最大的差异获得所有选定的波长，在时间0和96小时之间。时间0 ~ 96h 的最大差异在318nm 范围内。这个波长范围主要可以区分0和96小时的样品。图5显示了反射率的二阶导数与对数(ATP)之间的相关系数。这给出了二阶导数的值与对数(ATP)之间的高度相关性。考虑到暗色移动，这四个波长中的任何一个都可以作为 ATP 的最大吸收。另一方面，众所周知，ATP 的光谱吸收通常被蛋白质吸收所掩盖，在光谱学研究中不能被利用(Bagshaw，2001)。然而，图6所示的反射率二阶导数与对数(平板计数)之间的相关系数图在形状上与图5非常相似。这表明反射率的二阶导数涉及活细胞内 ATP 的信息。对这一点的理解也得到了 ATP 含量与平板计数相对应的结果的支持(图1)。从这些考虑，波长318纳米显示最高的相关系数被选中。在318nm 处的前48h，二阶导数的值与对数(ATP)之间的线性关系如图7所示，测定系数为0.89。在318nm 处测定前48h，二阶导数值与对数(平板计数)之间也存在相似的关系，测定系数为0.83。这里选择的前48小时的持续时间意味着猪肉样本是新鲜的。从这些结果可以看出，选择合适的波长可以利用反射率信息实时监测肉表面 ATP 和/或活细胞计数。平板计数给出了微生物污染的估计值，而本研究中使用的 ATP 生物发光法测量来自微生物和非微生物来源的总 ATP，可能是对屠体整体清洁度的更好测量。因此，不应期望两种方法之间有确切的关系，从两种测定系统获得的结果应该分开解释。在不同波长下使用多个反射率的多个线性回归分析是估计肉表面 ATP 和/或存活细胞数量的有力工具，并且可以导致更高的预测能力。然而，这样的范式可能会导致过度拟合。因此，在这项研究中，只有一个波长(即，318纳米)被选择用于 ATP 的预测。5结论建立了一种利用反射光谱实时监测肉表面 ATP 和活细胞的方法。结果表明，在15 °C 下贮藏84h，样品表面的平板数量增加，与 ATP 含量的增加完全一致。其测定系数为0.95，支持 ATP 含量与平板计数之间的线性关系。在紫外-可见光范围内，反射率随时间呈下降趋势，在318nm 的峰值，反射率的二阶导数与对数(ATP)呈高度相关。由于反射率的二阶导数与对数(平板计数)之间也存在相似的高相关性，提示反射率的二阶导数涉及活细胞中 ATP 的信息。根据这些观察结果，在肉表面反射率分析的基础上，给出了估算微生物源性 ATP 含量的线性关系。因此，开发的技术可以提供一个强有力的方式监测清洁度在屠宰场。这项研究部分由日本科学促进会(JSPS)拨款19:07178资助。参考文献 Adams and Moss 2007 Adams M.R. Moss 作案手法2007。食品微生物学。英国皇家化学学会，剑桥，第138页。巴格肖，2001年产。Bagshaw ATP 类似物一目了然细胞科学杂志1142001459460 Baumgart，1993 J. Baumgart Lebensmitteluberwachung und-qualitatssecherung Mikrobiologch-hygienische Schnellverfahren Fleischwirtschaft 731993292396 Bautista 等，1997 D.A。Bautista D.W.Sprung S. Barbut M.W.Griffiths 基于 ATP 生物发光测定的采样制度，用于评估食品研究国际组织在加工过程中关键控制点的家禽尸体的质量。301997803809 Brondum et al。 ，2000 J. Brondum L. Munck P. Henckel A. Karlsson E. Tornberg S.B。利用比较光谱法对猪肉的恩格尔森预测和持水性及其组成的研究[55]。肉类微生物学。(p. 410).纽约: 应用科学出版物。生物发光法作为一种快速测定肉类微生物区系的方法，国际食品微生物学杂志，1985年2月37日，381 Champiat 等人,2001 D. Champiat N. Matas B. Monofort H. Fraass 生物发光在 HACCP 发光中的应用162001193198 Chan 等，2002 D.E。陈 PN。Walker E.W.利用 ASAE 45200215191527 Clark 和 El-Shaarawi 的可见光和近红外光谱法预测猪肉品质特性。Clark A.H.用于检测总大肠菌群、大肠桿菌和其他指示细菌应用与环境微生物学的商业存在-缺失试剂盒的评估5921993380388 Cozzoliono 等人，2003 D. Cozzoliono N. Barlocco A. Vadell F. Ballesteros G. Gallieta 使用可见光和近红外反射光谱来预测完整和均化猪肉的颜色 LWT-Food Science and Technology 362003195202 de Boer and Beurmer，1999E.De Boer R.R.Beurmer 食源性微生物检测和分型方法国际食品微生物学杂志501999119130 del Moral 等，2009 F.G。德尔 · 道德 · A · 吉伦 · KG。马丁内斯 R.G。可见光和近红外反射光谱学对杜洛克和伊比利亚 Porl 神经网络的分类食品工程杂志902009540547 Dainty 等，1985 R.H。小巧的 R.A。爱德华兹 C.M。空气中自然污染牛肉冷藏过程中挥发性化合物形成的时间过程应用细菌学杂志591985303309 d’Souza，2001 S.F. 。D’Souza 微生物生物传感器生物传感器和生物电子学162001337353 Ellis and Goodacre，2001 D。肌肉食品微生物腐败的快速定量检测: 食品科学与技术的现状与未来趋势。122001414424 Fernandez-Cabanas 等，2007 V.M. Fernandez-Cabanas A. Garrido-Varo J. Gracia-Olmo E. De Pedro P. Dardenne 优化用于伊比利亚猪脂肪近红外校准的光谱预处理化学计量学和智能实验室系统872007104112 Fox，2001 S.Fox 世界卫生组织召开关于世界范围内疯牛病和 CJD 医学感染风险的会议。冈萨雷斯-马丁等人，2003 I. 冈萨雷斯-马丁冈萨雷斯-佩雷斯 · 埃尔南德斯 · 门德斯 · N · 阿尔瓦雷斯 · 格拉西亚用光纤探针近红外光谱学(NIRS)测定伊比利亚种猪皮下脂肪中的脂肪酸。肉类科学652003713719冈萨雷斯-马丁等人,2005年 I. Gonzalez-Martin C. Gonzalez-Perez N. Alvarez-Gracia J.M。冈萨雷斯-卡布雷拉利用远程反射光纤探针在线测定伊比利亚猪腰肌间脂肪脂肪酸组成。Gill K.G.牛顿冷藏肉类上有氧腐败菌群的发展应用细菌学杂志431977189195 Graumlich，1985 T.R。《食品科学生物发光杂志》1985116117,124 Griffiths，1996 M.W 对橙汁中微生物种群的 Graumlich 估算。三磷酸腺苷生物发光在食品工业中的作用: 对旧问题的新认识食品技术50619966272 Hawronskyj and Holah，1997 J.M。Hawronskyj J. Holah ATP: 食品科学与技术通用卫生监测器趋势819977984 Heeschen 等，1991 W.H。乳品工业中的快速方法。《微生物学和免疫学中的快速方法和自动化》 ，1991年，施普林格-出版社，Berlin Heidelberg 520532霍文-博林克等，2005年。霍温-博林克 H.W。Vedder J.W.M.Merks W.J.H.De Klein H.G.M.Reimert R. Frankhuizen W.H.A.M.范登布鲁克。返回文章页面兰布吉对近红外光谱仪测量结果的透视译者: pestwave 肉类科学692005417423杰克逊等人，1997年杰克逊 G.R. Acuff J.S. 迪克森肉类，家禽和海鲜 M.P. 道尔 L.R. 博伊查特 T.J. 蒙特维尔食品微生物学: 基础和前沿1997年美国广播公司出版社华盛顿 DC 83100杰伊，2005 J.M. 杰伊现代食品微生物学第六版。2005年 Aspen 出版社 Maryland Josell 等人，2000年 A. Josell L. Martinsson C. Borggaard J.R。利用视觉和近红外光谱技术肉类科学552000273278尼查斯等，1988 G.J。Nychas V.M.Dillon R.G.O </o < o </o < o </o < > < o </o < > < o </o < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o </o < > < o < o </o < > < o </o < > < o </o.Geesink J.G.P.利用可见/近红外反射光谱技术预测猪肉质量。Seymour M.B.Cole PJ.库特基质介导的细菌质子流出/内流的测定，以预测在寒冷温度下储存的牛肉肉糜的变质程度应用细菌学杂志761994608615 Siragusa 等，1996 G.R。天竺葵 W.J。Dorsa C.N.Cutter Perino L.J.Kooh-maraie 使用一种新开发的快速微生物 ATP 生物发光测定法来检测家禽尸体上的微生物污染。Stanbridge A.R.冷藏肉的微生物学 Davies R. Board 肉和家禽的微生物学1998 Blackie Academy & Professional London 174219 Stanley，1989 P.E。快速微生物学中的生物发光 ATP 技术综述《生物发光杂志》和化学发光4,1989375380。基于 Sjoberg HACCP 的食品质量控制和微生物快速检测方法食品控制71996263276|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large-Scale+Graph+Neural+Networks:+The+Past+and+New+Frontiers)|0|
|[Knowledge-augmented Graph Machine Learning for Drug Discovery: From Precision to Interpretability](https://doi.org/10.1145/3580305.3599563)|Zhiqiang Zhong, Davide Mottin||The integration of Artificial Intelligence (AI) into the field of drug discovery has been a growing area of interdisciplinary scientific research. However, conventional AI models are heavily limited in handling complex biomedical structures (such as 2D or 3D protein and molecule structures) and providing interpretations for outputs, which hinders their practical application. As of late, Graph Machine Learning (GML) has gained considerable attention for its exceptional ability to model graph-structured biomedical data and investigate their properties and functional relationships. Despite extensive efforts, GML methods still suffer from several deficiencies, such as the limited ability to handle supervision sparsity and provide interpretability in learning and inference processes, and their ineffectiveness in utilising relevant domain knowledge. In response, recent studies have proposed integrating external biomedical knowledge into the GML pipeline to realise more precise and interpretable drug discovery with limited training instances. However, a systematic definition for this burgeoning research direction is yet to be established. This survey presents a comprehensive overview of long-standing drug discovery principles, provides the foundational concepts and cutting-edge techniques for graph-structured data and knowledge databases, and formally summarises Knowledge-augmented Graph Machine Learning (KaGML) for drug discovery. we propose a thorough review of related KaGML works, collected following a carefully designed search methodology, and organise them into four categories following a novel-defined taxonomy. To facilitate research in this promptly emerging field, we also share collected practical resources that are valuable for intelligent drug discovery and provide an in-depth discussion of the potential avenues for future advancements.|人工智能(AI)与药物发现领域的结合已经成为跨学科科学研究的一个新兴领域。然而，传统的人工智能模型在处理复杂的生物医学结构(如2D 或3D 蛋白质和分子结构)和提供输出解释方面受到严重限制，这阻碍了它们的实际应用。近年来，图形机器学习(Graph Machine Learning，GML)由于其对图形结构化生物医学数据的建模能力以及对其性质和功能关系的研究而引起了人们的广泛关注。尽管付出了广泛的努力，GML 方法仍然存在一些缺陷，例如处理监督稀疏性和在学习和推理过程中提供可解释性的能力有限，以及它们在利用相关领域知识方面的无效性。作为回应，最近的研究提出将外部生物医学知识整合到 GML 管道中，以便在有限的培训实例下实现更精确和可解释的药物发现。然而，对这一新兴的研究方向的系统定义还有待确立。本调查全面概述了长期存在的药物发现原则，提供了图形结构数据和知识数据库的基本概念和尖端技术，并正式总结了用于药物发现的知识增强图形机器学习(KaGML)。我们建议对相关的 KaGML 作品进行一次彻底的回顾，这些作品是按照一个精心设计的搜索方法收集的，并按照一个新定义的分类法将它们组织成四个类别。为了促进这一迅速兴起的领域的研究，我们还分享了收集到的对智能药物发现有价值的实用资源，并对未来进展的潜在途径进行了深入讨论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge-augmented+Graph+Machine+Learning+for+Drug+Discovery:+From+Precision+to+Interpretability)|0|
|[Foundations and Applications in Large-scale AI Models: Pre-training, Fine-tuning, and Prompt-based Learning](https://doi.org/10.1145/3580305.3599209)|Derek Cheng, Dhaval Patel, Linsey Pang, Sameep Mehta, Kexin Xie, Ed H. Chi, Wei Liu, Nitesh V. Chawla, James Bailey||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Foundations+and+Applications+in+Large-scale+AI+Models:+Pre-training,+Fine-tuning,+and+Prompt-based+Learning)|0|
|[Minimizing Hitting Time between Disparate Groups with Shortcut Edges](https://doi.org/10.1145/3580305.3599434)|Florian Adriaens, Honglian Wang, Aristides Gionis|KTH Royal Institute of Technology; University of Helsinki|Structural bias or segregation of networks refers to situations where two or more disparate groups are present in the network, so that the groups are highly connected internally, but loosely connected to each other. In many cases it is of interest to increase the connectivity of disparate groups so as to, e.g., minimize social friction, or expose individuals to diverse viewpoints. A commonly-used mechanism for increasing the network connectivity is to add edge shortcuts between pairs of nodes. In many applications of interest, edge shortcuts typically translate to recommendations, e.g., what video to watch, or what news article to read next. The problem of reducing structural bias or segregation via edge shortcuts has recently been studied in the literature, and random walks have been an essential tool for modeling navigation and connectivity in the underlying networks. Existing methods, however, either do not offer approximation guarantees, or engineer the objective so that it satisfies certain desirable properties that simplify the optimization~task. In this paper we address the problem of adding a given number of shortcut edges in the network so as to directly minimize the average hitting time and the maximum hitting time between two disparate groups. Our algorithm for minimizing average hitting time is a greedy bicriteria that relies on supermodularity. In contrast, maximum hitting time is not supermodular. Despite, we develop an approximation algorithm for that objective as well, by leveraging connections with average hitting time and the asymmetric k-center problem.|网络的结构性偏差或隔离是指网络中存在两个或两个以上不同的群体，这些群体在内部高度连接，但彼此之间却松散地连接在一起。在许多情况下，增加不同群体之间的联系是有意义的，这样可以减少社会摩擦，或者使个人接触到不同的观点。增加网络连接性的一种常用机制是在节点对之间添加边快捷方式。在许多感兴趣的应用程序中，边缘快捷方式通常转化为推荐，例如，要看什么视频，或者接下来要读什么新闻文章。通过边缘捷径减少结构偏差或分离的问题最近在文献中进行了研究，并且随机游动已经成为一个必不可少的工具建模导航和连通性在基础网络。然而，现有的方法要么不提供近似保证，要么设计目标，使其满足某些理想的性质，简化了优化任务。本文讨论了在网络中增加一定数量的快捷边，以直接最小化两个不同组之间的平均命中时间和最大命中时间的问题。我们的最小化平均命中时间的算法是一个依赖于超模性的贪婪的双准则。相比之下，最大命中时间不是超模块化的。尽管如此，通过利用平均命中时间和非对称 k 中心问题之间的联系，我们还是为这个目标制定了一个近似演算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Minimizing+Hitting+Time+between+Disparate+Groups+with+Shortcut+Edges)|0|
|[Fair Allocation Over Time, with Applications to Content Moderation](https://doi.org/10.1145/3580305.3599340)|Amine Allouah, Christian Kroer, Xuan Zhang, Vashist Avadhanula, Nona Bohanon, Anil Dania, Caner Gocmen, Sergey Pupyrev, Parikshit Shah, Nicolás Stier Moses, Ken Rodríguez Taarup||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Allocation+Over+Time,+with+Applications+to+Content+Moderation)|0|
|[Maximizing Neutrality in News Ordering](https://doi.org/10.1145/3580305.3599425)|Rishi Advani, Paolo Papotti, Abolfazl Asudeh|EURECOM; University of Illinois Chicago|The detection of fake news has received increasing attention over the past few years, but there are more subtle ways of deceiving one's audience. In addition to the content of news stories, their presentation can also be made misleading or biased. In this work, we study the impact of the ordering of news stories on audience perception. We introduce the problems of detecting cherry-picked news orderings and maximizing neutrality in news orderings. We prove hardness results and present several algorithms for approximately solving these problems. Furthermore, we provide extensive experimental results and present evidence of potential cherry-picking in the real world.|在过去的几年里，假新闻的发现已经受到越来越多的关注，但是欺骗受众的方式更加微妙。除了新闻报道的内容之外，它们的表述也可能具有误导性或偏见性。在本研究中，我们研究了新闻故事的顺序对受众知觉的影响。我们介绍了检测精选新闻排序和最大化新闻排序中立性的问题。我们证明了硬度结果，并提出了几种近似求解这些问题的算法。此外，我们提供了广泛的实验结果和现实世界中潜在的樱桃采摘的证据。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Maximizing+Neutrality+in+News+Ordering)|0|
|[Knowledge Graph Reasoning over Entities and Numerical Values](https://doi.org/10.1145/3580305.3599399)|Jiaxin Bai, Chen Luo, Zheng Li, Qingyu Yin, Bing Yin, Yangqiu Song|Amazon.com Inc; HKUST|A complex logic query in a knowledge graph refers to a query expressed in logic form that conveys a complex meaning, such as where did the Canadian Turing award winner graduate from? Knowledge graph reasoning-based applications, such as dialogue systems and interactive search engines, rely on the ability to answer complex logic queries as a fundamental task. In most knowledge graphs, edges are typically used to either describe the relationships between entities or their associated attribute values. An attribute value can be in categorical or numerical format, such as dates, years, sizes, etc. However, existing complex query answering (CQA) methods simply treat numerical values in the same way as they treat entities. This can lead to difficulties in answering certain queries, such as which Australian Pulitzer award winner is born before 1927, and which drug is a pain reliever and has fewer side effects than Paracetamol. In this work, inspired by the recent advances in numerical encoding and knowledge graph reasoning, we propose numerical complex query answering. In this task, we introduce new numerical variables and operations to describe queries involving numerical attribute values. To address the difference between entities and numerical values, we also propose the framework of Number Reasoning Network (NRN) for alternatively encoding entities and numerical values into separate encoding structures. During the numerical encoding process, NRN employs a parameterized density function to encode the distribution of numerical values. During the entity encoding process, NRN uses established query encoding methods for the original CQA problem. Experimental results show that NRN consistently improves various query encoding methods on three different knowledge graphs and achieves state-of-the-art results.|知识图中的复杂逻辑查询是指以逻辑形式表达的、传达复杂意义的查询，比如加拿大图灵奖获得者是从哪里毕业的？基于知识图推理的应用程序，如对话系统和交互式搜索引擎，依赖于回答复杂逻辑查询的能力，这是一项基本任务。在大多数知识图中，边通常用于描述实体之间的关系或其相关属性值。属性值可以是分类格式或数字格式，如日期、年份、大小等。但是，现有的复杂查询应答(CQA)方法只是以处理实体的方式处理数值。这可能会导致回答某些问题的困难，例如哪位澳大利亚普利策奖得主出生于1927年以前，哪种药物是止痛药，副作用比扑热息痛少。本文受数字编码和知识图推理技术的启发，提出了数字复杂查询应答方法。在这个任务中，我们引入了新的数值变量和操作来描述涉及数值属性值的查询。为了解决实体和数值之间的差异，我们还提出了数字推理网络(NRN)的框架，将实体和数值交替编码成独立的编码结构。在数值编码过程中，NRN 采用参数化密度函数对数值的分布进行编码。在实体编码过程中，NRN 对原始的 CQA 问题使用已建立的查询编码方法。实验结果表明，NRN 能够一致地改进三种不同知识图上的各种查询编码方法，取得了较好的效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Reasoning+over+Entities+and+Numerical+Values)|0|
|[Communication Efficient and Differentially Private Logistic Regression under the Distributed Setting](https://doi.org/10.1145/3580305.3599279)|Ergute Bao, Dawei Gao, Xiaokui Xiao, Yaliang Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Communication+Efficient+and+Differentially+Private+Logistic+Regression+under+the+Distributed+Setting)|0|
|[Preemptive Detection of Fake Accounts on Social Networks via Multi-Class Preferential Attachment Classifiers](https://doi.org/10.1145/3580305.3599471)|Adam Breuer, Nazanin Khosravani Tehrani, Michael Tingley, Bradford Cottel|Dartmouth; Meta|In this paper, we describe a new algorithm called Preferential Attachment k-class Classifier (PreAttacK) for detecting fake accounts in a social network. Recently, several algorithms have obtained high accuracy on this problem. However, they have done so by relying on information about fake accounts' friendships or the content they share with others--the very things we seek to prevent.   PreAttacK represents a significant departure from these approaches. We provide some of the first detailed distributional analyses of how new fake (and real) accounts first attempt to request friends after joining a major network (Facebook). We show that even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth.   We use this model to derive a new algorithm, PreAttacK. We prove that in relevant problem instances, PreAttacK near-optimally approximates the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts' (not-yet-answered) friend requests. These are the first provable guarantees for fake account detection that apply to new users, and that do not require strong homophily assumptions.   This principled approach also makes PreAttacK the only algorithm with provable guarantees that obtains state-of-the-art performance on new users on the global Facebook network, where it converges to AUC=0.9 after new users send + receive a total of just 20 not-yet-answered friend requests. For comparison, state-of-the-art benchmarks do not obtain this AUC even after observing additional data on new users' first 100 friend requests. Thus, unlike mainstream algorithms, PreAttacK converges before the median new fake account has made a single friendship (accepted friend request) with a human.|本文提出了一种新的识别社交网络中虚假账户的算法——偏好依恋 k 类分类器(PreAttacK)。近年来，一些算法已经在这个问题上取得了很高的精度。然而，他们这样做的依据是关于虚假账户的友谊或与他人分享的内容的信息——这正是我们试图阻止的事情。PreAttacK 代表了对这些方法的重大背离。我们首先提供了一些详细的分布式分析，分析新的假账户(和真账户)如何在加入主流社交网络(Facebook)后首次尝试请求好友。我们发现，甚至在一个新账户交到朋友或分享内容之前，这些初始的好友请求行为就会自然地唤起社交网络成长的规范偏好依恋模型的多级扩展。我们使用这个模型来推导一个新的算法，PreAttacK。我们证明在相关的问题实例中，PreAttack 近似最优地近似了在这种新帐户的好友请求(尚未回复)的多类优先附件模型下，新帐户是假的后验概率。这些都是第一个可证明的假账户检测的保证，适用于新用户，并不需要强烈的同质假设。这种原则性的方法也使 PreAttacK 成为唯一一种可证实的算法，它可以在全球 Facebook 网络的新用户身上获得最先进的性能，在新用户发送 + 收到总共20个尚未回复的好友请求后，它会收敛到 AUC = 0.9。作为比较，即使在观察了新用户前100个好友请求的额外数据之后，最先进的基准测试也不能获得这个 AUC。因此，与主流算法不同的是，PreAttacK 会在新假账号与人类建立单个友谊(接受好友请求)之前收敛。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Preemptive+Detection+of+Fake+Accounts+on+Social+Networks+via+Multi-Class+Preferential+Attachment+Classifiers)|0|
|[On Improving the Cohesiveness of Graphs by Merging Nodes: Formulation, Analysis, and Algorithms](https://doi.org/10.1145/3580305.3599449)|Fanchen Bu, Kijung Shin|KAIST|Graphs are a powerful mathematical model, and they are used to represent real-world structures in various fields. In many applications, real-world structures with high connectivity and robustness are preferable. For enhancing the connectivity and robustness of graphs, two operations, adding edges and anchoring nodes, have been extensively studied. However, merging nodes, which is a realistic operation in many scenarios (e.g., bus station reorganization, multiple team formation), has been overlooked. In this work, we study the problem of improving graph cohesiveness by merging nodes. First, we formulate the problem mathematically using the size of the $k$-truss, for a given $k$, as the objective. Then, we prove the NP-hardness and non-modularity of the problem. After that, we develop BATMAN, a fast and effective algorithm for choosing sets of nodes to be merged, based on our theoretical findings and empirical observations. Lastly, we demonstrate the superiority of BATMAN over several baselines, in terms of speed and effectiveness, through extensive experiments on fourteen real-world graphs.|图是一种强大的数学模型，用于表示各个领域的真实世界结构。在许多应用中，具有高连通性和鲁棒性的实际结构是可取的。为了增强图的连通性和鲁棒性，增加边和锚定节点这两种操作已经被广泛研究。然而，合并节点，这是一个现实的操作在许多情况下(例如，公共汽车站重组，多个团队形成) ，一直被忽视。本文研究了通过合并节点来提高图的内聚性问题。首先，我们以给定的 $k $为目标，使用 $k $- 桁架的大小数学地表述问题。然后，证明了问题的 NP- 硬度和非模性。然后，根据我们的理论研究结果和经验观察，开发了一种快速有效的节点合并选择算法 BATMAN。最后，我们通过对14个真实世界图的大量实验，证明了蝙蝠侠在速度和效率方面优于几个基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Improving+the+Cohesiveness+of+Graphs+by+Merging+Nodes:+Formulation,+Analysis,+and+Algorithms)|0|
|[When to Pre-Train Graph Neural Networks? From Data Generation Perspective!](https://doi.org/10.1145/3580305.3599548)|Yuxuan Cao, Jiarong Xu, Carl Yang, Jiaan Wang, Yunchao Zhang, Chunping Wang, Lei Chen, Yang Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+to+Pre-Train+Graph+Neural+Networks?+From+Data+Generation+Perspective!)|0|
|[MBrain: A Multi-channel Self-Supervised Learning Framework for Brain Signals](https://doi.org/10.1145/3580305.3599426)|Donghong Cai, Junru Chen, Yang Yang, Teng Liu, Yafeng Li|Nuozhu Technology Co., Ltd.; Zhejiang University|Brain signals are important quantitative data for understanding physiological activities and diseases of human brain. Meanwhile, rapidly developing deep learning methods offer a wide range of opportunities for better modeling brain signals, which has attracted considerable research efforts recently. Most existing studies pay attention to supervised learning methods, which, however, require high-cost clinical labels. In addition, the huge difference in the clinical patterns of brain signals measured by invasive (e.g., SEEG) and non-invasive (e.g., EEG) methods leads to the lack of a unified method. To handle the above issues, in this paper, we propose to study the self-supervised learning (SSL) framework for brain signals that can be applied to pre-train either SEEG or EEG data. Intuitively, brain signals, generated by the firing of neurons, are transmitted among different connecting structures in human brain. Inspired by this, we propose to learn implicit spatial and temporal correlations between different channels (i.e., contacts of the electrode, corresponding to different brain areas) as the cornerstone for uniformly modeling different types of brain signals. Specifically, we capture the temporal correlation by designing the delayed-time-shift prediction task; we represent the spatial correlation by a graph structure, which is built with the goal to maximize the mutual information of each channel and its correlated ones. We further theoretically prove that our design can lead to a better predictive representation. Extensive experiments of seizure detection on both EEG and SEEG large-scale real- world datasets demonstrate our model outperforms several state-of-the-art time series SSL and unsupervised models.|脑信号是了解人脑生理活动和疾病的重要定量数据。与此同时，快速发展的深度学习方法为更好地建立大脑信号模型提供了广泛的机会，近年来引起了相当多的研究工作。大多数现有的研究都关注监督式学习疗法，然而，这需要高成本的临床标签。此外，由于有创(例如 SEEG)和无创(例如 EEG)方法测量的大脑信号的临床模式存在巨大差异，导致缺乏统一的方法。为了解决上述问题，本文提出了一种基于自监督学习(SSL)的大脑信号预训练方法。直观地说，由神经元放电产生的大脑信号在人类大脑的不同连接结构之间传递。受此启发，我们建议学习不同通道之间的内隐空间和时间相关性(即，电极的接触，对应于不同的大脑区域)作为统一建模不同类型的大脑信号的基石。具体来说，我们通过设计延迟时移预测任务来捕获时间相关性，我们用图形结构来表示空间相关性，其目的是最大化各个信道及其相关信道之间的互信息。我们进一步从理论上证明了我们的设计可以导致更好的预测表示。在 EEG 和 SEEG 大规模真实世界数据集上的大量癫痫发作检测实验表明，我们的模型优于几个最先进的时间序列 SSL 和无监督模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MBrain:+A+Multi-channel+Self-Supervised+Learning+Framework+for+Brain+Signals)|0|
|[Efficient Coreset Selection with Cluster-based Methods](https://doi.org/10.1145/3580305.3599326)|Chengliang Chai, Jiayi Wang, Nan Tang, Ye Yuan, Jiabin Liu, Yuhao Deng, Guoren Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Coreset+Selection+with+Cluster-based+Methods)|0|
|[SURE: Robust, Explainable, and Fair Classification without Sensitive Attributes](https://doi.org/10.1145/3580305.3599514)|Deepayan Chakrabarti||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SURE:+Robust,+Explainable,+and+Fair+Classification+without+Sensitive+Attributes)|0|
|[Data-Efficient and Interpretable Tabular Anomaly Detection](https://doi.org/10.1145/3580305.3599294)|ChunHao Chang, Jinsung Yoon, Sercan Ö. Arik, Madeleine Udell, Tomas Pfister|Stanford University; University of Toronto; Google|Anomaly detection (AD) plays an important role in numerous applications. We focus on two understudied aspects of AD that are critical for integration into real-world applications. First, most AD methods cannot incorporate labeled data that are often available in practice in small quantities and can be crucial to achieve high AD accuracy. Second, most AD methods are not interpretable, a bottleneck that prevents stakeholders from understanding the reason behind the anomalies. In this paper, we propose a novel AD framework that adapts a white-box model class, Generalized Additive Models, to detect anomalies using a partial identification objective which naturally handles noisy or heterogeneous features. In addition, the proposed framework, DIAD, can incorporate a small amount of labeled data to further boost anomaly detection performances in semi-supervised settings. We demonstrate the superiority of our framework compared to previous work in both unsupervised and semi-supervised settings using diverse tabular datasets. For example, under 5 labeled anomalies DIAD improves from 86.2\% to 89.4\% AUC by learning AD from unlabeled data. We also present insightful interpretations that explain why DIAD deems certain samples as anomalies.|异常检测(AD)在许多应用中起着重要作用。我们关注 AD 的两个被忽视的方面，这两个方面对于集成到真实世界的应用程序是至关重要的。首先，大多数 AD 方法不能合并标记数据，这些数据通常在实践中少量可用，并且可能是实现高 AD 准确性的关键。其次，大多数 AD 方法是不可解释的，这是一个瓶颈，阻碍了涉众理解异常背后的原因。在本文中，我们提出了一个新的 AD 框架，它适用于一个白盒模型类，广义加法模型，以检测异常使用一个部分识别目标，自然处理噪声或异质特征。此外，建议的 DIAD 框架可以合并少量标记数据，以进一步提高半监督环境下的异常检测性能。我们证明了我们的框架比以前的工作在无监督和半监督设置使用不同的表格数据集优越。例如，在5个标记异常下，通过从未标记的数据中学习 AD，DIAD 从86.2% 提高到89.4% AUC。我们还提出了有见地的解释，解释为什么 DIAD 认为某些样本作为异常。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data-Efficient+and+Interpretable+Tabular+Anomaly+Detection)|0|
|[Open-Set Semi-Supervised Text Classification with Latent Outlier Softening](https://doi.org/10.1145/3580305.3599456)|Junfan Chen, Richong Zhang, Junchi Chen, Chunming Hu, Yongyi Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Open-Set+Semi-Supervised+Text+Classification+with+Latent+Outlier+Softening)|0|
|[Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization](https://doi.org/10.1145/3580305.3599388)|Kaixuan Chen, Shunyu Liu, Tongtian Zhu, Ji Qiao, Yun Su, Yingjie Tian, Tongya Zheng, Haofei Zhang, Zunlei Feng, Jingwen Ye, Mingli Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Expressivity+of+GNNs+with+Subgraph-specific+Factor+Embedded+Normalization)|0|
|[Neural-Hidden-CRF: A Robust Weakly-Supervised Sequence Labeler](https://doi.org/10.1145/3580305.3599445)|Zhijun Chen, Hailong Sun, Wanhao Zhang, Chunyi Xu, Qianren Mao, Pengpeng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural-Hidden-CRF:+A+Robust+Weakly-Supervised+Sequence+Labeler)|0|
|[Classification of Edge-dependent Labels of Nodes in Hypergraphs](https://doi.org/10.1145/3580305.3599274)|Minyoung Choe, Sunwoo Kim, Jaemin Yoo, Kijung Shin|Korea Advanced Institute of Science and Technology; Carnegie Mellon University|A hypergraph is a data structure composed of nodes and hyperedges, where each hyperedge is an any-sized subset of nodes. Due to the flexibility in hyperedge size, hypergraphs represent group interactions (e.g., co-authorship by more than two authors) more naturally and accurately than ordinary graphs. Interestingly, many real-world systems modeled as hypergraphs contain edge-dependent node labels, i.e., node labels that vary depending on hyperedges. For example, on co-authorship datasets, the same author (i.e., a node) can be the primary author in a paper (i.e., a hyperedge) but the corresponding author in another paper (i.e., another hyperedge).   In this work, we introduce a classification of edge-dependent node labels as a new problem. This problem can be used as a benchmark task for hypergraph neural networks, which recently have attracted great attention, and also the usefulness of edge-dependent node labels has been verified in various applications. To tackle this problem, we propose WHATsNet, a novel hypergraph neural network that represents the same node differently depending on the hyperedges it participates in by reflecting its varying importance in the hyperedges. To this end, WHATsNet models the relations between nodes within each hyperedge, using their relative centrality as positional encodings. In our experiments, we demonstrate that WHATsNet significantly and consistently outperforms ten competitors on six real-world hypergraphs, and we also show successful applications of WHATsNet to (a) ranking aggregation, (b) node clustering, and (c) product return prediction.|超图是由节点和超边组成的数据结构，其中每个超边是节点的任意大小的子集。由于超边大小的灵活性，超图比普通图更自然、更准确地表示群体交互(例如，两个以上作者的合著)。有趣的是，许多建模为超图的现实世界系统包含依赖于边缘的节点标签，即依赖于超边缘而变化的节点标签。例如，在合著数据集上，同一作者(即节点)可以是一篇论文的主要作者(即超边缘) ，但是可以是另一篇论文的相应作者(即另一个超边缘)。在这项工作中，我们引入了一个新的问题，即边缘相关节点标签的分类。该问题可以作为超图神经网络的一个基准任务，近年来受到了广泛的关注，边缘相关节点标记在各种应用中的有效性也得到了验证。为了解决这个问题，我们提出了 WHATsNet，这是一种新的超图神经网络，通过在超边界中反映其不同的重要性，它可以根据所参与的超边界不同地表示同一个节点。为此，WHATsNet 使用每个超边界内的节点的相对中心性作为位置编码来建模它们之间的关系。在我们的实验中，我们证明了 WHATsNet 在六个真实世界的超图上显着而持续地优于十个竞争对手，我们还显示了 WHATsNet 在(a)排名聚合，(b)节点聚类和(c)产品返回预测方面的成功应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Classification+of+Edge-dependent+Labels+of+Nodes+in+Hypergraphs)|0|
|[Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers](https://doi.org/10.1145/3580305.3599490)|Chanyoung Chung, Jaejun Lee, Joyce Jiyoung Whang|KAIST|A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding them into the transformers, we reduce the computation cost of using transformers. Using HyNT, we can predict missing numeric values in addition to missing entities or relations in a hyper-relational knowledge graph. Experimental results show that HyNT significantly outperforms state-of-the-art methods on real-world datasets.|最近研究了一个超关系知识图，其中一个三元组与一组限定符相关联; 一个限定符由一个关系和一个实体组成，为一个三元组提供辅助信息。虽然现有的超关系知识图嵌入方法假设实体是离散的对象，一些信息应该使用数值表示，例如，(J.R.R。 ，诞生于，1892)。此外，三连体(J.R.R，牛津大学毕业)可以与限定词，如(开始时间，1911年)。本文提出了一个统一的知识表示框架 HyNT，它可以学习包含三联体或限定符的数值文字的超关系知识图的表示。我们定义了一个上下文转换器和一个预测转换器来学习不仅基于三元组及其限定符之间的相关性，而且基于数值信息的表示。通过学习三联体和限定符的简洁表示并将它们输入变形金刚，我们降低了使用变压器的计算成本。使用 HyNT，我们可以预测缺失的数值，除了缺失的实体或关系在一个超关系知识图。实验结果表明，HyNT 在真实世界数据集上的性能明显优于最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+Learning+on+Hyper-Relational+and+Numeric+Knowledge+Graphs+with+Transformers)|0|
|[Reducing Exposure to Harmful Content via Graph Rewiring](https://doi.org/10.1145/3580305.3599489)|Corinna Coupette, Stefan Neumann, Aristides Gionis|Max Planck Institute for Informatics; KTH Royal Institute of Technology|Most media content consumed today is provided by digital platforms that aggregate input from diverse sources, where access to information is mediated by recommendation algorithms. One principal challenge in this context is dealing with content that is considered harmful. Striking a balance between competing stakeholder interests, rather than block harmful content altogether, one approach is to minimize the exposure to such content that is induced specifically by algorithmic recommendations. Hence, modeling media items and recommendations as a directed graph, we study the problem of reducing the exposure to harmful content via edge rewiring. We formalize this problem using absorbing random walks, and prove that it is NP-hard and NP-hard to approximate to within an additive error, while under realistic assumptions, the greedy method yields a (1-1/e)-approximation. Thus, we introduce Gamine, a fast greedy algorithm that can reduce the exposure to harmful content with or without quality constraints on recommendations. By performing just 100 rewirings on YouTube graphs with several hundred thousand edges, Gamine reduces the initial exposure by 50%, while ensuring that its recommendations are at most 5% less relevant than the original recommendations. Through extensive experiments on synthetic data and real-world data from video recommendation and news feed applications, we confirm the effectiveness, robustness, and efficiency of Gamine in practice.|当今消费的大多数媒体内容都是由数字平台提供的，这些平台汇集了来自不同来源的输入，在这些平台中，信息的获取是通过推荐算法来实现的。这方面的一个主要挑战是处理被认为有害的内容。在相互竞争的利益相关者利益之间取得平衡，而不是完全屏蔽有害的内容，一种方法是尽量减少对这些内容的暴露，这些内容是由算法推荐引起的。因此，建模媒体项目和推荐作为一个有向图，我们研究的问题，减少接触到有害的内容通过边缘重新布线。利用吸收随机游动将该问题形式化，证明了在加性误差范围内逼近是 NP- 困难和 NP- 困难的，而在现实的假设条件下，贪婪方法得到了(1-1/e)-逼近。因此，我们引入 Gamine，一个快速贪婪的算法，可以减少对有害内容的暴露，有或没有质量约束的建议。通过在 YouTube 上执行100次有几十万条边的图表重新布线，Gamine 减少了50% 的初始曝光，同时确保它的建议最多比最初的建议少5% 的相关性。通过对合成数据和来自视频推荐和新闻馈送应用的真实数据的大量实验，我们证实了 Gamine 在实际应用中的有效性、鲁棒性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reducing+Exposure+to+Harmful+Content+via+Graph+Rewiring)|0|
|[MGNN: Graph Neural Networks Inspired by Distance Geometry Problem](https://doi.org/10.1145/3580305.3599431)|Guanyu Cui, Zhewei Wei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MGNN:+Graph+Neural+Networks+Inspired+by+Distance+Geometry+Problem)|0|
|[Deep Encoders with Auxiliary Parameters for Extreme Classification](https://doi.org/10.1145/3580305.3599301)|Kunal Dahiya, Sachin Yadav, Sushant Sondhi, Deepak Saini, Sonu Mehta, Jian Jiao, Sumeet Agarwal, Purushottam Kar, Manik Varma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Encoders+with+Auxiliary+Parameters+for+Extreme+Classification)|0|
|[A Unified Framework of Graph Information Bottleneck for Robustness and Membership Privacy](https://doi.org/10.1145/3580305.3599248)|Enyan Dai, Limeng Cui, Zhengyang Wang, Xianfeng Tang, Yinghan Wang, Monica Xiao Cheng, Bing Yin, Suhang Wang|Amazon; Pennsylvania State University|Graph Neural Networks (GNNs) have achieved great success in modeling graph-structured data. However, recent works show that GNNs are vulnerable to adversarial attacks which can fool the GNN model to make desired predictions of the attacker. In addition, training data of GNNs can be leaked under membership inference attacks. This largely hinders the adoption of GNNs in high-stake domains such as e-commerce, finance and bioinformatics. Though investigations have been made in conducting robust predictions and protecting membership privacy, they generally fail to simultaneously consider the robustness and membership privacy. Therefore, in this work, we study a novel problem of developing robust and membership privacy-preserving GNNs. Our analysis shows that Information Bottleneck (IB) can help filter out noisy information and regularize the predictions on labeled samples, which can benefit robustness and membership privacy. However, structural noises and lack of labels in node classification challenge the deployment of IB on graph-structured data. To mitigate these issues, we propose a novel graph information bottleneck framework that can alleviate structural noises with neighbor bottleneck. Pseudo labels are also incorporated in the optimization to minimize the gap between the predictions on the labeled set and unlabeled set for membership privacy. Extensive experiments on real-world datasets demonstrate that our method can give robust predictions and simultaneously preserve membership privacy.|图形神经网络(GNN)在建立图形结构数据模型方面取得了巨大的成功。然而，最近的工作表明，GNN 是脆弱的对手攻击，可以欺骗 GNN 模型，使所需的预测攻击者。另外，在成员推理攻击下，GNN 的训练数据可能会泄漏。这在很大程度上阻碍了 GNN 在电子商务、金融和生物信息学等高风险领域的采用。虽然在进行强有力的预测和保护成员隐私方面已经进行了调查，但是他们通常没有同时考虑到强健性和成员隐私。因此，在这项工作中，我们研究了一个新的问题，开发健壮和成员隐私保护 GNN。我们的分析表明，信息瓶颈(IB)可以帮助过滤掉噪声信息，规范标记样本的预测，这有利于鲁棒性和成员隐私。然而，节点分类中存在的结构噪声和缺乏标记等问题，给图结构数据 IB 的应用带来了挑战。为了解决这些问题，我们提出了一种新的图形信息瓶颈框架，可以减轻结构噪声的邻居瓶颈。在优化过程中还引入了伪标签，以最大限度地缩小标签集上的预测与未标签集上的成员隐私之间的差距。对真实世界数据集的大量实验表明，我们的方法可以给出稳健的预测，同时保护成员隐私。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Unified+Framework+of+Graph+Information+Bottleneck+for+Robustness+and+Membership+Privacy)|0|
|[Investigating Trojan Attacks on Pre-trained Language Model-powered Database Middleware](https://doi.org/10.1145/3580305.3599395)|Peiran Dong, Song Guo, Junxiao Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigating+Trojan+Attacks+on+Pre-trained+Language+Model-powered+Database+Middleware)|0|
|[Localised Adaptive Spatial-Temporal Graph Neural Network](https://doi.org/10.1145/3580305.3599418)|Wenying Duan, Xiaoxi He, Zimu Zhou, Lothar Thiele, Hong Rao|University of Macau; Nanchang University; ETH Zurich; City University of Hong Kong|Spatial-temporal graph models are prevailing for abstracting and modelling spatial and temporal dependencies. In this work, we ask the following question: \textit{whether and to what extent can we localise spatial-temporal graph models?} We limit our scope to adaptive spatial-temporal graph neural networks (ASTGNNs), the state-of-the-art model architecture. Our approach to localisation involves sparsifying the spatial graph adjacency matrices. To this end, we propose Adaptive Graph Sparsification (AGS), a graph sparsification algorithm which successfully enables the localisation of ASTGNNs to an extreme extent (fully localisation). We apply AGS to two distinct ASTGNN architectures and nine spatial-temporal datasets. Intriguingly, we observe that spatial graphs in ASTGNNs can be sparsified by over 99.5\% without any decline in test accuracy. Furthermore, even when ASTGNNs are fully localised, becoming graph-less and purely temporal, we record no drop in accuracy for the majority of tested datasets, with only minor accuracy deterioration observed in the remaining datasets. However, when the partially or fully localised ASTGNNs are reinitialised and retrained on the same data, there is a considerable and consistent drop in accuracy. Based on these observations, we reckon that \textit{(i)} in the tested data, the information provided by the spatial dependencies is primarily included in the information provided by the temporal dependencies and, thus, can be essentially ignored for inference; and \textit{(ii)} although the spatial dependencies provide redundant information, it is vital for the effective training of ASTGNNs and thus cannot be ignored during training. Furthermore, the localisation of ASTGNNs holds the potential to reduce the heavy computation overhead required on large-scale spatial-temporal data and further enable the distributed deployment of ASTGNNs.|时空图模型主要用于抽象和建模空间和时间依赖关系。在这项工作中，我们提出了以下问题: texttit {我们是否可以局部化时空图模型，以及在多大程度上可以局部化? }我们限制我们的范围，自适应时空图神经网络(ASTGNN) ，国家的最先进的模型架构。我们的定位方法包括稀疏空间图的邻接矩阵。为此，我们提出了自适应图稀疏化(AGS)算法，这是一种图稀疏化算法，它成功地使 ASTGNN 的本地化达到了一个极端的程度(完全本地化)。我们将 AGS 应用于两个不同的 ASTGNN 体系结构和九个时空数据集。有趣的是，我们观察到 ASTGNN 中的空间图可以稀疏超过99.5% ，而且测试精度没有任何下降。此外，即使当 ASTGNN 完全本地化，成为无图和纯粹的时间，我们没有记录大多数测试数据集的准确性下降，只有在其余数据集中观察到轻微的准确性恶化。然而，当部分或完全本地化的 ASTGNN 在相同的数据上重新初始化和再训练时，精度会有相当大的一致性下降。基于这些观测结果，我们认为在测试数据中的 texttit {(i)} ，空间依赖提供的信息主要包含在时间依赖提供的信息中，因此基本上可以被忽略进行推理; 而 texttit {(ii)}尽管空间依赖提供了冗余信息，但对于 ASTGNN 的有效训练是至关重要的，因此在训练过程中不能被忽略。此外，ASTGNN 的本地化有可能减少大规模时空数据所需的大量计算开销，并进一步实现 ASTGNN 的分布式部署。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Localised+Adaptive+Spatial-Temporal+Graph+Neural+Network)|0|
|[TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://doi.org/10.1145/3580305.3599533)|Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam|IBM Research|Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).|变压器已获得流行的时间序列预测，因为他们的能力捕捉长期序列的相互作用。然而，它们的高内存和计算需求成为长期预测的一个关键瓶颈。为了解决这个问题，我们提出了 TSMixer，一个轻量级的神经结构完全由多层感知器(MLP)模块组成。TSMixer 设计用于对修补后的时间序列进行多变量预测和表示学习，为变压器提供了一种有效的替代方案。我们的模型从计算机视觉中 MLP-Mixer 模型的成功中得到启发。我们展示了将视觉 MLP 混频器应用于时间序列所面临的挑战，并引入了经验验证的组件来提高精度。这包括一个新颖的设计范例，将在线协调头附加到 MLP-Mixer 骨干，用于显式建模时间序列属性，如层次结构和通道相关性。我们还提出了一种混合信道建模方法来有效地处理不同数据集之间的噪声信道交互和泛化，这是现有补丁信道混合方法的一个共同挑战。此外，在主干中引入了一个简单的门控注意机制来对重要特性进行优先排序。通过结合这些轻量级组件，我们显著提高了简单 MLP 结构的学习能力，以最少的计算使用超过了复杂的变压器模型。此外，TSMixer 的模块化设计能够兼容监督和掩蔽自监督学习方法，使其成为时间序列基础模型的一个有前途的组成部分。TSMixer 在预测方面优于最先进的 MLP 和变压器模型，优势可达8-60% 。它还优于最新的补丁变压器模型的强大基准测试(1-2%) ，大大减少了内存和运行时(2-3倍)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TSMixer:+Lightweight+MLP-Mixer+Model+for+Multivariate+Time+Series+Forecasting)|0|
|[Dependence and Model Selection in LLP: The Problem of Variants](https://doi.org/10.1145/3580305.3599307)|Gabriel Franco, Mark Crovella, Giovanni Comarela||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dependence+and+Model+Selection+in+LLP:+The+Problem+of+Variants)|0|
|[Pre-training Antibody Language Models for Antigen-Specific Computational Antibody Design](https://doi.org/10.1145/3580305.3599468)|Kaiyuan Gao, Lijun Wu, Jinhua Zhu, Tianbo Peng, Yingce Xia, Liang He, Shufang Xie, Tao Qin, Haiguang Liu, Kun He, TieYan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-training+Antibody+Language+Models+for+Antigen-Specific+Computational+Antibody+Design)|0|
|[GAL-VNE: Solving the VNE Problem with Global Reinforcement Learning and Local One-Shot Neural Prediction](https://doi.org/10.1145/3580305.3599358)|Haoyu Geng, Runzhong Wang, Fei Wu, Junchi Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAL-VNE:+Solving+the+VNE+Problem+with+Global+Reinforcement+Learning+and+Local+One-Shot+Neural+Prediction)|0|
|[Sparse Binary Transformers for Multivariate Time Series Modeling](https://doi.org/10.1145/3580305.3599508)|Matt Gorbett, Hossein Shirazi, Indrakshi Ray|Colorado State University|Compressed Neural Networks have the potential to enable deep learning across new applications and smaller computational environments. However, understanding the range of learning tasks in which such models can succeed is not well studied. In this work, we apply sparse and binary-weighted Transformers to multivariate time series problems, showing that the lightweight models achieve accuracy comparable to that of dense floating-point Transformers of the same structure. Our model achieves favorable results across three time series learning tasks: classification, anomaly detection, and single-step forecasting. Additionally, to reduce the computational complexity of the attention mechanism, we apply two modifications, which show little to no decline in model performance: 1) in the classification task, we apply a fixed mask to the query, key, and value activations, and 2) for forecasting and anomaly detection, which rely on predicting outputs at a single point in time, we propose an attention mask to allow computation only at the current time step. Together, each compression technique and attention modification substantially reduces the number of non-zero operations necessary in the Transformer. We measure the computational savings of our approach over a range of metrics including parameter count, bit size, and floating point operation (FLOPs) count, showing up to a 53x reduction in storage size and up to 10.5x reduction in FLOPs.|压缩神经网络具有在新的应用程序和较小的计算环境中实现深度学习的潜力。然而，理解这些模型能够成功的学习任务的范围并没有得到很好的研究。将稀疏变压器和二进制加权变压器应用于多变量时间序列问题，结果表明，轻量化模型的精度可以与同一结构的稠密浮点变压器相媲美。我们的模型在三个时间序列学习任务中取得了良好的结果: 分类、异常检测和单步预测。此外，为了降低注意力机制的计算复杂性，我们应用了两个修改，这两个修改显示模型性能几乎没有下降: 1)在分类任务中，我们对查询、键和值激活应用了一个固定的掩码; 2)预测和异常检测，这依赖于预测单个时间点的输出，我们提出了一个注意掩码，只允许在当前的时间步骤进行计算。总之，每种压缩技术和注意力修改大大减少了变压器中必需的非零操作的数量。我们通过一系列指标(包括参数计数、位大小和浮点运算(FLOPs)计数)来衡量我们的方法的计算节省，显示出存储大小减少了53倍，浮点运算减少了10.5倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sparse+Binary+Transformers+for+Multivariate+Time+Series+Modeling)|0|
|[3D-Polishing for Triangular Mesh Compression of Point Cloud Data](https://doi.org/10.1145/3580305.3599239)|Jiaqi Gu, Guosheng Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=3D-Polishing+for+Triangular+Mesh+Compression+of+Point+Cloud+Data)|0|
|[ESSA: Explanation Iterative Supervision via Saliency-guided Data Augmentation](https://doi.org/10.1145/3580305.3599336)|Siyi Gu, Yifei Zhang, Yuyang Gao, Xiaofeng Yang, Liang Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ESSA:+Explanation+Iterative+Supervision+via+Saliency-guided+Data+Augmentation)|0|
|[CounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations](https://doi.org/10.1145/3580305.3599290)|Hangzhi Guo, Thanh Hong Nguyen, Amulya Yadav|Pennsylvania State University; University of Oregon|Counterfactual (or CF) explanations are a type of local explanations for Machine Learning (ML) model predictions, which offer a contrastive case as an explanation by finding the smallest changes (in feature space) to the input data point, which will lead to a different prediction by the ML model. Existing CF explanation techniques suffer from two major limitations: (i) all of them are post-hoc methods designed for use with proprietary ML models --- as a result, their procedure for generating CF explanations is uninformed by the training of the ML model, which leads to misalignment between model predictions and explanations; and (ii) most of them rely on solving separate time-intensive optimization problems to find CF explanations for each input data point (which negatively impacts their runtime). This work makes a novel departure from the prevalent post-hoc paradigm (of generating CF explanations) by presenting CounterNet, an end-to-end learning framework which integrates predictive model training and the generation of counterfactual (CF) explanations into a single pipeline. We adopt a block-wise coordinate descent procedure which helps in effectively training CounterNet's network. Our extensive experiments on multiple real-world datasets show that CounterNet generates high-quality predictions, and consistently achieves 100% CF validity and very low proximity scores (thereby achieving a well-balanced cost-invalidity trade-off) for any new input instance, and runs 3X faster than existing state-of-the-art baselines.|反事实(CF)解释是对机器学习(ML)模型预测的一种局部解释，它通过寻找输入数据点的最小变化(在特征空间)来提供一个对比的案例作为解释，这将导致机器学习模型的一个不同的预测。现有的 CF 解释技术有两个主要的局限性: (i)它们都是专门为使用专有机器学习模型而设计的事后方法——因此，它们生成 CF 解释的过程没有受到机器学习模型训练的影响，这导致模型预测和解释之间的不一致; (ii)它们中的大多数依赖于解决单独的时间密集型优化问题来为每个输入数据点找到 CF 解释(这对它们的运行时间有负面影响)。这项工作通过提出 CounterNet，一个将预测模型训练和反事实(CF)解释生成集成到一个单一管道的端到端学习框架，从流行的事后范式(生成 CF 解释)做出了新的背离。我们采用分组坐标下降法的程序，有助有效地训练 CounterNet 的网络。我们在多个真实世界数据集上的广泛实验表明，CounterNet 产生高质量的预测，并始终达到100% CF 有效性和非常低的接近得分(从而实现了良好的成本-无效性权衡) ，对于任何新的输入实例，运行速度比现有的最先进的基线快3倍。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CounterNet:+End-to-End+Training+of+Prediction+Aware+Counterfactual+Explanations)|0|
|[Clenshaw Graph Neural Networks](https://doi.org/10.1145/3580305.3599275)|Yuhe Guo, Zhewei Wei|Renmin University of China|Graph Convolutional Networks (GCNs), which use a message-passing paradigm with stacked convolution layers, are foundational methods for learning graph representations. Recent GCN models use various residual connection techniques to alleviate the model degradation problem such as over-smoothing and gradient vanishing. Existing residual connection techniques, however, fail to make extensive use of underlying graph structure as in the graph spectral domain, which is critical for obtaining satisfactory results on heterophilic graphs. In this paper, we introduce ClenshawGCN, a GNN model that employs the Clenshaw Summation Algorithm to enhance the expressiveness of the GCN model. ClenshawGCN equips the standard GCN model with two straightforward residual modules: the adaptive initial residual connection and the negative second-order residual connection. We show that by adding these two residual modules, ClenshawGCN implicitly simulates a polynomial filter under the Chebyshev basis, giving it at least as much expressive power as polynomial spectral GNNs. In addition, we conduct comprehensive experiments to demonstrate the superiority of our model over spatial and spectral GNN models.|图卷积网络(GCNs)是学习图表示的基本方法，它使用一种带有层叠卷积层的消息传递范式。最近的 GCN 模型使用各种残差连接技术来缓解模型退化问题，如过度平滑和梯度消失。然而，现有的残差连接技术不能像在图谱域中那样广泛地利用底层图结构，这对于获得满意的异质图结果是至关重要的。在本文中，我们介绍了 Clenshaw GCN，一个使用 Clenshaw 求和算法来增强 GCN 模型表达能力的 GNN 模型。标准 GCN 模型具有两个简单的残差模块: 自适应初始残差连接和负二阶残差连接。通过添加这两个残差模块，ClenshawGCN 隐式地模拟了 Chebyshev 基下的多项式滤波器，使其至少具有与多项式谱 GNN 相同的表达能力。此外，我们进行了全面的实验，以证明我们的模型优于空间和光谱 GNN 模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Clenshaw+Graph+Neural+Networks)|0|
|[CampER: An Effective Framework for Privacy-Aware Deep Entity Resolution](https://doi.org/10.1145/3580305.3599266)|Yuxiang Guo, Lu Chen, Zhengjie Zhou, Baihua Zheng, Ziquan Fang, Zhikun Zhang, Yuren Mao, Yunjun Gao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CampER:+An+Effective+Framework+for+Privacy-Aware+Deep+Entity+Resolution)|0|
|[A Data-centric Framework to Endow Graph Neural Networks with Out-Of-Distribution Detection Ability](https://doi.org/10.1145/3580305.3599244)|Yuxin Guo, Cheng Yang, Yuluo Chen, Jixi Liu, Chuan Shi, Junping Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Data-centric+Framework+to+Endow+Graph+Neural+Networks+with+Out-Of-Distribution+Detection+Ability)|0|
|[Frigate: Frugal Spatio-temporal Forecasting on Road Networks](https://doi.org/10.1145/3580305.3599357)|Mridul Gupta, Hariprasad Kodamana, Sayan Ranu|Indian Institute of Technology Delhi|Modelling spatio-temporal processes on road networks is a task of growing importance. While significant progress has been made on developing spatio-temporal graph neural networks (Gnns), existing works are built upon three assumptions that are not practical on real-world road networks. First, they assume sensing on every node of a road network. In reality, due to budget-constraints or sensor failures, all locations (nodes) may not be equipped with sensors. Second, they assume that sensing history is available at all installed sensors. This is unrealistic as well due to sensor failures, loss of packets during communication, etc. Finally, there is an assumption of static road networks. Connectivity within networks change due to road closures, constructions of new roads, etc. In this work, we develop FRIGATE to address all these shortcomings. FRIGATE is powered by a spatio-temporal Gnn that integrates positional, topological, and temporal information into rich inductive node representations. The joint fusion of this diverse information is made feasible through a novel combination of gated Lipschitz embeddings with Lstms. We prove that the proposed Gnn architecture is provably more expressive than message-passing Gnns used in state-of-the-art algorithms. The higher expressivity of FRIGATE naturally translates to superior empirical performance conducted on real-world network-constrained traffic data. In addition, FRIGATE is robust to frugal sensor deployment, changes in road network connectivity, and temporal irregularity in sensing.|道路网络时空过程建模是一项日益重要的任务。虽然时空图形神经网络(Gnns)的开发已经取得了重大进展，但现有的工作是建立在三个假设之上的，这些假设在现实世界的道路网络中是不实际的。首先，它们假设对道路网络的每个节点进行检测。实际上，由于预算限制或传感器故障，所有位置(节点)可能不配备传感器。其次，他们假设所有安装的传感器都具有传感历史。由于传感器故障、数据包在通信过程中丢失等原因，这也是不现实的。最后，提出了静态路网的假设。由于道路封闭、新建道路等原因，网络内的连通性会发生变化。在这项工作中，我们开发 FRIGATE 来解决所有这些缺点。FRIGATE 由一个时空 Gnn 驱动，该 Gnn 将位置、拓扑和时间信息集成到丰富的归纳节点表示中。通过门控 Lipschitz 嵌入与 Lstms 的新颖结合，使得这种多样化信息的联合融合成为可能。我们证明了所提出的 Gnn 结构比最先进的算法中使用的消息传递 Gnn 具有更好的表达能力。FRIGATE 更高的表达能力自然地转化为在现实世界网络约束的流量数据上进行的更好的经验性能。此外，FRIGATE 对节约的传感器部署、道路网络连通性的变化和传感的时间不规则性具有鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Frigate:+Frugal+Spatio-temporal+Forecasting+on+Road+Networks)|0|
|[Mitigating Action Hysteresis in Traffic Signal Control with Traffic Predictive Reinforcement Learning](https://doi.org/10.1145/3580305.3599528)|Xiao Han, Xiangyu Zhao, Liang Zhang, Wanyu Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Action+Hysteresis+in+Traffic+Signal+Control+with+Traffic+Predictive+Reinforcement+Learning)|0|
|[GAT-MF: Graph Attention Mean Field for Very Large Scale Multi-Agent Reinforcement Learning](https://doi.org/10.1145/3580305.3599359)|Qianyue Hao, Wenzhen Huang, Tao Feng, Jian Yuan, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAT-MF:+Graph+Attention+Mean+Field+for+Very+Large+Scale+Multi-Agent+Reinforcement+Learning)|0|
|[Prescriptive PCA: Dimensionality Reduction for Two-stage Stochastic Optimization](https://doi.org/10.1145/3580305.3599474)|Long He, HoYin Mak|Georgetown University; George Washington University|In this paper, we consider the alignment between an upstream dimensionality reduction task of learning a low-dimensional representation of a set of high-dimensional data and a downstream optimization task of solving a stochastic program parameterized by said representation. In this case, standard dimensionality reduction methods (e.g., principal component analysis) may not perform well, as they aim to maximize the amount of information retained in the representation and do not generally reflect the importance of such information in the downstream optimization problem. To address this problem, we develop a prescriptive dimensionality reduction framework that aims to minimize the degree of suboptimality in the optimization phase. For the case where the downstream stochastic optimization problem has an expected value objective, we show that prescriptive dimensionality reduction can be performed via solving a distributionally-robust optimization problem, which admits a semidefinite programming relaxation. Computational experiments based on a warehouse transshipment problem and a vehicle repositioning problem show that our approach significantly outperforms principal component analysis with real and synthetic data sets.|在这篇文章中，我们考虑了上游的降维任务(学习一组高维数据的低维表示)和下游的优化任务(解决一个由该表示参数化的随机程序)之间的一致性。在这种情况下，标准的降维方法(例如主成分分析)可能不会有很好的效果，因为它们的目标是最大限度地提高表示中所保留的信息的数量，而且通常不能反映这些信息在下游最佳化问题中的重要性。为了解决这个问题，我们开发了一个规范的降维框架，旨在最小化优化阶段的次优化程度。对于下游随机最佳化问题具有期望值目标的情况，我们证明了规范降维可以通过求解一个允许半定规划松弛的分布鲁棒最佳化问题来实现。基于一个仓库转运问题和一个车辆重新定位问题的计算实验表明，我们的方法显著优于真实和合成数据集的主成分分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prescriptive+PCA:+Dimensionality+Reduction+for+Two-stage+Stochastic+Optimization)|0|
|[Graph Neural Processes for Spatio-Temporal Extrapolation](https://doi.org/10.1145/3580305.3599372)|Junfeng Hu, Yuxuan Liang, Zhencheng Fan, Hongyang Chen, Yu Zheng, Roger Zimmermann|National University of Singapore; Hong Kong University of Science and Technology (Guangzhou); JD Intelligent Cities Research; Zhejiang Lab; University of Technology Sydney|We study the task of spatio-temporal extrapolation that generates data at target locations from surrounding contexts in a graph. This task is crucial as sensors that collect data are sparsely deployed, resulting in a lack of fine-grained information due to high deployment and maintenance costs. Existing methods either use learning-based models like Neural Networks or statistical approaches like Gaussian Processes for this task. However, the former lacks uncertainty estimates and the latter fails to capture complex spatial and temporal correlations effectively. To address these issues, we propose Spatio-Temporal Graph Neural Processes (STGNP), a neural latent variable model which commands these capabilities simultaneously. Specifically, we first learn deterministic spatio-temporal representations by stacking layers of causal convolutions and cross-set graph neural networks. Then, we learn latent variables for target locations through vertical latent state transitions along layers and obtain extrapolations. Importantly during the transitions, we propose Graph Bayesian Aggregation (GBA), a Bayesian graph aggregator that aggregates contexts considering uncertainties in context data and graph structure. Extensive experiments show that STGNP has desirable properties such as uncertainty estimates and strong learning capabilities, and achieves state-of-the-art results by a clear margin.|我们研究的任务时空外推生成数据在目标位置从周围环境在一个图。这项任务至关重要，因为收集数据的传感器部署得很少，由于部署和维护成本高昂，导致缺乏细粒度信息。现有的方法要么使用像神经网络这样的基于学习的模型，要么使用像高斯过程这样的统计方法来完成这项任务。然而，前者缺乏不确定性估计，后者未能有效捕捉复杂的时空相关性。为了解决这些问题，我们提出了时空图形神经过程(STGNP) ，一个神经潜变量模型，它同时命令这些能力。具体来说，我们首先通过叠加因果卷积层和交集图神经网络来学习确定性时空表示。然后，通过层间的垂直潜状态转换来学习目标位置的潜变量，并得到外推结果。重要的是，在转换过程中，我们提出了图贝叶斯聚集(GBA) ，一个贝叶斯图聚集器，聚集上下文考虑不确定性的上下文数据和图结构。广泛的实验表明，STGNP 具有不确定性估计和强大的学习能力等优良特性，并以明显的优势取得了最先进的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Neural+Processes+for+Spatio-Temporal+Extrapolation)|0|
|[ST-iFGSM: Enhancing Robustness of Human Mobility Signature Identification Model via Spatial-Temporal Iterative FGSM](https://doi.org/10.1145/3580305.3599513)|Mingzhi Hu, Xin Zhang, Yanhua Li, Xun Zhou, Jun Luo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ST-iFGSM:+Enhancing+Robustness+of+Human+Mobility+Signature+Identification+Model+via+Spatial-Temporal+Iterative+FGSM)|0|
|[Leveraging Relational Graph Neural Network for Transductive Model Ensemble](https://doi.org/10.1145/3580305.3599414)|Zhengyu Hu, Jieyu Zhang, Haonan Wang, Siwei Liu, Shangsong Liang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Relational+Graph+Neural+Network+for+Transductive+Model+Ensemble)|0|
|[One for All: Unified Workload Prediction for Dynamic Multi-tenant Edge Cloud Platforms](https://doi.org/10.1145/3580305.3599453)|Shaoyuan Huang, Zheng Wang, Heng Zhang, Xiaofei Wang, Cheng Zhang, Wenyu Wang|Paiou Cloud Computing (Shanghai) Co., Ltd; Tianjin University; Tianjin University of Finance|Workload prediction in multi-tenant edge cloud platforms (MT-ECP) is vital for efficient application deployment and resource provisioning. However, the heterogeneous application patterns, variable infrastructure performance, and frequent deployments in MT-ECP pose significant challenges for accurate and efficient workload prediction. Clustering-based methods for dynamic MT-ECP modeling often incur excessive costs due to the need to maintain numerous data clusters and models, which leads to excessive costs. Existing end-to-end time series prediction methods are challenging to provide consistent prediction performance in dynamic MT-ECP. In this paper, we propose an end-to-end framework with global pooling and static content awareness, DynEformer, to provide a unified workload prediction scheme for dynamic MT-ECP. Meticulously designed global pooling and information merging mechanisms can effectively identify and utilize global application patterns to drive local workload predictions. The integration of static content-aware mechanisms enhances model robustness in real-world scenarios. Through experiments on five real-world datasets, DynEformer achieved state-of-the-art in the dynamic scene of MT-ECP and provided a unified end-to-end prediction scheme for MT-ECP.|多租户边缘云平台(MT-ECP)中的工作负载预测对于有效的应用程序部署和资源配置至关重要。然而，异构的应用程序模式、可变的基础设施性能以及 MT-ECP 中的频繁部署对准确有效的工作负载预测提出了严峻的挑战。基于聚类的动态 MT-ECP 建模方法由于需要维护大量的数据集群和模型，往往会产生过高的成本。现有的端到端时间序列预测方法难以在动态 MT-ECP 中提供一致的预测性能。本文提出了一个具有全局池和静态内容感知的端到端框架 DynEformer，为动态 MT-ECP 提供一个统一的工作负载预测方案。精心设计的全局池和信息合并机制可以有效地识别和利用全局应用程序模式来驱动本地工作负载预测。静态内容感知机制的集成增强了真实场景中模型的健壮性。通过对五个实际数据集的实验，DynEformer 实现了 MT-ECP 动态场景中的最新技术，为 MT-ECP 提供了一个统一的端到端预测方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One+for+All:+Unified+Workload+Prediction+for+Dynamic+Multi-tenant+Edge+Cloud+Platforms)|0|
|[Generalizing Graph ODE for Learning Complex System Dynamics across Environments](https://doi.org/10.1145/3580305.3599362)|Zijie Huang, Yizhou Sun, Wei Wang|University of California, Los Angeles|Learning multi-agent system dynamics has been extensively studied for various real-world applications, such as molecular dynamics in biology. Most of the existing models are built to learn single system dynamics from observed historical data and predict the future trajectory. In practice, however, we might observe multiple systems that are generated across different environments, which differ in latent exogenous factors such as temperature and gravity. One simple solution is to learn multiple environment-specific models, but it fails to exploit the potential commonalities among the dynamics across environments and offers poor prediction results where per-environment data is sparse or limited. Here, we present GG-ODE (Generalized Graph Ordinary Differential Equations), a machine learning framework for learning continuous multi-agent system dynamics across environments. Our model learns system dynamics using neural ordinary differential equations (ODE) parameterized by Graph Neural Networks (GNNs) to capture the continuous interaction among agents. We achieve the model generalization by assuming the dynamics across different environments are governed by common physics laws that can be captured via learning a shared ODE function. The distinct latent exogenous factors learned for each environment are incorporated into the ODE function to account for their differences. To improve model performance, we additionally design two regularization losses to (1) enforce the orthogonality between the learned initial states and exogenous factors via mutual information minimization; and (2) reduce the temporal variance of learned exogenous factors within the same system via contrastive learning. Experiments over various physical simulations show that our model can accurately predict system dynamics, especially in the long range, and can generalize well to new systems with few observations.|学习多智能体系统动力学已经被广泛研究用于各种现实世界的应用，例如生物学中的分子动力学。现有的大多数模型都是从观测的历史数据中学习单个系统的动力学，并预测未来的轨迹。然而，在实践中，我们可能会观察到在不同环境中产生的多个系统，这些系统在温度和重力等潜在的外部因素上有所不同。一个简单的解决方案是学习多个特定于环境的模型，但是它无法利用跨环境动态之间的潜在共性，并且在每个环境的数据稀少或有限的情况下提供较差的预测结果。在这里，我们介绍了广义图常微分方程(gg-ODE) ，一个机器学习框架，用于学习跨环境的连续多智能体系统动力学。该模型利用图神经网络(GNN)参数化的神经常微分方程(ODE)来学习系统动力学，以捕捉智能体之间的连续相互作用。我们通过假设不同环境中的动力学是由可以通过学习一个共享的 ODE 函数来捕获的公共物理定律控制的，从而实现了模型的泛化。每个环境所学到的不同的潜在外部因素被纳入 ODE 功能，以解释它们之间的差异。为了提高模型的性能，我们另外设计了两个正则化损失: (1)通过相互信息最小化增强学习初始状态与外生因素之间的正交性; (2)通过对比学习减少同一系统中学习外生因素的时间方差。通过各种物理仿真实验表明，该模型能够准确地预测系统动力学，特别是在长期范围内，并且能够很好地推广到观测较少的新系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generalizing+Graph+ODE+for+Learning+Complex+System+Dynamics+across+Environments)|0|
|[The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles](https://doi.org/10.1145/3580305.3599520)|Md. Shamim Hussain, Mohammed J. Zaki, Dharmashankar Subramanian|Rensselaer Polytechnic Institute; International Business Machines|Transformers use the dense self-attention mechanism which gives a lot of flexibility for long-range connectivity. Over multiple layers of a deep transformer, the number of possible connectivity patterns increases exponentially. However, very few of these contribute to the performance of the network, and even fewer are essential. We hypothesize that there are sparsely connected sub-networks within a transformer, called information pathways which can be trained independently. However, the dynamic (i.e., input-dependent) nature of these pathways makes it difficult to prune dense self-attention during training. But the overall distribution of these pathways is often predictable. We take advantage of this fact to propose Stochastically Subsampled self-Attention (SSA) - a general-purpose training strategy for transformers that can reduce both the memory and computational cost of self-attention by 4 to 8 times during training while also serving as a regularization method - improving generalization over dense training. We show that an ensemble of sub-models can be formed from the subsampled pathways within a network, which can achieve better performance than its densely attended counterpart. We perform experiments on a variety of NLP, computer vision and graph learning tasks in both generative and discriminative settings to provide empirical evidence for our claims and show the effectiveness of the proposed method.|变压器采用密集的自我注意机制，为长距离连接提供了很大的灵活性。在深度变压器的多层上，可能的连接模式的数量呈指数增长。然而，其中很少有对网络性能有贡献的，甚至更少是必不可少的。我们假设在一个变压器内部存在稀疏连接的子网络，称为信息路径，可以独立地进行训练。然而，这些通路的动态(即依赖输入)特性使得在训练期间很难修剪密集的自我注意力。但是这些通路的总体分布通常是可以预测的。我们利用这一事实提出了随机次采样自我注意(SSA)——一种通用的变压器训练策略，它可以在训练过程中将自我注意的记忆和计算成本降低4 ~ 8倍，同时也作为一种正则化方法——提高了密集训练的泛化能力。我们证明了子模型的集合可以从网络中的子采样路径形成，它可以达到比其密集参与的对应物更好的性能。我们在生成性和区分性环境下进行各种自然语言处理、计算机视觉和图形学习任务的实验，为我们的声称提供经验证明，并显示建议方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Information+Pathways+Hypothesis:+Transformers+are+Dynamic+Self-Ensembles)|0|
|[Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution](https://doi.org/10.1145/3580305.3599365)|Tsuyoshi Idé, Naoki Abe|IBM Research, Thomas J. Watson Research Center|We address the task of probabilistic anomaly attribution in the black-box regression setting, where the goal is to compute the probability distribution of the attribution score of each input variable, given an observed anomaly. The training dataset is assumed to be unavailable. This task differs from the standard XAI (explainable AI) scenario, since we wish to explain the anomalous deviation from a black-box prediction rather than the black-box model itself.   We begin by showing that mainstream model-agnostic explanation methods, such as the Shapley values, are not suitable for this task because of their ``deviation-agnostic property.'' We then propose a novel framework for probabilistic anomaly attribution that allows us to not only compute attribution scores as the predictive mean but also quantify the uncertainty of those scores. This is done by considering a generative process for perturbations that counter-factually bring the observed anomalous observation back to normalcy. We introduce a variational Bayes algorithm for deriving the distributions of per variable attribution scores. To the best of our knowledge, this is the first probabilistic anomaly attribution framework that is free from being deviation-agnostic.|我们在黑盒回归设置中处理概率异常归因的任务，其目标是计算每个输入变量的归因得分的概率分布，给定一个观察到的异常。假定训练数据集不可用。这个任务不同于标准的 XAI (可解释的 AI)场景，因为我们希望解释与黑盒预测的异常偏差，而不是黑盒模型本身。我们首先展示了主流的模型无关解释方法，例如 Shapley 值，由于它们的“偏差无关性”，不适合这个任务然后，我们提出了一个新的概率异常归因框架，使我们不仅计算归因分数作为预测平均值，而且量化这些分数的不确定性。这是通过考虑扰动的生成过程来完成的，扰动反事实地将观测到的反常观测恢复到正常状态。我们引入了一个变分贝叶斯算法来推导每个变量属性得分的分布。据我们所知，这是第一个没有偏差不可知的概率异常归因框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Perturbation+Analysis+for+Probabilistic+Black-Box+Anomaly+Attribution)|0|
|[Parameter-free Spikelet: Discovering Different Length and Warped Time Series Motifs using an Adaptive Time Series Representation](https://doi.org/10.1145/3580305.3599310)|Makoto Imamura, Takaaki Nakamura||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Parameter-free+Spikelet:+Discovering+Different+Length+and+Warped+Time+Series+Motifs+using+an+Adaptive+Time+Series+Representation)|0|
|[Hierarchical Proxy Modeling for Improved HPO in Time Series Forecasting](https://doi.org/10.1145/3580305.3599378)|Arindam Jati, Vijay Ekambaram, Shaonli Pal, Brian Quanz, Wesley M. Gifford, Pavithra Harsha, Stuart Siegel, Sumanta Mukherjee, Chandra Narayanaswami||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Proxy+Modeling+for+Improved+HPO+in+Time+Series+Forecasting)|0|
|[Precursor-of-Anomaly Detection for Irregular Time Series](https://doi.org/10.1145/3580305.3599469)|Sheo Yon Jhin, Jaehoon Lee, Noseong Park|Yonsei University; LG AI Research|Anomaly detection is an important field that aims to identify unexpected patterns or data points, and it is closely related to many real-world problems, particularly to applications in finance, manufacturing, cyber security, and so on. While anomaly detection has been studied extensively in various fields, detecting future anomalies before they occur remains an unexplored territory. In this paper, we present a novel type of anomaly detection, called \emph{\textbf{P}recursor-of-\textbf{A}nomaly} (PoA) detection. Unlike conventional anomaly detection, which focuses on determining whether a given time series observation is an anomaly or not, PoA detection aims to detect future anomalies before they happen. To solve both problems at the same time, we present a neural controlled differential equation-based neural network and its multi-task learning algorithm. We conduct experiments using 17 baselines and 3 datasets, including regular and irregular time series, and demonstrate that our presented method outperforms the baselines in almost all cases. Our ablation studies also indicate that the multitasking training method significantly enhances the overall performance for both anomaly and PoA detection.|异常检测是一个重要的领域，旨在识别意想不到的模式或数据点，它与许多现实世界的问题密切相关，特别是在金融、制造业、网络安全等方面的应用。虽然异常检测已经在各个领域得到了广泛的研究，但是在未来的异常发生之前探测到它们仍然是一个未知的领域。在这篇文章中，我们提出了一种新的异常检测检测方法，称为 emph { textbf { P }-textbf { A }异常}(PoA)检测。与传统的异常检测不同，PoA 检测的重点是确定给定的时间序列观测是否是异常现象，而 PoA 检测的目的是在未来的异常现象发生之前检测出来。为了同时解决这两个问题，本文提出了一种基于神经控制微分方程的神经网络及其多任务学习算法。我们使用17个基线和3个数据集(包括规则和不规则时间序列)进行实验，并证明我们提出的方法在几乎所有情况下都优于基线。我们的消融研究还表明，多任务训练方法显著提高了异常和 PoA 检测的整体性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Precursor-of-Anomaly+Detection+for+Irregular+Time+Series)|0|
|[Community-based Dynamic Graph Learning for Popularity Prediction](https://doi.org/10.1145/3580305.3599281)|Shuo Ji, Xiaodong Lu, Mingzhe Liu, Leilei Sun, Chuanren Liu, Bowen Du, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Community-based+Dynamic+Graph+Learning+for+Popularity+Prediction)|0|
|[GetPt: Graph-enhanced General Table Pre-training with Alternate Attention Network](https://doi.org/10.1145/3580305.3599366)|Ran Jia, Haoming Guo, Xiaoyuan Jin, Chao Yan, Lun Du, Xiaojun Ma, Tamara Stankovic, Marko Lozajic, Goran Zoranovic, Igor Ilic, Shi Han, Dongmei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GetPt:+Graph-enhanced+General+Table+Pre-training+with+Alternate+Attention+Network)|0|
|[Enhancing Node-Level Adversarial Defenses by Lipschitz Regularization of Graph Neural Networks](https://doi.org/10.1145/3580305.3599335)|Yaning Jia, Dongmian Zou, Hongfei Wang, Hai Jin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Node-Level+Adversarial+Defenses+by+Lipschitz+Regularization+of+Graph+Neural+Networks)|0|
|[Complementary Classifier Induced Partial Label Learning](https://doi.org/10.1145/3580305.3599282)|Yuheng Jia, Chongjie Si, MinLing Zhang|Shanghai Jiao Tong University; Southeast University|In partial label learning (PLL), each training sample is associated with a set of candidate labels, among which only one is valid. The core of PLL is to disambiguate the candidate labels to get the ground-truth one. In disambiguation, the existing works usually do not fully investigate the effectiveness of the non-candidate label set (a.k.a. complementary labels), which accurately indicates a set of labels that do not belong to a sample. In this paper, we use the non-candidate labels to induce a complementary classifier, which naturally forms an adversarial relationship against the traditional PLL classifier, to eliminate the false-positive labels in the candidate label set. Besides, we assume the feature space and the label space share the same local topological structure captured by a dynamic graph, and use it to assist disambiguation. Extensive experimental results validate the superiority of the proposed approach against state-of-the-art PLL methods on 4 controlled UCI data sets and 6 real-world data sets, and reveal the usefulness of complementary learning in PLL. The code has been released in the link https://github.com/Chongjie-Si/PL-CL.|在部分标签学习(PLL)中，每个训练样本都与一组候选标签相关联，其中只有一个是有效的。锁相环的核心是消除候选标签的歧义，从而得到地面真值。在消歧中，现有的作品通常没有充分考察非候选标签集(又称互补标签)的有效性，它准确地指出了一组不属于样本的标签。本文利用非候选标签诱导出一个与传统 PLL 分类器自然形成对抗关系的互补分类器，以消除候选标签集中的假阳性标签。此外，我们假设特征空间和标签空间共享一个动态图所捕获的相同的局部拓扑结构，并利用它来协助消歧。大量的实验结果验证了该方法在4个受控 UCI 数据集和6个实际数据集上与现有的锁相环方法相比的优越性，并揭示了补充学习在锁相环中的有效性。代码已经在链接 https://github.com/chongjie-si/pl-cl 中发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Complementary+Classifier+Induced+Partial+Label+Learning)|0|
|[Anomaly Detection with Score Distribution Discrimination](https://doi.org/10.1145/3580305.3599258)|Minqi Jiang, Songqiao Han, Hailiang Huang|Shanghai University of Finance and Economics|Recent studies give more attention to the anomaly detection (AD) methods that can leverage a handful of labeled anomalies along with abundant unlabeled data. These existing anomaly-informed AD methods rely on manually predefined score target(s), e.g., prior constant or margin hyperparameter(s), to realize discrimination in anomaly scores between normal and abnormal data. However, such methods would be vulnerable to the existence of anomaly contamination in the unlabeled data, and also lack adaptation to different data scenarios. In this paper, we propose to optimize the anomaly scoring function from the view of score distribution, thus better retaining the diversity and more fine-grained information of input data, especially when the unlabeled data contains anomaly noises in more practical AD scenarios. We design a novel loss function called Overlap loss that minimizes the overlap area between the score distributions of normal and abnormal samples, which no longer depends on prior anomaly score targets and thus acquires adaptability to various datasets. Overlap loss consists of Score Distribution Estimator and Overlap Area Calculation, which are introduced to overcome challenges when estimating arbitrary score distributions, and to ensure the boundness of training loss. As a general loss component, Overlap loss can be effectively integrated into multiple network architectures for constructing AD models. Extensive experimental results indicate that Overlap loss based AD models significantly outperform their state-of-the-art counterparts, and achieve better performance on different types of anomalies.|最近的研究更多地关注异常检测(AD)方法，这种方法可以利用一些标记的异常和大量未标记的数据。这些现有的反常信息 AD 方法依赖于人工预定义的评分目标，如先验常数或边界超参数，以实现正常数据和异常数据之间的异常评分判别。然而，这些方法容易受到未标记数据中异常污染的影响，并且缺乏对不同数据场景的适应性。本文提出从分数分布的角度对异常评分函数进行优化，从而更好地保留输入数据的多样性和更细粒度的信息，特别是当未标记数据中含有异常噪声时，在更实际的 AD 场景中更能保持这种多样性和更细粒度的信息。我们设计了一个新的丢失函数叫做重叠丢失，它使得正常和异常样本的分数分布之间的重叠区域最小化，不再依赖于先前的异常分数目标，从而获得了对各种数据集的适应性。重叠损失包括分数分布估计和重叠面积计算，它们被用来克服估计任意分数分布时的困难，并确保训练损失的有界性。重叠损耗作为一种通用损耗分量，可以有效地集成到多种网络结构中，用于构建 AD 模型。大量的实验结果表明，基于重叠损耗的 AD 模型显著优于其最先进的同类模型，并在不同类型的异常情况下取得更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Anomaly+Detection+with+Score+Distribution+Discrimination)|0|
|[CF-GODE: Continuous-Time Causal Inference for Multi-Agent Dynamical Systems](https://doi.org/10.1145/3580305.3599272)|Song Jiang, Zijie Huang, Xiao Luo, Yizhou Sun|University of California, Los Angeles|Multi-agent dynamical systems refer to scenarios where multiple units interact with each other and evolve collectively over time. To make informed decisions in multi-agent dynamical systems, such as determining the optimal vaccine distribution plan, it is essential for decision-makers to estimate the continuous-time counterfactual outcomes. However, existing studies of causal inference over time rely on the assumption that units are mutually independent, which is not valid for multi-agent dynamical systems. In this paper, we aim to bridge this gap and study how to estimate counterfactual outcomes in multi-agent dynamical systems. Causal inference in a multi-agent dynamical system has unique challenges: 1) Confounders are time-varying and are present in both individual unit covariates and those of other units; 2) Units are affected by not only their own but also others' treatments; 3) The treatments are naturally dynamic, such as receiving vaccines and boosters in a seasonal manner. We model a multi-agent dynamical system as a graph and propose CounterFactual GraphODE (CF-GODE), a causal model that estimates continuous-time counterfactual outcomes in the presence of inter-dependencies between units. To facilitate continuous-time estimation, we propose Treatment-Induced GraphODE, a novel ordinary differential equation based on GNN, which incorporates dynamical treatments as additional inputs to predict potential outcomes over time. To remove confounding bias, we propose two domain adversarial learning based objectives that learn balanced continuous representation trajectories, which are not predictive of treatments and interference. We further provide theoretical justification to prove their effectiveness. Experiments on two semi-synthetic datasets confirm that CF-GODE outperforms baselines on counterfactual estimation. We also provide extensive analyses to understand how our model works.|多代理动态系统是指多个单元相互作用并随着时间的推移共同发展的场景。在多智能体动态系统中，为了作出知情决策，如确定最优疫苗分配计划，决策者必须估计连续时间的反事实结果。然而，现有的因果推理研究都是基于单元之间相互独立的假设，这种假设对于多智能体动力系统是不成立的。本文主要研究多智能体动力系统中反事实结果的估计问题。多动力系统因果推理面临着独特的挑战: 1)混杂因素是随时间变化的，存在于个体单位协变量和其他单位的协变量中; 2)单位不仅受到自身治疗的影响，还受到其他单位治疗的影响; 3)治疗是自然动态的，例如以季节性方式接种疫苗和加强剂。我们将一个多动力系统模型建模为一个图形，并提出 CounterFact GraphODE (CF-GODE) ，这是一个因果模型，在单位之间存在相互依赖的情况下，估计连续时间的反事实结果。为了便于连续时间估计，我们提出了治疗诱导的 GraphoDE，这是一种基于 GNN 的新型常微分方程，它将动态治疗作为额外的输入来预测随着时间的推移的潜在结果。为了消除混杂偏差，我们提出了两个领域对抗性学习的目标，学习平衡连续表征轨迹，这是不预测治疗和干扰。我们进一步提供理论论证来证明它们的有效性。在两个半合成数据集上的实验证实了 CF-GODE 算法在反事实估计上优于基线算法。我们还提供了广泛的分析，以了解我们的模型是如何工作的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CF-GODE:+Continuous-Time+Causal+Inference+for+Multi-Agent+Dynamical+Systems)|0|
|[FedSkill: Privacy Preserved Interpretable Skill Learning via Imitation](https://doi.org/10.1145/3580305.3599349)|Yushan Jiang, Wenchao Yu, Dongjin Song, Lu Wang, Wei Cheng, Haifeng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedSkill:+Privacy+Preserved+Interpretable+Skill+Learning+via+Imitation)|0|
|[Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks](https://doi.org/10.1145/3580305.3599376)|Bowen Jin, Yu Zhang, Qi Zhu, Jiawei Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heterformer:+Transformer-based+Deep+Node+Representation+Learning+on+Heterogeneous+Text-Rich+Networks)|0|
|[Transferable Graph Structure Learning for Graph-based Traffic Forecasting Across Cities](https://doi.org/10.1145/3580305.3599529)|Yilun Jin, Kai Chen, Qiang Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Transferable+Graph+Structure+Learning+for+Graph-based+Traffic+Forecasting+Across+Cities)|0|
|[When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting](https://doi.org/10.1145/3580305.3599547)|Harshavardhan Kamarthi, Lingkai Kong, Alexander Rodríguez, Chao Zhang, B. Aditya Prakash||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Rigidity+Hurts:+Soft+Consistency+Regularization+for+Probabilistic+Hierarchical+Time+Series+Forecasting)|0|
|[R-Mixup: Riemannian Mixup for Biological Networks](https://doi.org/10.1145/3580305.3599483)|Xuan Kan, Zimu Li, Hejie Cui, Yue Yu, Ran Xu, Shaojun Yu, Zilong Zhang, Ying Guo, Carl Yang|University of International Business and Economics; Emory University; Georgia Institute of Technology; University of Chicago|Biological networks are commonly used in biomedical and healthcare domains to effectively model the structure of complex biological systems with interactions linking biological entities. However, due to their characteristics of high dimensionality and low sample size, directly applying deep learning models on biological networks usually faces severe overfitting. In this work, we propose R-MIXUP, a Mixup-based data augmentation technique that suits the symmetric positive definite (SPD) property of adjacency matrices from biological networks with optimized training efficiency. The interpolation process in R-MIXUP leverages the log-Euclidean distance metrics from the Riemannian manifold, effectively addressing the swelling effect and arbitrarily incorrect label issues of vanilla Mixup. We demonstrate the effectiveness of R-MIXUP with five real-world biological network datasets on both regression and classification tasks. Besides, we derive a commonly ignored necessary condition for identifying the SPD matrices of biological networks and empirically study its influence on the model performance. The code implementation can be found in Appendix E.|生物网络通常用于生物医学和医疗保健领域，以有效地建模复杂的生物系统的结构与连接生物实体的相互作用。然而，由于生物网络具有高维数、低样本量的特点，直接将深度学习模型应用于生物网络往往面临严重的过拟合问题。在这项工作中，我们提出了 R-MIXUP，一种基于混合的数据增强技术，适合于对称正定(SPD)性质的邻接矩阵从生物网络与优化训练效率。在 R-MIXUP 中的插值过程利用了来自黎曼流形的 log-Euclidean 距离度量，有效地解决了香草 Mixup 的膨胀效应和任意错误的标签问题。我们用五个真实世界的生物网络数据集在回归和分类任务上证明了 R-MIXUP 的有效性。此外，我们推导了一个常被忽略的识别生物网络 SPD 矩阵的必要条件，并对其对模型性能的影响进行了实证研究。代码实现可以在附录 E 中找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=R-Mixup:+Riemannian+Mixup+for+Biological+Networks)|0|
|[Task-Equivariant Graph Few-shot Learning](https://doi.org/10.1145/3580305.3599515)|Sungwon Kim, Junseok Lee, Namkyeong Lee, Wonjoong Kim, Seungyoon Choi, Chanyoung Park|KAIST|Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strategies using a limited number of training meta-tasks, allowing it to acquire meta-knowledge for a wide range of meta-tasks. By incorporating equivariant neural networks, TEG can utilize their strong generalization abilities to learn highly adaptable task-specific strategies. As a result, TEG achieves state-of-the-art performance with limited training meta-tasks. Our experiments on various benchmark datasets demonstrate TEG's superiority in terms of accuracy and generalization ability, even when using minimal meta-training data, highlighting the effectiveness of our proposed approach in addressing the challenges of meta-learning based few-shot node classification. Our code is available at the following link: https://github.com/sung-won-kim/TEG|尽管图神经网络(GNN)在节点分类任务中取得了成功，但它的性能在很大程度上依赖于每个类有足够数量的标记节点。在实际情况中，并不是所有的类都有许多带标签的节点，而且可能存在模型需要对新类进行分类的实例，这使得手动标签变得困难。为了解决这个问题，GNN 必须能够对标记节点数量有限的节点进行分类，这就是所谓的少镜头节点分类。以往的基于情节的元学习方法已经证明了在少镜头节点分类中的成功，但是我们的研究结果表明，只有通过大量不同的训练元任务才能获得最佳性能。为了解决基于元学习的少镜头学习(FSL)面临的挑战，我们提出了一种新的学习方法——任务等变图少镜头学习(TEG)框架。我们的 TEG 框架使模型能够使用有限数量的训练元任务学习可转移的任务适应策略，允许模型获取广泛元任务的元知识。通过结合等变神经网络，TEG 可以利用其强大的泛化能力来学习高度适应性的任务特定策略。因此，TEG 在有限的训练元任务的情况下达到了最高水平的表现。我们在各种基准数据集上的实验证明了 TEG 在准确性和泛化能力方面的优势，即使使用最小的元训练数据，突出了我们提出的方法在解决基于元学习的少镜头节点分类的挑战方面的有效性。我们的代码可在以下连结下载:  https://github.com/sung-won-kim/teg|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Task-Equivariant+Graph+Few-shot+Learning)|0|
|[Deception by Omission: Using Adversarial Missingness to Poison Causal Structure Learning](https://doi.org/10.1145/3580305.3599297)|Deniz Koyuncu, Alex Gittens, Bülent Yener, Moti Yung|Rensselaer Polytechnic Institute; Google LLC|Inference of causal structures from observational data is a key component of causal machine learning; in practice, this data may be incompletely observed. Prior work has demonstrated that adversarial perturbations of completely observed training data may be used to force the learning of inaccurate causal structural models (SCMs). However, when the data can be audited for correctness (e.g., it is crytographically signed by its source), this adversarial mechanism is invalidated. This work introduces a novel attack methodology wherein the adversary deceptively omits a portion of the true training data to bias the learned causal structures in a desired manner. Theoretically sound attack mechanisms are derived for the case of arbitrary SCMs, and a sample-efficient learning-based heuristic is given for Gaussian SCMs. Experimental validation of these approaches on real and synthetic data sets demonstrates the effectiveness of adversarial missingness attacks at deceiving popular causal structure learning algorithms.|从观测数据推断因果结构是因果机器学习的一个关键组成部分，在实践中，这些数据可能不完全被观测到。先前的工作已经证明，完全观察到的训练数据的对抗性扰动可能被用来强制学习不准确的因果结构模型(SCM)。然而，当数据可以被审计为正确(例如，它是由它的来源签名) ，这个对抗机制是无效的。这项工作介绍了一种新的攻击方法，其中对手欺骗性省略了一部分真实的训练数据，以偏向学习的因果结构在一个理想的方式。从理论上推导了任意单片机情况下的声音攻击机制，并给出了高斯单片机的一种基于样本有效学习的启发式算法。这些方法在真实和合成数据集上的实验验证证明了对抗性缺失攻击在欺骗流行的因果结构学习算法上的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deception+by+Omission:+Using+Adversarial+Missingness+to+Poison+Causal+Structure+Learning)|0|
|[Optimizing Traffic Control with Model-Based Learning: A Pessimistic Approach to Data-Efficient Policy Inference](https://doi.org/10.1145/3580305.3599459)|Mayuresh Kunjir, Sanjay Chawla, Siddarth Chandrasekar, Devika Jay, Balaraman Ravindran||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Traffic+Control+with+Model-Based+Learning:+A+Pessimistic+Approach+to+Data-Efficient+Policy+Inference)|0|
|[Shift-Robust Molecular Relational Learning with Causal Substructure](https://doi.org/10.1145/3580305.3599437)|Namkyeong Lee, Kanghoon Yoon, Gyoung S. Na, Sein Kim, Chanyoung Park|KRICT; KAIST|Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and synthetic datasets demonstrate the superiority of CMRL over state-of-the-art baseline models. Our code is available at https://github.com/Namkyeong/CMRL.|近年来，以预测分子间相互作用行为为目标的分子关系学习由于其广泛的应用而引起了分子科学界的极大兴趣。在这项工作中，我们提出的 CMRL 是鲁棒的分子关系学习的分布转移，通过检测核心子结构的因果关系相关的化学反应。为了做到这一点，我们首先假设一个因果关系的基础上的领域知识的分子科学和建立一个结构的因果模型(SCM) ，揭示变量之间的关系。在 SCM 的基础上，我们引入了一种新的条件干预框架，其干预以配对分子为条件。在条件干预框架下，我们的模型成功地从因果子结构中学习，并减轻了与化学反应虚假相关的捷径子结构的混杂效应。通过对现实世界和合成数据集的各种任务的大量实验，证明了 CMRL 相对于最先进的基线模型的优越性。我们的代码可以在 https://github.com/namkyeong/cmrl 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shift-Robust+Molecular+Relational+Learning+with+Causal+Substructure)|0|
|[Boosting Multitask Learning on Graphs through Higher-Order Task Affinities](https://doi.org/10.1145/3580305.3599265)|Dongyue Li, Haotian Ju, Aneesh Sharma, Hongyang R. Zhang|Google; Northeastern University|Predicting node labels on a given graph is a widely studied problem with many applications, including community detection and molecular graph prediction. This paper considers predicting multiple node labeling functions on graphs simultaneously and revisits this problem from a multitask learning perspective. For a concrete example, consider overlapping community detection: each community membership is a binary node classification task. Due to complex overlapping patterns, we find that negative transfer is prevalent when we apply naive multitask learning to multiple community detection, as task relationships are highly nonlinear across different node labeling. To address the challenge, we develop an algorithm to cluster tasks into groups based on a higher-order task affinity measure. We then fit a multitask model on each task group, resulting in a boosting procedure on top of the baseline model. We estimate the higher-order task affinity measure between two tasks as the prediction loss of one task in the presence of another task and a random subset of other tasks. Then, we use spectral clustering on the affinity score matrix to identify task grouping. We design several speedup techniques to compute the higher-order affinity scores efficiently and show that they can predict negative transfers more accurately than pairwise task affinities. We validate our procedure using various community detection and molecular graph prediction data sets, showing favorable results compared with existing methods. Lastly, we provide a theoretical analysis to show that under a planted block model of tasks on graphs, our affinity scores can provably separate tasks into groups.|对给定图上的节点标签进行预测是一个有着广泛应用前景的问题，包括社区检测和分子图预测。本文从多任务学习的角度研究了图上多节点标记函数的同时预测问题。对于一个具体的例子，考虑重叠社区检测: 每个社区成员是一个二进制节点分类任务。由于复杂的重叠模式，我们发现负迁移在幼稚多任务学习应用于多社区检测时是普遍存在的，因为不同节点间的任务关系是高度非线性的。为了解决这一问题，我们提出了一种基于高阶任务亲和度的任务分组算法。然后，我们在每个任务组上安装一个多任务模型，从而在基线模型之上形成一个推进过程。我们将两个任务之间的高阶任务亲和度估计为一个任务在另一个任务存在时的预测损失和其他任务的随机子集。然后，我们使用亲和力得分矩阵上的 SVD 来识别任务分组。我们设计了几种加速技术来有效地计算高阶亲和力得分，并表明它们能比成对任务亲和力更准确地预测负向转移。我们验证了我们的程序使用各种社区检测和分子图预测数据集，显示良好的结果与现有的方法相比。最后，我们提供了一个理论分析，表明在图上的任务种植块模型下，我们的亲和力得分可以证明任务分组。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Boosting+Multitask+Learning+on+Graphs+through+Higher-Order+Task+Affinities)|0|
|[Interpretable Sparsification of Brain Graphs: Better Practices and Effective Designs for Graph Neural Networks](https://doi.org/10.1145/3580305.3599394)|Gaotang Li, Marlena Duda, Xiang Zhang, Danai Koutra, Yujun Yan|Georgia State University; Dartmouth College; University of Michigan, Ann Arbor; University of North Carolina, Charlotte|Brain graphs, which model the structural and functional relationships between brain regions, are crucial in neuroscientific and clinical applications involving graph classification. However, dense brain graphs pose computational challenges including high runtime and memory usage and limited interpretability. In this paper, we investigate effective designs in Graph Neural Networks (GNNs) to sparsify brain graphs by eliminating noisy edges. While prior works remove noisy edges based on explainability or task-irrelevant properties, their effectiveness in enhancing performance with sparsified graphs is not guaranteed. Moreover, existing approaches often overlook collective edge removal across multiple graphs.   To address these issues, we introduce an iterative framework to analyze different sparsification models. Our findings are as follows: (i) methods prioritizing interpretability may not be suitable for graph sparsification as they can degrade GNNs' performance in graph classification tasks; (ii) simultaneously learning edge selection with GNN training is more beneficial than post-training; (iii) a shared edge selection across graphs outperforms separate selection for each graph; and (iv) task-relevant gradient information aids in edge selection. Based on these insights, we propose a new model, Interpretable Graph Sparsification (IGS), which enhances graph classification performance by up to 5.1% with 55.0% fewer edges. The retained edges identified by IGS provide neuroscientific interpretations and are supported by well-established literature.|脑图，模拟大脑区域之间的结构和功能关系，在涉及图形分类的神经科学和临床应用中是至关重要的。然而，密集的大脑图形带来了计算上的挑战，包括高运行时间和内存使用率以及有限的可解释性。本文研究了图神经网络(GNN)中通过去除噪声边缘来稀疏大脑图形的有效设计方法。先前的工作基于可解释性或任务无关性质去除噪声边缘，但其在稀疏图增强性能方面的有效性并不能得到保证。此外，现有的方法往往忽略了跨多个图的集体边缘去除。为了解决这些问题，我们引入了一个迭代框架来分析不同的稀疏化模型。我们的研究结果如下: (i)优先考虑可解释性的方法可能不适合于图的稀疏化，因为它们可能会降低 GNN 在图分类任务中的表现; (ii)同时学习 GNN 训练的边缘选择比训练后更有益; (iii)跨图的共享边缘选择优于每个图的单独选择; 和(iv)任务相关的梯度信息有助于边缘选择。在此基础上，我们提出了一种新的图分类模型——可解释图稀疏化(IGS)模型，该模型将图分类性能提高了5.1% ，边数减少了55.0% 。由 IGS 确定的保留边缘提供了神经科学的解释，并得到了成熟文献的支持。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretable+Sparsification+of+Brain+Graphs:+Better+Practices+and+Effective+Designs+for+Graph+Neural+Networks)|0|
|[What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders](https://doi.org/10.1145/3580305.3599546)|Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu, Changhua Meng, Zibin Zheng, Weiqiang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What's+Behind+the+Mask:+Understanding+Masked+Graph+Modeling+for+Graph+Autoencoders)|0|
|[OPORP: One Permutation + One Random Projection](https://doi.org/10.1145/3580305.3599457)|Ping Li, Xiaoyun Li|LinkedIn Ads|Consider two $D$-dimensional data vectors (e.g., embeddings): $u, v$. In many embedding-based retrieval (EBR) applications where the vectors are generated from trained models, $D=256\sim 1024$ are common. In this paper, OPORP (one permutation + one random projection) uses a variant of the ``count-sketch'' type of data structures for achieving data reduction/compression. With OPORP, we first apply a permutation on the data vectors. A random vector $r$ is generated i.i.d. with moments: $E(r_i) = 0, E(r_i^2)=1, E(r_i^3) =0, E(r_i^4)=s$. We multiply (as dot product) $r$ with all permuted data vectors. Then we break the $D$ columns into $k$ equal-length bins and aggregate (i.e., sum) the values in each bin to obtain $k$ samples from each data vector. One crucial step is to normalize the $k$ samples to the unit $l_2$ norm. We show that the estimation variance is essentially: $(s-1)A + \frac{D-k}{D-1}\frac{1}{k}\left[ (1-\rho^2)^2 -2A\right]$, where $A\geq 0$ is a function of the data ($u,v$). This formula reveals several key properties: (1) We need $s=1$. (2) The factor $\frac{D-k}{D-1}$ can be highly beneficial in reducing variances. (3) The term $\frac{1}{k}(1-\rho^2)^2$ is actually the asymptotic variance of the classical correlation estimator.   We illustrate that by letting the $k$ in OPORP to be $k=1$ and repeat the procedure $m$ times, we exactly recover the work of ``very spars random projections'' (VSRP). This immediately leads to a normalized estimator for VSRP which substantially improves the original estimator of VSRP.   In summary, with OPORP, the two key steps: (i) the normalization and (ii) the fixed-length binning scheme, have considerably improved the accuracy in estimating the cosine similarity, which is a routine (and crucial) task in modern embedding-based retrieval (EBR) applications.|考虑两个 $D $- 维数据向量(例如，嵌入) : $u，v $。在许多基于嵌入的检索(EBR)应用中，向量是由训练好的模型生成的，$D = 256 sim 1024 $是常见的。OPORP (一个置换 + 一个随机投影)采用“计数-示意图”类型的数据结构的一种变体来实现数据约简/压缩。使用 OPORP，我们首先对数据向量进行排列。生成一个随机向量 $r $，其矩为: $E (r _ i) = 0，E (r _ i ^ 2) = 1，E (r _ i ^ 3) = 0，E (r _ i ^ 4) = s $。我们用所有置换的数据向量乘以(作为点乘) $r $。然后，我们将 $D $列分解成 $k $等长的容器并聚合(即和)每个容器中的值，以从每个数据向量获取 $k $样本。一个关键的步骤是将 $k $样本标准化为单位 $l _ 2 $范数。我们证明了估计方差本质上是: $(s-1) A + frac { D-k }{ D-1} frac {1}{ k } left [(1-rho ^ 2) ^ 2 -2 A right ] $，其中 $A geq 0 $是数据的函数($u，v $)。这个公式揭示了几个关键性质: (1)我们需要 $s = 1 $。(2)因子 $frac { D-k }{ D-1} $在减少方差方面是非常有益的。(3)项 $frac {1}{ k }(1-rho ^ 2) ^ 2 $实际上是经典相关估计的渐近方差。我们举例说明，通过让 OPORP 中的 $k $为 $k = 1 $并重复过程 $m $次，我们可以精确地恢复“ very spars 随机投影”(VSRP)的工作。这立即导致了 VSRP 的归一化估计，从而大大改进了 VSRP 的原始估计。总之，使用 OPORP，两个关键步骤: (i)归一化和(ii)固定长度分组方案，大大提高了估计余弦距离的准确性，这是现代嵌入式检索(EBR)应用中的一个常规(和关键)任务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OPORP:+One+Permutation+++One+Random+Projection)|0|
|[Multi-Temporal Relationship Inference in Urban Areas](https://doi.org/10.1145/3580305.3599440)|Shuangli Li, Jingbo Zhou, Ji Liu, Tong Xu, Enhong Chen, Hui Xiong|Baidu Research; University of Science and Technology of China; The Hong Kong University of Science and Technology (Guangzhou)|Finding multiple temporal relationships among locations can benefit a bunch of urban applications, such as dynamic offline advertising and smart public transport planning. While some efforts have been made on finding static relationships among locations, little attention is focused on studying time-aware location relationships. Indeed, abundant location-based human activities are time-varying and the availability of these data enables a new paradigm for understanding the dynamic relationships in a period among connective locations. To this end, we propose to study a new problem, namely multi-Temporal relationship inference among locations (Trial for short), where the major challenge is how to integrate dynamic and geographical influence under the relationship sparsity constraint. Specifically, we propose a solution to Trial with a graph learning scheme, which includes a spatially evolving graph neural network (SEENet) with two collaborative components: spatially evolving graph convolution module (SEConv) and spatially evolving self-supervised learning strategy (SE-SSL). SEConv performs the intra-time aggregation and inter-time propagation to capture the multifaceted spatially evolving contexts from the view of location message passing. In addition, SE-SSL designs time-aware self-supervised learning tasks in a global-local manner with additional evolving constraint to enhance the location representation learning and further handle the relationship sparsity. Finally, experiments on four real-world datasets demonstrate the superiority of our method over several state-of-the-art approaches.|寻找位置之间的多个时间关系可以有利于一系列城市应用，如动态离线广告和智能公共交通规划。虽然在寻找位置之间的静态关系方面已经做了一些努力，但是很少有人关注时间感知的位置关系的研究。事实上，丰富的基于位置的人类活动是随时间变化的，这些数据的可用性为理解连通位置之间一个时期的动态关系提供了一个新的范例。为此，我们提出了一个新的研究问题，即多时间地点间关系推理(简称“试用”) ，其主要挑战是如何在关系稀疏约束下整合动态和地理影响。具体来说，我们提出了一种基于图形学习方案的解决方案，该方案包括一个具有两个协作组件的空间演化图形神经网络(SEENet) : 空间演化图卷积模块(SEConv)和空间演化自监督学习策略(SE-SSL)。SEConv 执行时间内聚合和时间间传播，从位置消息传递的角度捕获多方面的空间演化上下文。此外，SE-SSL 以全局-局部方式设计具有时间感知的自监督学习任务，并附加进化约束，以增强位置表示学习，进一步处理关系稀疏性。最后，在四个真实世界数据集上的实验证明了我们的方法优于几种最先进的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Temporal+Relationship+Inference+in+Urban+Areas)|0|
|[HomoGCL: Rethinking Homophily in Graph Contrastive Learning](https://doi.org/10.1145/3580305.3599380)|WenZhi Li, ChangDong Wang, Hui Xiong, JianHuang Lai|Sun Yat-sen University; The Hong Kong University of Science and Technology (Guangzhou)|Contrastive learning (CL) has become the de-facto learning paradigm in self-supervised learning on graphs, which generally follows the "augmenting-contrasting" learning scheme. However, we observe that unlike CL in computer vision domain, CL in graph domain performs decently even without augmentation. We conduct a systematic analysis of this phenomenon and argue that homophily, i.e., the principle that "like attracts like", plays a key role in the success of graph CL. Inspired to leverage this property explicitly, we propose HomoGCL, a model-agnostic framework to expand the positive set using neighbor nodes with neighbor-specific significances. Theoretically, HomoGCL introduces a stricter lower bound of the mutual information between raw node features and node embeddings in augmented views. Furthermore, HomoGCL can be combined with existing graph CL models in a plug-and-play way with light extra computational overhead. Extensive experiments demonstrate that HomoGCL yields multiple state-of-the-art results across six public datasets and consistently brings notable performance improvements when applied to various graph CL methods. Code is avilable at https://github.com/wenzhilics/HomoGCL.|对比学习已成为图形自监督学习的事实学习范式，一般遵循“增强-对比”学习模式。然而，我们观察到，与计算机视觉领域的 CL 不同，图形领域的 CL 在没有增强的情况下仍然表现良好。我们对这一现象进行了系统的分析，认为同调性，即“相似吸引相似”的原则，在图 CL 的成功中起着关键作用。受此启发，我们提出了 HomoGCL，一个模型不可知的框架，使用具有邻居特定意义的邻居节点来扩展正集。理论上，HomoGCL 在增广视图中引入了更严格的原始节点特征和节点嵌入之间的互信息下界。此外，HomoGCL 还可以以即插即用的方式与现有的图形 CL 模型结合，从而减少额外的计算开销。大量的实验表明，HomoGCL 在六个公共数据集中产生了多个最先进的结果，并且在应用于各种图形 CL 方法时始终带来显著的性能改进。代码可在 https://github.com/wenzhilics/homogcl 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HomoGCL:+Rethinking+Homophily+in+Graph+Contrastive+Learning)|0|
|[Urban Region Representation Learning with OpenStreetMap Building Footprints](https://doi.org/10.1145/3580305.3599538)|Yi Li, Weiming Huang, Gao Cong, Hao Wang, Zheng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Urban+Region+Representation+Learning+with+OpenStreetMap+Building+Footprints)|0|
|[Machine Unlearning in Gradient Boosting Decision Trees](https://doi.org/10.1145/3580305.3599420)|Huawei Lin, Jun Woo Chung, Yingjie Lao, Weijie Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Machine+Unlearning+in+Gradient+Boosting+Decision+Trees)|0|
|[Fire: An Optimization Approach for Fast Interpretable Rule Extraction](https://doi.org/10.1145/3580305.3599353)|Brian Liu, Rahul Mazumder|Massachusetts Institute of Technology|We present FIRE, Fast Interpretable Rule Extraction, an optimization-based framework to extract a small but useful collection of decision rules from tree ensembles. FIRE selects sparse representative subsets of rules from tree ensembles, that are easy for a practitioner to examine. To further enhance the interpretability of the extracted model, FIRE encourages fusing rules during selection, so that many of the selected decision rules share common antecedents. The optimization framework utilizes a fusion regularization penalty to accomplish this, along with a non-convex sparsity-inducing penalty to aggressively select rules. Optimization problems in FIRE pose a challenge to off-the-shelf solvers due to problem scale and the non-convexity of the penalties. To address this, making use of problem-structure, we develop a specialized solver based on block coordinate descent principles; our solver performs up to 40x faster than existing solvers. We show in our experiments that FIRE outperforms state-of-the-art rule ensemble algorithms at building sparse rule sets, and can deliver more interpretable models compared to existing methods.|我们提出 FIRE，快速解释规则提取，一个基于优化的框架，提取一个小但有用的决策规则集合从树集合。FIRE 从树集合中选择稀疏的代表性规则子集，这对于从业者来说很容易检查。为了进一步提高提取模型的可解释性，FIRE 鼓励在选择过程中融合规则，使得所选择的决策规则具有共同的前因。优化框架利用融合正则化惩罚来实现这一点，同时利用非凸稀疏诱导惩罚来积极选择规则。FIRE 中的优化问题由于问题的规模和处罚的非凸性而对现成的求解器提出了挑战。为了解决这个问题，利用问题结构，我们开发了一个基于块坐标下降法原则的专业解决方案，我们的解决方案比现有的解决方案快40倍。在实验中，我们发现 FIRE 在构建稀疏规则集方面优于最先进的规则集合算法，并且与现有的方法相比，它能够提供更多可解释的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fire:+An+Optimization+Approach+for+Fast+Interpretable+Rule+Extraction)|0|
|[Robust Spatiotemporal Traffic Forecasting with Reinforced Dynamic Adversarial Training](https://doi.org/10.1145/3580305.3599492)|Fan Liu, Weijia Zhang, Hao Liu|The Hong Kong University of Science and Technology (Guangzhou)|Machine learning-based forecasting models are commonly used in Intelligent Transportation Systems (ITS) to predict traffic patterns and provide city-wide services. However, most of the existing models are susceptible to adversarial attacks, which can lead to inaccurate predictions and negative consequences such as congestion and delays. Therefore, improving the adversarial robustness of these models is crucial for ITS. In this paper, we propose a novel framework for incorporating adversarial training into spatiotemporal traffic forecasting tasks. We demonstrate that traditional adversarial training methods designated for static domains cannot be directly applied to traffic forecasting tasks, as they fail to effectively defend against dynamic adversarial attacks. Then, we propose a reinforcement learning-based method to learn the optimal node selection strategy for adversarial examples, which simultaneously strengthens the dynamic attack defense capability and reduces the model overfitting. Additionally, we introduce a self-knowledge distillation regularization module to overcome the "forgetting issue" caused by continuously changing adversarial nodes during training. We evaluate our approach on two real-world traffic datasets and demonstrate its superiority over other baselines. Our method effectively enhances the adversarial robustness of spatiotemporal traffic forecasting models. The source code for our framework is available at https://github.com/usail-hkust/RDAT.|基于机器学习的预测模型是智能交通系统(ITS)中常用的预测交通模式和提供城市服务的模型。然而，现有的大多数模型容易受到对抗性攻击，这可能导致不准确的预测和负面后果，如拥挤和延迟。因此，提高这些模型的对抗鲁棒性是智能交通系统的关键。在本文中，我们提出了一个新的框架结合对抗训练的时空交通预测任务。我们证明了传统的针对静态域的对抗性训练方法不能直接应用于流量预测任务，因为它们不能有效地防御动态对抗性攻击。然后，提出了一种基于强化学习的方法来学习对手实例的最优节点选择策略，同时增强了动态攻击防御能力，减少了模型的过拟合。此外，为了克服训练过程中对手节点不断变化所引起的“遗忘问题”，本文还引入了自知识精馏正则化模型。我们评估了我们的方法在两个真实世界的交通数据集，并证明了其优势超过其他基线。该方法有效地增强了时空流量预测模型的对抗鲁棒性。我们框架的源代码可在 https://github.com/usail-hkust/rdat 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Spatiotemporal+Traffic+Forecasting+with+Reinforced+Dynamic+Adversarial+Training)|0|
|[Discovering Dynamic Causal Space for DAG Structure Learning](https://doi.org/10.1145/3580305.3599309)|Fangfu Liu, Wenchang Ma, An Zhang, Xiang Wang, Yueqi Duan, TatSeng Chua|Tsinghua University; University of Science and Technology of China; National University of Singapore|Discovering causal structure from purely observational data (i.e., causal discovery), aiming to identify causal relationships among variables, is a fundamental task in machine learning. The recent invention of differentiable score-based DAG learners is a crucial enabler, which reframes the combinatorial optimization problem into a differentiable optimization with a DAG constraint over directed graph space. Despite their great success, these cutting-edge DAG learners incorporate DAG-ness independent score functions to evaluate the directed graph candidates, lacking in considering graph structure. As a result, measuring the data fitness alone regardless of DAG-ness inevitably leads to discovering suboptimal DAGs and model vulnerabilities. Towards this end, we propose a dynamic causal space for DAG structure learning, coined CASPER, that integrates the graph structure into the score function as a new measure in the causal space to faithfully reflect the causal distance between estimated and ground truth DAG. CASPER revises the learning process as well as enhances the DAG structure learning via adaptive attention to DAG-ness. Grounded by empirical visualization, CASPER, as a space, satisfies a series of desired properties, such as structure awareness and noise robustness. Extensive experiments on both synthetic and real-world datasets clearly validate the superiority of our CASPER over the state-of-the-art causal discovery methods in terms of accuracy and robustness.|从纯观察数据中发现因果结构(即因果发现) ，目的是确定变量之间的因果关系，是机器学习的基本任务。最近发明的基于可微分数的 DAG 学习器是一个关键的推动者，它将组合优化问题重新定义为在有向图空间上带有 DAG 约束的可微优化问题。这些前沿的 DAG 学习者虽然取得了很大的成功，但是他们缺乏对图结构的考虑，因此在评价有向图候选者时引入了独立的 DAG 评分函数。因此，单独测量数据适应性而不考虑 DAG 的性质，不可避免地会发现次优的 DAG 和模型漏洞。为此，我们提出了 DAG 结构学习的动态因果空间，称为 CASPER，它将图结构集成到评分函数中，作为因果空间中的一个新度量，以忠实地反映估计的 DAG 和地面真值之间的因果距离。CASPER 通过对 DAG 性质的自适应注意，修正了学习过程，提高了 DAG 结构的学习效率。CASPER 作为一个空间，以经验可视化为基础，满足结构感知和噪声鲁棒性等一系列要求。在合成和真实世界数据集上的大量实验清楚地验证了我们的 CASPER 在准确性和鲁棒性方面优于最先进的因果发现方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discovering+Dynamic+Causal+Space+for+DAG+Structure+Learning)|0|
|[Semi-Supervised Graph Imbalanced Regression](https://doi.org/10.1145/3580305.3599497)|Gang Liu, Tong Zhao, Eric Inae, Tengfei Luo, Meng Jiang|Snap Inc.; University of Notre Dame|Data imbalance is easily found in annotated data when the observations of certain continuous label values are difficult to collect for regression tasks. When they come to molecule and polymer property predictions, the annotated graph datasets are often small because labeling them requires expensive equipment and effort. To address the lack of examples of rare label values in graph regression tasks, we propose a semi-supervised framework to progressively balance training data and reduce model bias via self-training. The training data balance is achieved by (1) pseudo-labeling more graphs for under-represented labels with a novel regression confidence measurement and (2) augmenting graph examples in latent space for remaining rare labels after data balancing with pseudo-labels. The former is to identify quality examples from unlabeled data whose labels are confidently predicted and sample a subset of them with a reverse distribution from the imbalanced annotated data. The latter collaborates with the former to target a perfect balance using a novel label-anchored mixup algorithm. We perform experiments in seven regression tasks on graph datasets. Results demonstrate that the proposed framework significantly reduces the error of predicted graph properties, especially in under-represented label areas.|当某些连续标号值的观测值难以收集用于回归任务时，注释数据中很容易出现数据不平衡。当涉及到分子和聚合物特性预测时，注释图表数据集通常很小，因为标记它们需要昂贵的设备和努力。针对图形回归任务中缺乏稀有标号值的问题，提出了一种半监督框架，通过自学习逐步平衡训练数据，减少模型偏差。训练数据的平衡是通过: (1)使用一种新的回归置信度度量方法对未被充分表示的标签进行伪标记，从而获得更多的图; (2)使用伪标记进行数据平衡后，在潜在空间中增加剩余稀有标签的图示例。前者是从标签可信预测的未标记数据中识别出高质量样本，并从不平衡的注释数据中采用逆向分布对其子集进行抽样。后者与前者协作，使用一种新的标签锚定混合算法来实现完美的平衡。我们在图形数据集上进行七个回归任务的实验。结果表明，该框架显著降低了预测图性质的误差，特别是在表示不足的标号区域。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semi-Supervised+Graph+Imbalanced+Regression)|0|
|[Enhancing Graph Representations Learning with Decorrelated Propagation](https://doi.org/10.1145/3580305.3599334)|Hua Liu, Haoyu Han, Wei Jin, Xiaorui Liu, Hui Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Graph+Representations+Learning+with+Decorrelated+Propagation)|0|
|[Guiding Mathematical Reasoning via Mastering Commonsense Formula Knowledge](https://doi.org/10.1145/3580305.3599375)|Jiayu Liu, Zhenya Huang, Zhiyuan Ma, Qi Liu, Enhong Chen, Tianhuang Su, Haifeng Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Guiding+Mathematical+Reasoning+via+Mastering+Commonsense+Formula+Knowledge)|0|
|[Using Motif Transitions for Temporal Graph Generation](https://doi.org/10.1145/3580305.3599540)|Penghang Liu, Ahmet Erdem Sariyüce||Graph generative models are highly important for sharing surrogate data and benchmarking purposes. Real-world complex systems often exhibit dynamic nature, where the interactions among nodes change over time in the form of a temporal network. Most temporal network generation models extend the static graph generation models by incorporating temporality in the generation process. More recently, temporal motifs are used to generate temporal networks with better success. However, existing models are often restricted to a small set of predefined motif patterns due to the high computational cost of counting temporal motifs. In this work, we develop a practical temporal graph generator, Motif Transition Model (MTM), to generate synthetic temporal networks with realistic global and local features. Our key idea is modeling the arrival of new events as temporal motif transition processes. We first calculate the transition properties from the input graph and then simulate the motif transition processes based on the transition probabilities and transition rates. We demonstrate that our model consistently outperforms the baselines with respect to preserving various global and local temporal graph statistics and runtime performance.|图形生成模型对于共享代理数据和基准测试非常重要。现实世界中的复杂系统往往表现出动态特性，其中节点之间的交互作用随时间以时间网络的形式发生变化。大多数时态网络生成模型通过在生成过程中引入时态性来扩展静态图生成模型。最近，时间模式被用来生成更好的时间网络。然而，由于计算时间模式的计算成本较高，现有模型往往局限于一小组预定义的模式。在这项工作中，我们开发了一个实用的时态图生成器，基元转换模型(MTM) ，以生成具有真实的全局和局部特征的合成时态网络。我们的关键思想是将新事件的到来建模为时序转换过程。我们首先从输入图中计算出转换特性，然后基于转换概率和转换速率模拟基序转换过程。我们证明了我们的模型在保持各种全局和局部时态图统计和运行时性能方面始终优于基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+Motif+Transitions+for+Temporal+Graph+Generation)|0|
|[Fairness-Aware Continuous Predictions of Multiple Analytics Targets in Dynamic Networks](https://doi.org/10.1145/3580305.3599341)|Ruifeng Liu, Qu Liu, Tingjian Ge||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness-Aware+Continuous+Predictions+of+Multiple+Analytics+Targets+in+Dynamic+Networks)|0|
|[Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipschitz Restraint](https://doi.org/10.1145/3580305.3599299)|Wei Liu, Jun Wang, Haozhao Wang, Ruixuan Li, Yang Qiu, Yuankai Zhang, Jie Han, Yixiong Zou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoupled+Rationalization+with+Asymmetric+Learning+Rates:+A+Flexible+Lipschitz+Restraint)|0|
|[FLOOD: A Flexible Invariant Learning Framework for Out-of-Distribution Generalization on Graphs](https://doi.org/10.1145/3580305.3599355)|Yang Liu, Xiang Ao, Fuli Feng, Yunshan Ma, Kuan Li, TatSeng Chua, Qing He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FLOOD:+A+Flexible+Invariant+Learning+Framework+for+Out-of-Distribution+Generalization+on+Graphs)|0|
|[QTIAH-GNN: Quantity and Topology Imbalance-aware Heterogeneous Graph Neural Network for Bankruptcy Prediction](https://doi.org/10.1145/3580305.3599479)|Yucheng Liu, Zipeng Gao, Xiangyang Liu, Pengfei Luo, Yang Yang, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QTIAH-GNN:+Quantity+and+Topology+Imbalance-aware+Heterogeneous+Graph+Neural+Network+for+Bankruptcy+Prediction)|0|
|[Multi-Grained Multimodal Interaction Network for Entity Linking](https://doi.org/10.1145/3580305.3599439)|Pengfei Luo, Tong Xu, Shiwei Wu, Chen Zhu, Linli Xu, Enhong Chen|University of Science and Technology of China; BOSS Zhipin|Multimodal entity linking (MEL) task, which aims at resolving ambiguous mentions to a multimodal knowledge graph, has attracted wide attention in recent years. Though large efforts have been made to explore the complementary effect among multiple modalities, however, they may fail to fully absorb the comprehensive expression of abbreviated textual context and implicit visual indication. Even worse, the inevitable noisy data may cause inconsistency of different modalities during the learning process, which severely degenerates the performance. To address the above issues, in this paper, we propose a novel Multi-GraIned Multimodal InteraCtion Network $\textbf{(MIMIC)}$ framework for solving the MEL task. Specifically, the unified inputs of mentions and entities are first encoded by textual/visual encoders separately, to extract global descriptive features and local detailed features. Then, to derive the similarity matching score for each mention-entity pair, we device three interaction units to comprehensively explore the intra-modal interaction and inter-modal fusion among features of entities and mentions. In particular, three modules, namely the Text-based Global-Local interaction Unit (TGLU), Vision-based DuaL interaction Unit (VDLU) and Cross-Modal Fusion-based interaction Unit (CMFU) are designed to capture and integrate the fine-grained representation lying in abbreviated text and implicit visual cues. Afterwards, we introduce a unit-consistency objective function via contrastive learning to avoid inconsistency and model degradation. Experimental results on three public benchmark datasets demonstrate that our solution outperforms various state-of-the-art baselines, and ablation studies verify the effectiveness of designed modules.|多模态实体连接任务是近年来受到广泛关注的一种解决多模态知识图中模糊提及问题的任务。尽管人们在探索多种语言形式之间的互补效应方面做出了很大的努力，但是，它们可能无法充分吸收语篇缩略语境和隐含视觉表征的综合表达。更糟糕的是，在学习过程中，不可避免的噪声数据可能导致不同模式的不一致性，从而严重影响学习效果。针对上述问题，本文提出了一种新的多粒度多模式交互网络解决 MEL 任务的框架。具体来说，提及和实体的统一输入首先由文本/可视化编码器分别进行编码，以提取全局描述特征和局部详细特征。然后，为了得到每个提及实体对的相似性匹配得分，我们设计了三个交互单元来全面探索实体和提及特征之间的模式内交互和模式间融合。特别是基于文本的全局-局部交互单元(TGLU)、基于视觉的双向交互单元(VDLU)和基于交叉模态融合的交互单元(CMFU)这三个模块被设计用来捕获和整合缩略文本和隐式视觉线索中的细粒度表示。然后，通过对比学习引入单位一致性目标函数，避免了不一致性和模型退化。在三个公共基准数据集上的实验结果表明，我们的解决方案优于各种最先进的基准，烧蚀研究验证了所设计模块的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Grained+Multimodal+Interaction+Network+for+Entity+Linking)|0|
|[Learning Strong Graph Neural Networks with Weak Information](https://doi.org/10.1145/3580305.3599410)|Yixin Liu, Kaize Ding, Jianling Wang, Vincent C. S. Lee, Huan Liu, Shirui Pan|Texas AM University; Griffith University; Arizona State University; Monash University|Graph Neural Networks (GNNs) have exhibited impressive performance in many graph learning tasks. Nevertheless, the performance of GNNs can deteriorate when the input graph data suffer from weak information, i.e., incomplete structure, incomplete features, and insufficient labels. Most prior studies, which attempt to learn from the graph data with a specific type of weak information, are far from effective in dealing with the scenario where diverse data deficiencies exist and mutually affect each other. To fill the gap, in this paper, we aim to develop an effective and principled approach to the problem of graph learning with weak information (GLWI). Based on the findings from our empirical analysis, we derive two design focal points for solving the problem of GLWI, i.e., enabling long-range propagation in GNNs and allowing information propagation to those stray nodes isolated from the largest connected component. Accordingly, we propose D$^2$PT, a dual-channel GNN framework that performs long-range information propagation not only on the input graph with incomplete structure, but also on a global graph that encodes global semantic similarities. We further develop a prototype contrastive alignment algorithm that aligns the class-level prototypes learned from two channels, such that the two different information propagation processes can mutually benefit from each other and the finally learned model can well handle the GLWI problem. Extensive experiments on eight real-world benchmark datasets demonstrate the effectiveness and efficiency of our proposed methods in various GLWI scenarios.|图形神经网络(GNN)在许多图形学习任务中表现出令人印象深刻的性能。然而，当输入图数据的结构不完整、特征不完整、标签不充分等信息不充分时，GNN 的性能会下降。大多数先前的研究试图从具有特定类型的薄弱信息的图表数据中学习，但在处理存在不同数据缺陷并相互影响的情况方面远远不够有效。为了填补这一空白，本文旨在开发一种有效的原则性方法来解决弱信息图学习问题(GLWI)。基于我们的实证分析结果，我们得出了解决 GLWI 问题的两个设计重点，即在 GNN 中实现远程传播和允许信息传播到那些从最大的连接元件(图论)中隔离出来的杂散节点。因此，我们提出了一个双通道 GNN 框架 D $^ 2 $PT，它不仅在结构不完整的输入图上进行远程信息传播，而且在编码全局语义相似性的全局图上进行远程信息传播。我们进一步开发了一个原型对比对齐算法，该算法对齐了从两个通道学习的类级原型，使得两个不同的信息传播过程能够相互受益，最终学习的模型能够很好地处理 GLWI 问题。在八个真实世界的基准数据集上的大量实验证明了我们提出的方法在不同 GLWI 场景下的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Strong+Graph+Neural+Networks+with+Weak+Information)|0|
|[Augmenting Recurrent Graph Neural Networks with a Cache](https://doi.org/10.1145/3580305.3599260)|Guixiang Ma, Vy A. Vo, Theodore L. Willke, Nesreen K. Ahmed||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Augmenting+Recurrent+Graph+Neural+Networks+with+a+Cache)|0|
|[Learning for Counterfactual Fairness from Observational Data](https://doi.org/10.1145/3580305.3599408)|Jing Ma, Ruocheng Guo, Aidong Zhang, Jundong Li|University of Virginia; Bytedance Research|Fairness-aware machine learning has attracted a surge of attention in many domains, such as online advertising, personalized recommendation, and social media analysis in web applications. Fairness-aware machine learning aims to eliminate biases of learning models against certain subgroups described by certain protected (sensitive) attributes such as race, gender, and age. Among many existing fairness notions, counterfactual fairness is a popular notion defined from a causal perspective. It measures the fairness of a predictor by comparing the prediction of each individual in the original world and that in the counterfactual worlds in which the value of the sensitive attribute is modified. A prerequisite for existing methods to achieve counterfactual fairness is the prior human knowledge of the causal model for the data. However, in real-world scenarios, the underlying causal model is often unknown, and acquiring such human knowledge could be very difficult. In these scenarios, it is risky to directly trust the causal models obtained from information sources with unknown reliability and even causal discovery methods, as incorrect causal models can consequently bring biases to the predictor and lead to unfair predictions. In this work, we address the problem of counterfactually fair prediction from observational data without given causal models by proposing a novel framework CLAIRE. Specifically, under certain general assumptions, CLAIRE effectively mitigates the biases from the sensitive attribute with a representation learning framework based on counterfactual data augmentation and an invariant penalty. Experiments conducted on both synthetic and real-world datasets validate the superiority of CLAIRE in both counterfactual fairness and prediction performance.|公平感知机器学习在许多领域引起了广泛的关注，如在线广告、个性化推荐和网络应用中的社会媒体分析。公平意识机器学习旨在消除学习模型对某些受保护(敏感)属性(如种族、性别和年龄)描述的子群体的偏见。在众多现有的公平观念中，反事实公平是一个从因果关系角度定义的流行概念。它通过比较原始世界中每个个体的预测和反事实世界中敏感属性值被修改的预测来衡量预测的公平性。现有方法实现反事实公平的一个先决条件是人类对数据的因果模型的先验知识。然而，在现实世界的情况下，潜在的因果模型往往是未知的，并获得这样的人类知识可能是非常困难的。在这些情况下，直接相信从信息来源获得的具有未知可靠性甚至因果发现方法的因果模型是有风险的，因为不正确的因果模型会因此给预测者带来偏差并导致不公平的预测。在这项工作中，我们通过提出一个新的框架 CLAIRE 来解决没有给定因果模型的观测数据的反事实公平预测问题。特别地，在一定的一般假设下，CLAIRE 通过一个基于反事实数据增强和不变惩罚的表示学习框架有效地减轻了敏感属性的偏差。在合成数据集和真实数据集上进行的实验验证了 CLAIRE 在反事实公平性和预测性能方面的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+for+Counterfactual+Fairness+from+Observational+Data)|0|
|[Towards Graph-level Anomaly Detection via Deep Evolutionary Mapping](https://doi.org/10.1145/3580305.3599524)|Xiaoxiao Ma, Jia Wu, Jian Yang, Quan Z. Sheng|Macquarie University|Graph-level anomaly detection aims at depicting anomalous individual graphs in a graph set. Due to its significance in various real-world application fields, such as identifying rare molecules in chemistry and detecting potential frauds in online social networks, graph-level anomaly detection has received great attention. In distinction from node- and edge-level anomaly detection that is devoted to identifying anomalies on a single graph, graph-level anomaly detection faces more significant challenges because both the intra- and inter-graph structural and attribute patterns need to be taken into account to distinguish anomalies that exhibit deviating structures, rare attributes or the both. Although deep graph representation learning shows effectiveness in fusing high-level representations and capturing characters of individual graphs, most of the existing works are defective in graph-level anomaly detection because of their limited capability in exploring information across graphs, the imbalanced data distribution of anomalies, and low interpretability of the black-box graph neural networks (GNNs). To bridge these gaps, we propose a novel deep evolutionary graph mapping framework named GmapAD, which can adaptively map each graph into a new feature space based on its similarity to a set of representative nodes chosen from the graph set. By automatically adjusting the candidate nodes using a specially designed evolutionary algorithm, anomalies and normal graphs are mapped to separate areas in the new feature space where a clear boundary between them can be learned. The selected candidate nodes can therefore be regarded as a benchmark for explaining anomalies because anomalies are more dissimilar/similar to the benchmark than normal graphs. Through our extensive experiments on nine real-world datasets, we demonstrate that exploring both intra- and inter-graph structural and attribute information are critical to spot anomalous graphs, and our framework outperforms the state of the art on all datasets used in the experiments.|图级异常检测的目的是在一个图集中描述不规则的单个图。由于其在各种现实应用领域的重要性，例如识别化学中的稀有分子和在线社交网络中发现潜在的欺诈行为，图级异常检测已经受到了极大的关注。与专门用于识别单个图表上的异常的节点级和边界级异常检测不同，图表级异常检测面临着更大的挑战，因为需要考虑图表内部和图表间的结构和属性模式，以区分表现出偏离结构、罕见属性或两者兼而有之的异常。尽管深度图表示学习在融合高层次表示和捕获单个图的特征方面显示出有效性，但是现有的大多数工作在图级异常检测方面存在缺陷，因为它们在跨图探索信息方面的能力有限，异常数据分布不平衡，以及黑盒图神经网络(GNN)的可解释性较低。为了弥补这些不足，我们提出了一种新的深度进化图映射框架 GmapAD，它可以根据每个图与从图集中选择的一组代表性节点的相似性，自适应地将每个图映射到一个新的特征空间。通过使用专门设计的进化算法自动调整候选节点，异常和正态图被映射到新特征空间中的分离区域，在这些区域之间可以学习到清晰的边界。因此，所选择的候选节点可以被视为解释异常的基准，因为异常与基准比正常图表更不相似/更相似。通过对9个真实世界数据集的广泛实验，我们证明了探索图内和图间结构和属性信息对于发现异常图至关重要，并且我们的框架在实验中使用的所有数据集上优于最先进的状态。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Graph-level+Anomaly+Detection+via+Deep+Evolutionary+Mapping)|0|
|[Context-aware Event Forecasting via Graph Disentanglement](https://doi.org/10.1145/3580305.3599285)|Yunshan Ma, Chenchen Ye, Zijian Wu, Xiang Wang, Yixin Cao, TatSeng Chua|Singapore Management University; University of Science and Technology of China; National University of Singapore|Event forecasting has been a demanding and challenging task throughout the entire human history. It plays a pivotal role in crisis alarming and disaster prevention in various aspects of the whole society. The task of event forecasting aims to model the relational and temporal patterns based on historical events and makes forecasting to what will happen in the future. Most existing studies on event forecasting formulate it as a problem of link prediction on temporal event graphs. However, such pure structured formulation suffers from two main limitations: 1) most events fall into general and high-level types in the event ontology, and therefore they tend to be coarse-grained and offers little utility which inevitably harms the forecasting accuracy; and 2) the events defined by a fixed ontology are unable to retain the out-of-ontology contextual information. To address these limitations, we propose a novel task of context-aware event forecasting which incorporates auxiliary contextual information. First, the categorical context provides supplementary fine-grained information to the coarse-grained events. Second and more importantly, the context provides additional information towards specific situation and condition, which is crucial or even determinant to what will happen next. However, it is challenging to properly integrate context into the event forecasting framework, considering the complex patterns in the multi-context scenario. Towards this end, we design a novel framework named Separation and Collaboration Graph Disentanglement (short as SeCoGD) for context-aware event forecasting. Since there is no available dataset for this novel task, we construct three large-scale datasets based on GDELT. Experimental results demonstrate that our model outperforms a list of SOTA methods.|在整个人类历史中，事件预测一直是一项艰巨而富有挑战性的任务。它在全社会的各个方面对危机预警和灾害预防起着举足轻重的作用。事件预测的任务是建立基于历史事件的关系模型和时间模型，并对未来发生的事件进行预测。现有的大多数事件预测研究都将其归结为时间事件图上的链接预测问题。然而，这种纯结构化公式存在两个主要的局限性: 1)大多数事件在事件本体中属于一般的和高级的类型，因此它们往往是粗粒度的，提供的效用很小，这不可避免地损害了预测的准确性; 2)由固定本体定义的事件不能保留本体外的上下文信息。为了解决这些局限性，我们提出了一个新的任务上下文感知事件预测，其中包括辅助上下文信息。首先，分类上下文为粗粒度事件提供补充的细粒度信息。其次，也是更重要的一点，上下文为特定的情况和条件提供了额外的信息，这对于接下来会发生什么是至关重要的，甚至是决定性的。然而，考虑到多上下文场景中的复杂模式，将上下文适当地集成到事件预测框架中是一个挑战。为此，我们设计了一个新的框架，命名为分离和协作图分离(简称 SeCoGD) ，用于上下文感知事件预测。由于这个新的任务没有可用的数据集，我们构建了三个基于 GDELT 的大规模数据集。实验结果表明，该模型的性能优于一系列 SOTA 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Context-aware+Event+Forecasting+via+Graph+Disentanglement)|0|
|[End-to-End Inventory Prediction and Contract Allocation for Guaranteed Delivery Advertising](https://doi.org/10.1145/3580305.3599332)|Wuyang Mao, Chuanren Liu, Yundu Huang, Zhonglin Zu, M. Harshvardhan, Liang Wang, Bo Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=End-to-End+Inventory+Prediction+and+Contract+Allocation+for+Guaranteed+Delivery+Advertising)|0|
|[Densest Diverse Subgraphs: How to Plan a Successful Cocktail Party with Diversity](https://doi.org/10.1145/3580305.3599306)|Atsushi Miyauchi, Tianyi Chen, Konstantinos Sotiropoulos, Charalampos E. Tsourakakis|CENTAI Institute; Boston University|Dense subgraph discovery methods are routinely used in a variety of applications including the identification of a team of skilled individuals for collaboration from a social network. However, when the network's node set is associated with a sensitive attribute such as race, gender, religion, or political opinion, the lack of diversity can lead to lawsuits.   In this work, we focus on the problem of finding a densest diverse subgraph in a graph whose nodes have different attribute values/types that we refer to as colors. We propose two novel formulations motivated by different realistic scenarios. Our first formulation, called the densest diverse subgraph problem (DDSP), guarantees that no color represents more than some fraction of the nodes in the output subgraph, which generalizes the state-of-the-art due to Anagnostopoulos et al. (CIKM 2020). By varying the fraction we can range the diversity constraint and interpolate from a diverse dense subgraph where all colors have to be equally represented to an unconstrained dense subgraph. We design a scalable $\Omega(1/\sqrt{n})$-approximation algorithm, where $n$ is the number of nodes. Our second formulation is motivated by the setting where any specified color should not be overlooked. We propose the densest at-least-$\vec{k}$-subgraph problem (Dal$\vec{k}$S), a novel generalization of the classic Dal$k$S, where instead of a single value $k$, we have a vector ${\mathbf k}$ of cardinality demands with one coordinate per color class. We design a $1/3$-approximation algorithm using linear programming together with an acceleration technique. Computational experiments using synthetic and real-world datasets demonstrate that our proposed algorithms are effective in extracting dense diverse clusters.|密集子图发现方法常用于各种应用程序中，包括从社交网络中识别一组技术熟练的个人进行协作。然而，当网络的节点集与一个敏感的属性(如种族、性别、宗教或政治观点)相关联时，缺乏多样性可能导致诉讼。在这项工作中，我们的重点是在一个图的节点有不同的属性值/类型，我们称之为颜色的图中找到一个密度最大的不同子图的问题。我们提出了两个新的公式动机不同的现实情景。我们的第一个公式称为最密集多样化子图问题(DDSP) ，保证没有颜色代表输出子图中的一些节点部分，这推广了由于 Anagnostopoulos 等人(CIKM 2020)的最先进状态。通过改变分数，我们可以扩大多样性约束，并从一个不同的密集子图插值，其中所有颜色必须平等地表示为一个无约束的密集子图。我们设计了一个可伸缩的 $Omega (1/sqrt { n }) $- 近似演算法，其中 $n $是节点数。我们的第二个配方是动机的设置，其中任何指定的颜色不应该被忽视。我们提出了最密集的至少 $vec { k } $- 子图问题(Dal $vec { k } $S) ，这是经典 Dal $k $S 的一个新的推广，其中，我们有一个向量 ${ mathbf k } $的基数需求，每个颜色类有一个坐标。我们设计了一个1/3美元的近似演算法，使用了线性规划和加速技术。使用合成和真实世界数据集的计算实验表明，我们提出的算法是有效的提取密集不同的簇。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Densest+Diverse+Subgraphs:+How+to+Plan+a+Successful+Cocktail+Party+with+Diversity)|0|
|[Causal Inference via Style Transfer for Out-of-distribution Generalisation](https://doi.org/10.1145/3580305.3599270)|Toan Nguyen, Kien Do, Duc Thanh Nguyen, Bao Duong, Thin Nguyen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Inference+via+Style+Transfer+for+Out-of-distribution+Generalisation)|0|
|[A Higher-Order Temporal H-Index for Evolving Networks](https://doi.org/10.1145/3580305.3599242)|Lutz Oettershagen, Nils M. Kriege, Petra Mutzel|KTH Royal Institute of Technology; University of Bonn; University of Vienna|The H-index of a node in a static network is the maximum value $h$ such that at least $h$ of its neighbors have a degree of at least $h$. Recently, a generalized version, the $n$-th order H-index, was introduced, allowing to relate degree centrality, H-index, and the $k$-core of a node. We extend the $n$-th order H-index to temporal networks and define corresponding temporal centrality measures and temporal core decompositions. Our $n$-th order temporal H-index respects the reachability in temporal networks leading to node rankings, which reflect the importance of nodes in spreading processes. We derive natural decompositions of temporal networks into subgraphs with strong temporal coherence. We analyze a recursive computation scheme and develop a highly scalable streaming algorithm. Our experimental evaluation demonstrates the efficiency of our algorithms and the conceptional validity of our approach. Specifically, we show that the $n$-th order temporal H-index is a strong heuristic for identifying super-spreaders in evolving social networks and detects temporally well-connected components.|静态网络中节点的 H 指标是最大值 $h $，这样至少 $h $的邻居具有至少 $h $的度。最近，引入了一个通用版本 $n $th 阶 H 指标，它允许关联节点的度中心性、 H 指标和 $k $- 核。将 n 阶 H 指标扩展到时态网络，定义了相应的时态中心度量和时态核分解。我们的 n 阶时态 H 指数考虑了时态网络中节点排名的可达性，反映了节点在扩展过程中的重要性。我们将时间网络自然分解为具有强时间相干性的子图。我们分析了一个递归计算方案，并开发了一个高度可扩展的流式算法。我们的实验评估证明了我们的算法的有效性和我们的方法的概念的有效性。具体地说，我们证明了 $n 阶时间 H 指数是一个强大的启发式识别超级传播者在不断发展的社会网络和检测时间上良好连接的组件。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Higher-Order+Temporal+H-Index+for+Evolving+Networks)|0|
|[Deep Weakly-supervised Anomaly Detection](https://doi.org/10.1145/3580305.3599302)|Guansong Pang, Chunhua Shen, Huidong Jin, Anton van den Hengel|Singapore Management University; University of Adelaide; Data61; Zhejiang University|Anomaly detection is typically posited as an unsupervised learning task in the literature due to the prohibitive cost and difficulty to obtain large-scale labeled anomaly data, but this ignores the fact that a very small number (eg,, a few dozens) of labeled anomalies can often be made available with small/trivial cost in many real-world anomaly detection applications. To leverage such labeled anomaly data, we study an important anomaly detection problem termed weakly-supervised anomaly detection, in which, in addition to a large amount of unlabeled data, a limited number of labeled anomalies are available during modeling. Learning with the small labeled anomaly data enables anomaly-informed modeling, which helps identify anomalies of interest and address the notorious high false positives in unsupervised anomaly detection. However, the problem is especially challenging, since (i) the limited amount of …|由于成本过高和难以获得大规模的标记异常数据，异常检测在文献中通常被假定为一个非监督式学习任务，但这忽略了一个事实，即在许多现实世界的异常检测应用中，只有很少数量(例如，几十个)的标记异常可以用很小的成本获得。为了利用这些已标记的异常数据，我们研究了一个重要的异常检测问题，称为弱监督异常检测。在这个问题中，除了大量未标记的数据之外，在建模过程中只有有限数量的已标记的异常可用。利用小标记的异常数据进行学习，可以建立异常信息模型，有助于识别感兴趣的异常，并解决无监督异常检测中出名的高假阳性问题。然而，这个问题是特别具有挑战性的，因为(i)数量有限..。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Weakly-supervised+Anomaly+Detection)|0|
|[FedDefender: Client-Side Attack-Tolerant Federated Learning](https://doi.org/10.1145/3580305.3599346)|Sungwon Park, Sungwon Han, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, Meeyoung Cha|IBS; KAIST; GIST; Microsoft Research Asia|Federated learning enables learning from decentralized data sources without compromising privacy, which makes it a crucial technique. However, it is vulnerable to model poisoning attacks, where malicious clients interfere with the training process. Previous defense mechanisms have focused on the server-side by using careful model aggregation, but this may not be effective when the data is not identically distributed or when attackers can access the information of benign clients. In this paper, we propose a new defense mechanism that focuses on the client-side, called FedDefender, to help benign clients train robust local models and avoid the adverse impact of malicious model updates from attackers, even when a server-side defense cannot identify or remove adversaries. Our method consists of two main components: (1) attack-tolerant local meta update and (2) attack-tolerant global knowledge distillation. These components are used to find noise-resilient model parameters while accurately extracting knowledge from a potentially corrupted global model. Our client-side defense strategy has a flexible structure and can work in conjunction with any existing server-side strategies. Evaluations of real-world scenarios across multiple datasets show that the proposed method enhances the robustness of federated learning against model poisoning attacks.|联合学习能够在不损害隐私的情况下从分散的数据源中学习，这使得它成为一种关键技术。但是，它很容易受到模型中毒攻击，因为恶意客户机会干扰培训过程。以前的防御机制通过使用谨慎的模型聚合将重点放在服务器端，但是当数据分布不完全相同或者当攻击者可以访问良性客户端的信息时，这可能不会有效。在本文中，我们提出了一种新的以客户端为中心的防御机制，称为 FedDefender，以帮助良性客户端训练健壮的本地模型，并避免来自攻击者的恶意模型更新的负面影响，即使服务器端防御不能识别或移除对手。该方法由两个主要部分组成: (1)容忍攻击的局部元更新和(2)容忍攻击的全局知识提取。这些分量被用来寻找抗噪声的模型参数，同时准确地从潜在损坏的全局模型中提取知识。我们的客户端防御策略具有灵活的结构，可以与任何现有的服务器端策略协同工作。对多个数据集的实际场景的评估表明，该方法增强了联邦学习对模型中毒攻击的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedDefender:+Client-Side+Attack-Tolerant+Federated+Learning)|0|
|[Few-shot Low-resource Knowledge Graph Completion with Multi-view Task Representation Generation](https://doi.org/10.1145/3580305.3599350)|Shichao Pei, Ziyi Kou, Qiannan Zhang, Xiangliang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Few-shot+Low-resource+Knowledge+Graph+Completion+with+Multi-view+Task+Representation+Generation)|0|
|[Efficient Centrality Maximization with Rademacher Averages](https://doi.org/10.1145/3580305.3599325)|Leonardo Pellegrina|University of Padova|The identification of the set of k most central nodes of a graph, or centrality maximization, is a key task in network analysis, with various applications ranging from finding communities in social and biological networks to understanding which seed nodes are important to diffuse information in a graph. As the exact computation of centrality measures does not scale to modern-sized networks, the most practical solution is to resort to rigorous, but efficiently computable, randomized approximations. In this work we present CentRA, the first algorithm based on progressive sampling to compute high-quality approximations of the set of k most central nodes. CentRA is based on a novel approach to efficiently estimate Monte Carlo Rademacher Averages, a powerful tool from statistical learning theory to compute sharp data-dependent approximation bounds. Then, we study the sample complexity of centrality maximization using the VC-dimension, a key concept from statistical learning theory. We show that the number of random samples required to compute high-quality approximations scales with finer characteristics of the graph, such as its vertex diameter, or of the centrality of interest, significantly improving looser bounds derived from standard techniques. We apply CentRA to analyze large real-world networks, showing that it significantly outperforms the state-of-the-art approximation algorithm in terms of number of samples, running times, and accuracy.|图的 k 个最中心节点集的识别，或中心性最大化，是网络分析中的一个关键任务，其应用范围广泛，从寻找社会和生物网络中的社区，到了解哪些种子节点对传播图中的信息是重要的。由于中心性度量的精确计算并不适用于现代规模的网络，最实际的解决方案是采用严格的、但可有效计算的随机近似。在这项工作中，我们提出的 CentRA，第一个算法的基础上逐步采样计算高质量的近似集 k 最中心节点。CentRA 基于一种有效估计 Monte Carlo Rademacher 平均值的新方法，这是一种来自统计学习理论的强大工具，用于计算与数据相关的近似界限。然后，利用统计学习理论中的一个关键概念—— VC 维，研究了中心性最大化的样本复杂度问题。我们表明，随机样本的数量需要计算高质量的近似尺度与更细的图的特征，如其顶点直径，或感兴趣的中心性，显着改善松散的界限源自标准技术。我们使用 CentRA 来分析大型现实世界的网络，结果显示，它在样本数量、运行时间和准确性方面都明显优于最先进的近似演算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Centrality+Maximization+with+Rademacher+Averages)|0|
|[Learning from Positive and Unlabeled Multi-Instance Bags in Anomaly Detection](https://doi.org/10.1145/3580305.3599409)|Lorenzo Perini, Vincent Vercruyssen, Jesse Davis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+from+Positive+and+Unlabeled+Multi-Instance+Bags+in+Anomaly+Detection)|0|
|[Generalizable Low-Resource Activity Recognition with Diverse and Discriminative Representation Learning](https://doi.org/10.1145/3580305.3599360)|Xin Qin, Jindong Wang, Shuo Ma, Wang Lu, Yongchun Zhu, Xing Xie, Yiqiang Chen|Beijing Key Lab. of Mobile Com., CAS; Microsoft Research Asia|Human activity recognition (HAR) is a time series classification task that focuses on identifying the motion patterns from human sensor readings. Adequate data is essential but a major bottleneck for training a generalizable HAR model, which assists customization and optimization of online web applications. However, it is costly in time and economy to collect large-scale labeled data in reality, i.e., the low-resource challenge. Meanwhile, data collected from different persons have distribution shifts due to different living habits, body shapes, age groups, etc. The low-resource and distribution shift challenges are detrimental to HAR when applying the trained model to new unseen subjects. In this paper, we propose a novel approach called Diverse and Discriminative representation Learning (DDLearn) for generalizable low-resource HAR. DDLearn simultaneously considers diversity and discrimination learning. With the constructed self-supervised learning task, DDLearn enlarges the data diversity and explores the latent activity properties. Then, we propose a diversity preservation module to preserve the diversity of learned features by enlarging the distribution divergence between the original and augmented domains. Meanwhile, DDLearn also enhances semantic discrimination by learning discriminative representations with supervised contrastive learning. Extensive experiments on three public HAR datasets demonstrate that our method significantly outperforms state-of-art methods by an average accuracy improvement of 9.5% under the low-resource distribution shift scenarios, while being a generic, explainable, and flexible framework.|人类活动识别(HAR)是一个时间序列分类任务，重点是从人类传感器读数识别运动模式。充足的数据是必不可少的，但也是培训一个可推广的 HAR 模型的主要瓶颈，该模型有助于在线 Web 应用程序的定制和优化。然而，在现实生活中收集大规模的标记数据在时间和经济上是昂贵的，也就是说，低资源的挑战。同时，由于生活习惯、体型、年龄等因素的不同，从不同人群收集到的数据也有分布变化。低资源和分配转移的挑战是有害的 HAR 时，应用训练模型的新的看不见的主题。针对可推广的低资源 HAR 问题，提出了一种新的多元鉴别表示学习方法。DDLearning 同时考虑多样性和歧视性学习。通过构建自监督学习任务，扩大了数据的多样性，探索了潜在活动特性。然后，我们提出了一个多样性保持模块，通过扩大原始域和扩展域之间的分布差异来保持学习特征的多样性。同时，通过有监督的对比学习来学习判别表征，DDLearning 也增强了语义识别能力。在三个公共 HAR 数据集上的大量实验表明，在低资源分布转移情景下，我们的方法显着优于最先进的方法，平均准确度提高了9.5% ，同时是一个通用的，可解释的和灵活的框架。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generalizable+Low-Resource+Activity+Recognition+with+Diverse+and+Discriminative+Representation+Learning)|0|
|[3D-IDS: Doubly Disentangled Dynamic Intrusion Detection](https://doi.org/10.1145/3580305.3599238)|Chenyang Qiu, Yingsheng Geng, Junrui Lu, Kaida Chen, Shitong Zhu, Ya Su, Guoshun Nan, Can Zhang, Junsong Fu, Qimei Cui, Xiaofeng Tao|Beijing University of Posts and Telecommunications|Network-based intrusion detection system (NIDS) monitors network traffic for malicious activities, forming the frontline defense against increasing attacks over information infrastructures. Although promising, our quantitative analysis shows that existing methods perform inconsistently in declaring various unknown attacks (e.g., 9% and 35% F1 respectively for two distinct unknown threats for an SVM-based method) or detecting diverse known attacks (e.g., 31% F1 for the Backdoor and 93% F1 for DDoS by a GCN-based state-of-the-art method), and reveals that the underlying cause is entangled distributions of flow features. This motivates us to propose 3D-IDS, a novel method that aims to tackle the above issues through two-step feature disentanglements and a dynamic graph diffusion scheme. Specifically, we first disentangle traffic features by a non-parameterized optimization based on mutual information, automatically differentiating tens and hundreds of complex features of various attacks. Such differentiated features will be fed into a memory model to generate representations, which are further disentangled to highlight the attack-specific features. Finally, we use a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. By doing so, we can effectively identify various attacks in encrypted traffics, including unknown threats and known ones that are not easily detected. Experiments show the superiority of our 3D-IDS. We also demonstrate that our two-step feature disentanglements benefit the explainability of NIDS.|基于网络的入侵预防系统(nIDS)监控网络流量以防范恶意活动，从而形成一道防线，抵御对信息基础设施日益增多的攻击。尽管有希望，但是我们的定量分析表明，现有的方法在声明各种未知攻击(例如，基于 SVM 的方法的两种不同的未知威胁分别为9% 和35% F1)或检测不同的已知攻击(例如，通过基于 GCN 的最新技术的方法，后门为31% F1，DDoS 为93% F1) ，并且揭示了潜在的原因是流特征的纠缠分布。这促使我们提出了三维入侵检测系统(3D-IDS) ，这是一种通过两步特征分离和动态图扩散来解决上述问题的新方法。具体来说，我们首先通过一个基于互信息的非参数化优化来解析流量特征，自动区分出数十个和数百个各种攻击的复杂特征。这些差异化的特征将被输入到一个记忆模型中以生成表征，这些表征将被进一步分离以突出攻击特定的特征。最后，我们使用一种新的图形扩散方法，在不断演化的数据流中动态地融合时空聚合的网络拓扑。通过这样做，我们可以有效地识别加密流量中的各种攻击，包括未知威胁和不容易检测到的已知威胁。实验表明了我们的3D 入侵检测系统的优越性。我们还演示了我们的两步特征分离有利于 NIDS 的可解释性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=3D-IDS:+Doubly+Disentangled+Dynamic+Intrusion+Detection)|0|
|[Reconstructing Graph Diffusion History from a Single Snapshot](https://doi.org/10.1145/3580305.3599488)|Ruizhong Qiu, Dingsu Wang, Lei Ying, H. Vincent Poor, Yifang Zhang, Hanghang Tong|Princeton University; University of Michigan; University of Illinois Urbana-Champaign; Cai Digital Transformation Institute|Diffusion on graphs is ubiquitous with numerous high-impact applications. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot} (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) estimation error of diffusion parameters is unavoidable due to NP-hardness of diffusion parameter estimation, and (b) the MLE formulation is sensitive to estimation error of diffusion parameters. To overcome the inherent limitation of the MLE formulation, we propose a novel barycenter formulation: finding the barycenter of the posterior distribution of histories, which is provably stable against the estimation error of diffusion parameters. We further develop an effective solver named DIffusion hiTting Times with Optimal proposal (DITTO) by reducing the problem to estimating posterior expected hitting times via the Metropolis--Hastings Markov chain Monte Carlo method (M--H MCMC) and employing an unsupervised graph neural network to learn an optimal proposal to accelerate the convergence of M--H MCMC. We conduct extensive experiments to demonstrate the efficacy of the proposed method.|图上的扩散是普遍存在的许多高影响应用。在这些应用中，完整的扩散历史在识别动态模式、反映预防措施和预测干预效果方面发挥着至关重要的作用。尽管它们的重要性，完整的扩散历史很少可用，并且由于不适定性，爆炸搜索空间和缺乏训练数据，重建是非常具有挑战性的。迄今为止，弥散历史重建的方法很少。它们完全基于最大似然估计(MLE)公式，需要知道真实的扩散参数。在本文中，我们研究了一个更加困难的问题，即从一个单一的 SnapsHot }(DASH)重建扩散历史，其中我们寻求只从最终的快照重建历史，而不知道真正的扩散参数。我们从理论分析开始，揭示了极大似然估计公式的基本局限性。我们证明: (a)由于扩散参数估计的 NP 难度，扩散参数的估计误差是不可避免的; (b)扩散参数估计误差对最大似然估计公式是敏感的。为了克服极大似然估计公式的固有局限性，我们提出了一种新的重心公式: 找到历史后验概率的重心，这种公式可以证明对抗扩散参数的估计误差是稳定的。通过使用 Metropolis-Hastings 马尔科夫蒙特卡洛方法(M-H MCMC)来减少估计后期预期命中时间的问题，并使用一个无监督的图形神经网络来学习一个加速 M-H MCMC 收敛的最优方案，我们进一步开发了一个有效的解决方案——扩散命中时间与最优方案(dITTO)。我们进行了广泛的实验，以证明所提出的方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reconstructing+Graph+Diffusion+History+from+a+Single+Snapshot)|0|
|[FedPseudo: Privacy-Preserving Pseudo Value-Based Deep Learning Models for Federated Survival Analysis](https://doi.org/10.1145/3580305.3599348)|Md. Mahmudur Rahman, Sanjay Purushotham||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedPseudo:+Privacy-Preserving+Pseudo+Value-Based+Deep+Learning+Models+for+Federated+Survival+Analysis)|0|
|[Robustness Certification for Structured Prediction with General Inputs via Safe Region Modeling in the Semimetric Output Space](https://doi.org/10.1145/3580305.3599493)|Huaqing Shao, Lanjun Wang, Junchi Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robustness+Certification+for+Structured+Prediction+with+General+Inputs+via+Safe+Region+Modeling+in+the+Semimetric+Output+Space)|0|
|[CARL-G: Clustering-Accelerated Representation Learning on Graphs](https://doi.org/10.1145/3580305.3599268)|William Shiao, Uday Singh Saini, Yozen Liu, Tong Zhao, Neil Shah, Evangelos E. Papalexakis|Snap Inc.; University of California, Riverside|Self-supervised learning on graphs has made large strides in achieving great performance in various downstream tasks. However, many state-of-the-art methods suffer from a number of impediments, which prevent them from realizing their full potential. For instance, contrastive methods typically require negative sampling, which is often computationally costly. While non-contrastive methods avoid this expensive step, most existing methods either rely on overly complex architectures or dataset-specific augmentations. In this paper, we ask: Can we borrow from classical unsupervised machine learning literature in order to overcome those obstacles? Guided by our key insight that the goal of distance-based clustering closely resembles that of contrastive learning: both attempt to pull representations of similar items together and dissimilar items apart. As a result, we propose CARL-G - a novel clustering-based framework for graph representation learning that uses a loss inspired by Cluster Validation Indices (CVIs), i.e., internal measures of cluster quality (no ground truth required). CARL-G is adaptable to different clustering methods and CVIs, and we show that with the right choice of clustering method and CVI, CARL-G outperforms node classification baselines on 4/5 datasets with up to a 79x training speedup compared to the best-performing baseline. CARL-G also performs at par or better than baselines in node clustering and similarity search tasks, training up to 1,500x faster than the best-performing baseline. Finally, we also provide theoretical foundations for the use of CVI-inspired losses in graph representation learning.|图上的自我监督学习在各种下游任务中取得了巨大的进展。然而，许多最先进的方法都存在一些障碍，这些障碍使它们无法充分发挥其潜力。例如，对比方法通常需要负采样，这往往是计算成本高昂。虽然非对比方法避免了这一代价高昂的步骤，但是大多数现有方法要么依赖于过于复杂的体系结构，要么依赖于特定于数据集的扩展。在本文中，我们会问: 我们能否借鉴古典非监督式学习文学来克服这些障碍？基于距离聚类的目标与对比学习的目标非常相似: 两者都试图将相似项目的表示放在一起，而将不同项目的表示分开。因此，我们提出了 CARL-G-一种新的基于聚类的图表示学习框架，其使用由聚类验证指数(CVI)启发的损失，即集群质量的内部测量(不需要地面真相)。CARL-G 适应不同的聚类方法和 CVI，我们表明，如果正确选择聚类方法和 CVI，CARL-G 在4/5数据集上优于节点分类基线，与最佳基线相比，训练加速高达79倍。CARL-G 在节点群集和最近邻搜索任务方面的表现也达到或优于基线，训练速度比最佳基线快1500倍。最后，我们还为 CVI 启发的损失在图表示学习中的应用提供了理论基础。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CARL-G:+Clustering-Accelerated+Representation+Learning+on+Graphs)|0|
|[One-shot Joint Extraction, Registration and Segmentation of Neuroimaging Data](https://doi.org/10.1145/3580305.3599452)|Yao Su, Zhentian Qian, Lei Ma, Lifang He, Xiangnan Kong|Worcester Polytechnic Institute; Lehigh University|Brain extraction, registration and segmentation are indispensable preprocessing steps in neuroimaging studies. The aim is to extract the brain from raw imaging scans (i.e., extraction step), align it with a target brain image (i.e., registration step) and label the anatomical brain regions (i.e., segmentation step). Conventional studies typically focus on developing separate methods for the extraction, registration and segmentation tasks in a supervised setting. The performance of these methods is largely contingent on the quantity of training samples and the extent of visual inspections carried out by experts for error correction. Nevertheless, collecting voxel-level labels and performing manual quality control on high-dimensional neuroimages (e.g., 3D MRI) are expensive and time-consuming in many medical studies. In this paper, we study the problem of one-shot joint extraction, registration and segmentation in neuroimaging data, which exploits only one labeled template image (a.k.a. atlas) and a few unlabeled raw images for training. We propose a unified end-to-end framework, called JERS, to jointly optimize the extraction, registration and segmentation tasks, allowing feedback among them. Specifically, we use a group of extraction, registration and segmentation modules to learn the extraction mask, transformation and segmentation mask, where modules are interconnected and mutually reinforced by self-supervision. Empirical results on real-world datasets demonstrate that our proposed method performs exceptionally in the extraction, registration and segmentation tasks. Our code and data can be found at https://github.com/Anonymous4545/JERS|脑提取、配准和分割是神经影像学研究中不可缺少的预处理步骤。目的是从原始成像扫描(即提取步骤)中提取大脑，将其与目标大脑图像(即注册步骤)对齐，并标记解剖大脑区域(即分割步骤)。传统的研究通常侧重于在监督环境下为提取、配准和分割任务开发单独的方法。这些方法的性能在很大程度上取决于训练样本的数量和专家为纠正错误而进行的目视检查的程度。然而，在许多医学研究中，收集体素级标签和对高维神经图像(例如3D MRI)进行手动质量控制是昂贵和耗时的。本文研究了神经影像数据的一次性关节提取、配准和分割问题，该方法只利用一个标记的模板图像(又称地图集)和少量未标记的原始图像进行训练。我们提出了一个统一的端到端框架，称为 JERS，以联合优化提取，注册和分割任务，允许它们之间的反馈。具体来说，我们使用一组抽取、配准和分割模块来学习抽取掩模、变换和分割掩模，这些模块通过自我监督相互联系、相互增强。对实际数据集的实验结果表明，该方法在提取、配准和分割任务中表现优异。我们的代码和数据可以在 https://github.com/anonymous4545/jers 找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One-shot+Joint+Extraction,+Registration+and+Segmentation+of+Neuroimaging+Data)|0|
|[Learning Autoregressive Model in LSM-Tree based Store](https://doi.org/10.1145/3580305.3599405)|Yunxiang Su, Wenxuan Ma, Shaoxu Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Autoregressive+Model+in+LSM-Tree+based+Store)|0|
|[Enhance Diffusion to Improve Robust Generalization](https://doi.org/10.1145/3580305.3599333)|Jianhui Sun, Sanchit Sinha, Aidong Zhang|University of Virginia|Deep neural networks are susceptible to human imperceptible adversarial perturbations. One of the strongest defense mechanisms is \emph{Adversarial Training} (AT). In this paper, we aim to address two predominant problems in AT. First, there is still little consensus on how to set hyperparameters with a performance guarantee for AT research, and customized settings impede a fair comparison between different model designs in AT research. Second, the robustly trained neural networks struggle to generalize well and suffer from tremendous overfitting. This paper focuses on the primary AT framework - Projected Gradient Descent Adversarial Training (PGD-AT). We approximate the dynamic of PGD-AT by a continuous-time Stochastic Differential Equation (SDE), and show that the diffusion term of this SDE determines the robust generalization. An immediate implication of this theoretical finding is that robust generalization is positively correlated with the ratio between learning rate and batch size. We further propose a novel approach, \emph{Diffusion Enhanced Adversarial Training} (DEAT), to manipulate the diffusion term to improve robust generalization with virtually no extra computational burden. We theoretically show that DEAT obtains a tighter generalization bound than PGD-AT. Our empirical investigation is extensive and firmly attests that DEAT universally outperforms PGD-AT by a significant margin.|深层神经网络容易受到人类无法察觉的对抗性扰动的影响。最强大的防御机制之一就是防御训练。在本文中，我们的目标是解决两个主要的问题在 AT。首先，关于如何为 AT 研究设置具有性能保证的超参数仍然没有多少共识，定制的设置妨碍了 AT 研究中不同模型设计之间的公平比较。其次，经过严格训练的神经网络难以很好地推广，并遭受了极大的过度拟合。这篇文章主要关注的是初级 AT 框架——梯度下降法对抗训练(pgd-AT)。我们用一个连续时间随机微分方程(SDE)来近似 PGD-AT 的动态，并表明这个 SDE 的扩散项决定了鲁棒性的推广。这一理论发现的一个直接含义是，鲁棒推广与学习率和批量大小之间的比率正相关。我们进一步提出了一种新的方法，即扩散增强对抗训练(DEAT) ，通过操纵扩散项来提高鲁棒推广能力，而且几乎没有额外的计算负担。从理论上证明了 DEAT 比 PGD-AT 具有更紧的推广界。我们的实证研究是广泛和坚定地证明，DEAT 普遍优于 PGD-AT 的显着差距。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhance+Diffusion+to+Improve+Robust+Generalization)|0|
|[ShapleyFL: Robust Federated Learning Based on Shapley Value](https://doi.org/10.1145/3580305.3599500)|Qiheng Sun, Xiang Li, Jiayao Zhang, Li Xiong, Weiran Liu, Jinfei Liu, Zhan Qin, Kui Ren||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ShapleyFL:+Robust+Federated+Learning+Based+on+Shapley+Value)|0|
|[Mastering Stock Markets with Efficient Mixture of Diversified Trading Experts](https://doi.org/10.1145/3580305.3599424)|Shuo Sun, Xinrun Wang, Wanqi Xue, Xiaoxuan Lou, Bo An||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mastering+Stock+Markets+with+Efficient+Mixture+of+Diversified+Trading+Experts)|0|
|[Joint Pre-training and Local Re-training: Transferable Representation Learning on Multi-source Knowledge Graphs](https://doi.org/10.1145/3580305.3599397)|Zequn Sun, Jiacheng Huang, Jinghao Lin, Xiaozhou Xu, Qijin Chen, Wei Hu|Alibaba Group; Nanjing University|In this paper, we present the ``joint pre-training and local re-training'' framework for learning and applying multi-source knowledge graph (KG) embeddings. We are motivated by the fact that different KGs contain complementary information to improve KG embeddings and downstream tasks. We pre-train a large teacher KG embedding model over linked multi-source KGs and distill knowledge to train a student model for a task-specific KG. To enable knowledge transfer across different KGs, we use entity alignment to build a linked subgraph for connecting the pre-trained KGs and the target KG. The linked subgraph is re-trained for three-level knowledge distillation from the teacher to the student, i.e., feature knowledge distillation, network knowledge distillation, and prediction knowledge distillation, to generate more expressive embeddings. The teacher model can be reused for different target KGs and tasks without having to train from scratch. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our framework.|本文提出了一种用于学习和应用多源知识图(KG)嵌入的“联合预训练和局部再训练”框架。我们的动机是，不同的幼稚园包含互补的信息，以改善幼稚园的嵌入和下游的任务。通过对一个大型教师幼儿园嵌入模型的预训练，提取知识，对一个任务特定的幼儿园进行学生模型的训练。为了实现不同幼稚园之间的知识转移，我们使用实体对齐来建立连接子图，以连接预先训练的幼稚园和目标幼稚园。通过对连通子图的重新训练，实现了从教师到学生的三级知识提取，即特征知识提取、网络知识提取和预测知识提取，从而生成更具表达性的嵌入。教师模型可以重用于不同的目标幼儿园和任务，而无需从头开始培训。我们进行了广泛的实验，以证明我们的框架的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Joint+Pre-training+and+Local+Re-training:+Transferable+Representation+Learning+on+Multi-source+Knowledge+Graphs)|0|
|[PERT-GNN: Latency Prediction for Microservice-based Cloud-Native Applications via Graph Neural Networks](https://doi.org/10.1145/3580305.3599465)|Da Sun Handason Tam, Yang Liu, Huanle Xu, Siyue Xie, Wing Cheong Lau||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PERT-GNN:+Latency+Prediction+for+Microservice-based+Cloud-Native+Applications+via+Graph+Neural+Networks)|0|
|[ExplainableFold: Understanding AlphaFold Prediction with Explainable AI](https://doi.org/10.1145/3580305.3599337)|Juntao Tan, Yongfeng Zhang|Rutgers University|This paper presents ExplainableFold, an explainable AI framework for protein structure prediction. Despite the success of AI-based methods such as AlphaFold in this field, the underlying reasons for their predictions remain unclear due to the black-box nature of deep learning models. To address this, we propose a counterfactual learning framework inspired by biological principles to generate counterfactual explanations for protein structure prediction, enabling a dry-lab experimentation approach. Our experimental results demonstrate the ability of ExplainableFold to generate high-quality explanations for AlphaFold's predictions, providing near-experimental understanding of the effects of amino acids on 3D protein structure. This framework has the potential to facilitate a deeper understanding of protein structures.|本文提出了一个可解释的人工智能框架，用于蛋白质结构预测。尽管 AlphaFold 等基于人工智能的方法在这一领域取得了成功，但由于深度学习模型的黑盒子特性，他们预测的潜在原因仍不清楚。为了解决这个问题，我们提出了一个受生物学原理启发的反事实学习框架，以产生对蛋白质结构预测的反事实解释，从而实现了干实验室实验方法。我们的实验结果证明了 ExplainableFold 对 AlphaFold 的预测产生高质量解释的能力，为氨基酸对3D 蛋白质结构的影响提供了近乎实验性的理解。这个框架有可能促进对蛋白质结构的更深入的理解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ExplainableFold:+Understanding+AlphaFold+Prediction+with+Explainable+AI)|0|
|[Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations](https://doi.org/10.1145/3580305.3599343)|Vy Vo, Trung Le, Van Nguyen, He Zhao, Edwin V. Bonilla, Gholamreza Haffari, Dinh Q. Phung||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Feature-based+Learning+for+Diverse+and+Privacy-Preserving+Counterfactual+Explanations)|0|
|[Adversaries with Limited Information in the Friedkin-Johnsen Model](https://doi.org/10.1145/3580305.3599255)|Sijing Tu, Stefan Neumann, Aristides Gionis|KTH Royal Institute of Technology|In recent years, online social networks have been the target of adversaries who seek to introduce discord into societies, to undermine democracies and to destabilize communities. Often the goal is not to favor a certain side of a conflict but to increase disagreement and polarization. To get a mathematical understanding of such attacks, researchers use opinion-formation models from sociology, such as the Friedkin--Johnsen model, and formally study how much discord the adversary can produce when altering the opinions for only a small set of users. In this line of work, it is commonly assumed that the adversary has full knowledge about the network topology and the opinions of all users. However, the latter assumption is often unrealistic in practice, where user opinions are not available or simply difficult to estimate accurately.   To address this concern, we raise the following question: Can an attacker sow discord in a social network, even when only the network topology is known? We answer this question affirmatively. We present approximation algorithms for detecting a small set of users who are highly influential for the disagreement and polarization in the network. We show that when the adversary radicalizes these users and if the initial disagreement/polarization in the network is not very high, then our method gives a constant-factor approximation on the setting when the user opinions are known. To find the set of influential users, we provide a novel approximation algorithm for a variant of MaxCut in graphs with positive and negative edge weights. We experimentally evaluate our methods, which have access only to the network topology, and we find that they have similar performance as methods that have access to the network topology and all user opinions. We further present an NP-hardness proof, which was an open question by Chen and Racz [IEEE Trans. Netw. Sci. Eng., 2021].|近年来，在线社交网络一直是敌人的目标，他们试图将不和谐引入社会，破坏民主，破坏社区稳定。通常情况下，目标不是支持冲突的某一方，而是增加分歧和两极分化。为了从数学上理解这种攻击，研究人员使用了社会学中的观点形成模型，比如 Friedkin-Johnsen 模型，并正式研究了当只为一小部分用户改变观点时，对手可以产生多少不和谐。在这方面的工作中，通常假设对手完全了解网络拓扑和所有用户的意见。然而，后一种假设在实践中往往是不现实的，因为用户的意见是不可用的，或者仅仅是难以准确估计。为了解决这个问题，我们提出以下问题: 攻击者能否在社交网络中播种不和谐因素，即使只知道网络拓扑？我们肯定地回答这个问题。我们提出了一种近似算法来检测少量的用户，这些用户对网络中的分歧和极化有很大的影响。我们证明了当对手激化这些用户时，如果网络中最初的分歧/极化不是很高，那么我们的方法在用户意见已知的情况下给出了一个常数因子近似的设置。为了找到一组有影响力的用户，我们提供了一个新的近似演算法，对于一个具有正负边权值的图的变体 MaxCut。我们通过实验来评估我们的方法，这些方法只能访问网络拓扑，我们发现它们的性能与那些能访问网络拓扑和所有用户意见的方法相似。我们进一步提出了一个 NP 硬性证明，这是陈和 Racz [ IEEE Trans ]提出的一个开放性问题。网络。科学。工程师，2021]。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adversaries+with+Limited+Information+in+the+Friedkin-Johnsen+Model)|0|
|[Pattern Expansion and Consolidation on Evolving Graphs for Continual Traffic Prediction](https://doi.org/10.1145/3580305.3599463)|Binwu Wang, Yudong Zhang, Xu Wang, Pengkun Wang, Zhengyang Zhou, Lei Bai, Yang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pattern+Expansion+and+Consolidation+on+Evolving+Graphs+for+Continual+Traffic+Prediction)|0|
|[Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning](https://doi.org/10.1145/3580305.3599351)|Daixin Wang, Zhiqiang Zhang, Yeyu Zhao, Kai Huang, Yulin Kang, Jun Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Financial+Default+Prediction+via+Motif-preserving+Graph+Neural+Network+with+Curriculum+Learning)|0|
|[Accelerating Antimicrobial Peptide Discovery with Latent Structure](https://doi.org/10.1145/3580305.3599249)|Danqing Wang, Zeyu Wen, Fei Ye, Lei Li, Hao Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Accelerating+Antimicrobial+Peptide+Discovery+with+Latent+Structure)|0|
|[Efficient and Effective Edge-wise Graph Representation Learning](https://doi.org/10.1145/3580305.3599321)|Hewen Wang, Renchi Yang, Keke Huang, Xiaokui Xiao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Effective+Edge-wise+Graph+Representation+Learning)|0|
|[PROSE: Graph Structure Learning via Progressive Strategy](https://doi.org/10.1145/3580305.3599476)|Huizhao Wang, Yao Fu, Tao Yu, Linghui Hu, Weihao Jiang, Shiliang Pu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PROSE:+Graph+Structure+Learning+via+Progressive+Strategy)|0|
|[Empower Post-hoc Graph Explanations with Information Bottleneck: A Pre-training and Fine-tuning Perspective](https://doi.org/10.1145/3580305.3599330)|Jihong Wang, Minnan Luo, Jundong Li, Yun Lin, Yushun Dong, Jin Song Dong, Qinghua Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empower+Post-hoc+Graph+Explanations+with+Information+Bottleneck:+A+Pre-training+and+Fine-tuning+Perspective)|0|
|[WHEN: A Wavelet-DTW Hybrid Attention Network for Heterogeneous Time Series Analysis](https://doi.org/10.1145/3580305.3599549)|Jingyuan Wang, Chen Yang, Xiaohan Jiang, Junjie Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WHEN:+A+Wavelet-DTW+Hybrid+Attention+Network+for+Heterogeneous+Time+Series+Analysis)|0|
|[Federated Few-shot Learning](https://doi.org/10.1145/3580305.3599347)|Song Wang, Xingbo Fu, Kaize Ding, Chen Chen, Huiyuan Chen, Jundong Li|School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, China|We are interested in developing a unified machine learning framework for effectively training machine learning models from many small data sources such as mobile devices. This is a commonly encountered situation in mobile computing scenarios, where data is scarce and distributed while the tasks are distinct. In this paper, we propose a federated few-shot learning (FedFSL) framework to learn a few-...|我们有兴趣开发一个统一的机器学习框架来有效地训练机器学习模型从许多小数据源，如移动设备。这是移动计算场景中经常遇到的情况，其中数据稀缺且分布式，而任务是不同的。本文提出了一种联邦少镜头学习(FedFSL)框架来学习少镜头学习算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Few-shot+Learning)|0|
|[Contrastive Meta-Learning for Few-shot Node Classification](https://doi.org/10.1145/3580305.3599288)|Song Wang, Zhen Tan, Huan Liu, Jundong Li|University of Virginia; Arizona State University|Few-shot node classification, which aims to predict labels for nodes on graphs with only limited labeled nodes as references, is of great significance in real-world graph mining tasks. Particularly, in this paper, we refer to the task of classifying nodes in classes with a few labeled nodes as the few-shot node classification problem. To tackle such a label shortage issue, existing works generally leverage the meta-learning framework, which utilizes a number of episodes to extract transferable knowledge from classes with abundant labeled nodes and generalizes the knowledge to other classes with limited labeled nodes. In essence, the primary aim of few-shot node classification is to learn node embeddings that are generalizable across different classes. To accomplish this, the GNN encoder must be able to distinguish node embeddings between different classes, while also aligning embeddings for nodes in the same class. Thus, in this work, we propose to consider both the intra-class and inter-class generalizability of the model. We create a novel contrastive meta-learning framework on graphs, named COSMIC, with two key designs. First, we propose to enhance the intra-class generalizability by involving a contrastive two-step optimization in each episode to explicitly align node embeddings in the same classes. Second, we strengthen the inter-class generalizability by generating hard node classes via a novel similarity-sensitive mix-up strategy. Extensive experiments on few-shot node classification datasets verify the superiority of our framework over state-of-the-art baselines. Our code is provided at https://github.com/SongW-SW/COSMIC.|少镜头节点分类是一种仅以有限标记节点为参考的图上节点标记预测方法，在现实图挖掘任务中具有重要意义。特别地，本文将在有少量标记节点的类中对节点进行分类的任务称为少镜头节点分类问题。为了解决这样的标签短缺问题，现有的工作通常利用元学习框架，该框架利用一个系列长度从有大量标签节点的类中提取可转移的知识，并将知识推广到有限标签节点的其他类中。本质上，少镜头节点分类的主要目的是学习可以在不同类之间推广的节点嵌入。为了实现这一点，GNN 编码器必须能够区分不同类之间的节点嵌入，同时对同一类中的节点的嵌入进行对齐。因此，在这项工作中，我们建议同时考虑类内和类间的泛化模型。我们创建了一个新的对比元学习框架，命名为 COSMIC，有两个关键的设计。首先，我们提出通过在每一集中包含一个对比的两步优化来增强类内的泛化能力，以显式地对同一类中的节点嵌入进行对齐。其次，通过一种新的相似敏感混合策略生成硬节点类，增强了类间的泛化能力。在少镜头节点分类数据集上的大量实验验证了该框架相对于最先进基线的优越性。我们的代码是 https://github.com/songw-sw/cosmic 提供的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrastive+Meta-Learning+for+Few-shot+Node+Classification)|0|
|[An Observed Value Consistent Diffusion Model for Imputing Missing Values in Multivariate Time Series](https://doi.org/10.1145/3580305.3599257)|Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang, Zhengyang Zhou, Yang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Observed+Value+Consistent+Diffusion+Model+for+Imputing+Missing+Values+in+Multivariate+Time+Series)|0|
|[Automated 3D Pre-Training for Molecular Property Prediction](https://doi.org/10.1145/3580305.3599252)|Xu Wang, Huan Zhao, WeiWei Tu, Quanming Yao|Tsinghua University; 4Paradigm Inc.|Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction, 3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the \textit{total energy} to search for weight distribution of the three pretext task since total energy corresponding to the quality of 3D conformer.Extensive experiments on 2D molecular graphs are conducted to demonstrate the accuracy, efficiency and generalization ability of the proposed 3D PGT compared to various pre-training baselines.|分子性质预测是药物发现和材料科学中的一个重要问题。由于几何结构已被证明是分子性质预测的必要条件，因此将三维信息与各种图形学习方法相结合以提高预测性能。然而，由于分子几何结构的计算成本较高，在许多实际应用中难以获得分子的几何结构。在这项工作中，我们提出了一个新的3D 预训练框架(称为3D PGT) ，它在3D 分子图上预训练模型，然后在没有3D 结构的分子图上进行微调。基于键长、键角和二面角是对应于完整分子三维构象的三个基本几何描述符这一事实，我们首先基于这三个属性开发了一个多任务生成预训练框架。接下来，为了自动融合这三个生成任务，我们设计了一个替代度量，使用文本{总能量}搜索三个托辞任务的权重分布，因为总能量对应于三维构象的质量。通过对二维分子图的大量实验，验证了所提出的三维 PGT 与各种预训练基线相比的准确性、效率和泛化能力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+3D+Pre-Training+for+Molecular+Property+Prediction)|0|
|[Rapid Image Labeling via Neuro-Symbolic Learning](https://doi.org/10.1145/3580305.3599485)|Yifeng Wang, Zhi Tu, Yiwen Xiang, Shiyuan Zhou, Xiyuan Chen, Bingxuan Li, Tianyi Zhang|The Hong Kong Polytechnic University; University of Toronto; Purdue University; Chongqing University|The success of Computer Vision (CV) relies heavily on manually annotated data. However, it is prohibitively expensive to annotate images in key domains such as healthcare, where data labeling requires significant domain expertise and cannot be easily delegated to crowd workers. To address this challenge, we propose a neuro-symbolic approach called Rapid, which infers image labeling rules from a small amount of labeled data provided by domain experts and automatically labels unannotated data using the rules. Specifically, Rapid combines pre-trained CV models and inductive logic learning to infer the logic-based labeling rules. Rapid achieves a labeling accuracy of 83.33% to 88.33% on four image labeling tasks with only 12 to 39 labeled samples. In particular, Rapid significantly outperforms finetuned CV models in two highly specialized tasks. These results demonstrate the effectiveness of Rapid in learning from small data and its capability to generalize among different tasks. Code and our dataset are publicly available at https://github.com/Neural-Symbolic-Image-Labeling/|计算机视觉(CV)的成功在很大程度上依赖于手工注释的数据。然而，在医疗保健等关键领域对图像进行注释的成本高得令人望而却步，因为在这些领域，数据标签需要大量的领域专业知识，而且不能轻易地委托给群体工作者。为了应对这一挑战，我们提出了一种称为 Rapid 的神经符号方法，它从领域专家提供的少量标记数据中推断出图像标记规则，并使用这些规则自动标记未加注释的数据。具体来说，Rapid 将预先训练好的 CV 模型和归纳逻辑学习相结合，推导出基于逻辑的标记规则。在仅有12 ~ 39个标记样本的情况下，在4个图像标记任务中，该算法的标记准确率分别为83.33% ~ 88.33% 。特别是，在两个高度专业化的任务中，Rapid 的表现明显优于微调 CV 模型。这些结果表明，在小数据学习快速的有效性和它的能力之间的推广不同的任务。代码和我们的数据集在 https://github.com/neural-symbolic-image-labeling/是公开的|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rapid+Image+Labeling+via+Neuro-Symbolic+Learning)|0|
|[Learning to Schedule in Diffusion Probabilistic Models](https://doi.org/10.1145/3580305.3599412)|Yunke Wang, Xiyu Wang, AnhDung Dinh, Bo Du, Charles Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Schedule+in+Diffusion+Probabilistic+Models)|0|
|[A Message Passing Neural Network Space for Better Capturing Data-dependent Receptive Fields](https://doi.org/10.1145/3580305.3599243)|Zhili Wang, Shimin Di, Lei Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Message+Passing+Neural+Network+Space+for+Better+Capturing+Data-dependent+Receptive+Fields)|0|
|[To Aggregate or Not? Learning with Separate Noisy Labels](https://doi.org/10.1145/3580305.3599522)|Jiaheng Wei, Zhaowei Zhu, Tianyi Luo, Ehsan Amid, Abhishek Kumar, Yang Liu|Google Research, Brain Team; Amazon Search Science and AI; University of California, Santa Cruz|The rawly collected training data often comes with separate noisy labels collected from multiple imperfect annotators (e.g., via crowdsourcing). A typical way of using these separate labels is to first aggregate them into one and apply standard training methods. The literature has also studied extensively on effective aggregation approaches. This paper revisits this choice and aims to provide an answer to the question of whether one should aggregate separate noisy labels into single ones or use them separately as given. We theoretically analyze the performance of both approaches under the empirical risk minimization framework for a number of popular loss functions, including the ones designed specifically for the problem of learning with noisy labels. Our theorems conclude that label separation is preferred over label aggregation when the noise rates are high, or the number of labelers/annotations is insufficient. Extensive empirical results validate our conclusions.|原始收集的训练数据通常伴随着从多个不完美的注释者(例如，通过众包)收集的单独的噪音标签。使用这些单独标签的一个典型方法是首先将它们聚合成一个标签，然后应用标准的培训方法。文献还广泛研究了有效的聚合方法。本文重新审视这一选择，旨在回答是否应该将分离的噪声标签合并为单个标签，还是按照给定的方式分别使用这些标签的问题。我们从理论上分析了这两种方法在经验风险最小化框架下对一些流行的损失函数的性能，包括那些专门为带噪声标签的学习问题而设计的函数。我们的定理得出结论，当噪声率较高或标签/注释数量不足时，标签分离优于标签聚合。大量的实证结果验证了我们的结论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=To+Aggregate+or+Not?+Learning+with+Separate+Noisy+Labels)|0|
|[Granger Causal Chain Discovery for Sepsis-Associated Derangements via Continuous-Time Hawkes Processes](https://doi.org/10.1145/3580305.3599369)|Song Wei, Yao Xie, Christopher S. Josef, Rishikesan Kamaleswaran||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Granger+Causal+Chain+Discovery+for+Sepsis-Associated+Derangements+via+Continuous-Time+Hawkes+Processes)|0|
|[Deep Bayesian Active Learning for Accelerating Stochastic Simulation](https://doi.org/10.1145/3580305.3599300)|Dongxia Wu, Ruijia Niu, Matteo Chinazzi, Alessandro Vespignani, YiAn Ma, Rose Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Bayesian+Active+Learning+for+Accelerating+Stochastic+Simulation)|0|
|[Self-Adaptive Perturbation Radii for Adversarial Training](https://doi.org/10.1145/3580305.3599495)|Huimin Wu, Wanli Shi, Chenkang Zhang, Bin Gu|NUIST; Nanjing University of Information Science & Technology; Mohamed bin Zayed University of Artificial Intelligence|Adversarial training has been shown to be the most popular and effective  technique to protect models from imperceptible adversarial samples. Despite its success, it also accompanies the significant performance degeneration to clean data. To achieve a good performance on both clean and adversarial samples, the main effort  is searching for an adaptive perturbation radius for each training sample, which essentially suffers from a  conflict between exact searching  and  computational overhead. To address this conflict, in this paper, firstly we show the superiority of adaptive perturbation radii intuitively and theoretically regarding the   accuracy and robustness respectively. Then we propose our novel self-adaptive adjustment framework for perturbation radii without tedious searching. We also discuss this framework on both deep neural networks (DNNs) and kernel support vector machines (SVMs).  Finally, extensive experimental results show that our framework can improve not only natural generalization performance but also adversarial robustness. It is also competitive with existing searching strategies in terms of running time.|对抗性训练已被证明是最流行和有效的技术，以保护模型从不易察觉的对抗性样本。尽管它取得了成功，但在清理数据方面也伴随着显著的性能退化。为了在干净样本和对手样本上获得良好的性能，主要的工作是为每个训练样本搜索一个自适应扰动半径，这实质上遭受了精确搜索和计算开销之间的冲突。针对这一矛盾，本文首先从理论上和直观上分别论证了自适应摄动半径在精度和鲁棒性方面的优越性。然后，我们提出了新的自适应调整框架扰动半径没有繁琐的搜索。我们还讨论了深度神经网络(DNN)和核支持向量机(SVM)的框架。最后，大量的实验结果表明，该框架不仅提高了自然泛化性能，而且增强了对抗鲁棒性。在运行时间方面，它也与现有的搜索策略具有竞争力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Adaptive+Perturbation+Radii+for+Adversarial+Training)|0|
|[DECOR: Degree-Corrected Social Graph Refinement for Fake News Detection](https://doi.org/10.1145/3580305.3599298)|Jiaying Wu, Bryan Hooi|National University of Singapore|Recent efforts in fake news detection have witnessed a surge of interest in using graph neural networks (GNNs) to exploit rich social context. Existing studies generally leverage fixed graph structures, assuming that the graphs accurately represent the related social engagements. However, edge noise remains a critical challenge in real-world graphs, as training on suboptimal structures can severely limit the expressiveness of GNNs. Despite initial efforts in graph structure learning (GSL), prior works often leverage node features to update edge weights, resulting in heavy computational costs that hinder the methods' applicability to large-scale social graphs. In this work, we approach the fake news detection problem with a novel aspect of social graph refinement. We find that the degrees of news article nodes exhibit distinctive patterns, which are indicative of news veracity. Guided by this, we propose DECOR, a novel application of Degree-Corrected Stochastic Blockmodels to the fake news detection problem. Specifically, we encapsulate our empirical observations into a lightweight social graph refinement component that iteratively updates the edge weights via a learnable degree correction mask, which allows for joint optimization with a GNN-based detector. Extensive experiments on two real-world benchmarks validate the effectiveness and efficiency of DECOR.|最近在假新闻检测方面的努力见证了利用图形神经网络(GNN)来开发丰富的社会背景的兴趣激增。现有的研究通常利用固定的图结构，假设图准确地表示相关的社会活动。然而，边缘噪声仍然是一个关键的挑战，在现实世界的图，因为训练次优结构可以严重限制 GNN 的表达。尽管在图结构学习(GSL)方面做出了初步的努力，但是以前的工作经常利用节点特征来更新边权重，从而导致沉重的计算代价，阻碍了该方法对大规模社会图的适用性。在这项工作中，我们探讨了假新闻检测问题与一个新的方面的社会图精化。我们发现新闻文章节点的程度表现出独特的模式，这表明了新闻的真实性。在此基础上，我们提出了一种基于度修正随机块模型的伪新闻检测方法 DECOR。具体来说，我们将我们的经验观察封装到一个轻量级的社会图精化组件中，该组件通过一个可学习的度修正掩码迭代更新边缘权重，该掩码允许与基于 GNN 的检测器进行联合优化。在两个实际基准上的大量实验验证了 DECOR 的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DECOR:+Degree-Corrected+Social+Graph+Refinement+for+Fake+News+Detection)|0|
|[Certified Edge Unlearning for Graph Neural Networks](https://doi.org/10.1145/3580305.3599271)|Kun Wu, Jie Shen, Yue Ning, Ting Wang, Wendy Hui Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Certified+Edge+Unlearning+for+Graph+Neural+Networks)|0|
|[Recognizing Unseen Objects via Multimodal Intensive Knowledge Graph Propagation](https://doi.org/10.1145/3580305.3599486)|Likang Wu, Zhi Li, Hongke Zhao, Zhefeng Wang, Qi Liu, Baoxing Huai, Nicholas Jing Yuan, Enhong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recognizing+Unseen+Objects+via+Multimodal+Intensive+Knowledge+Graph+Propagation)|0|
|[Towards Reliable Rare Category Analysis on Graphs via Individual Calibration](https://doi.org/10.1145/3580305.3599525)|Longfeng Wu, Bowen Lei, Dongkuan Xu, Dawei Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Reliable+Rare+Category+Analysis+on+Graphs+via+Individual+Calibration)|0|
|[TransformerLight: A Novel Sequence Modeling Based Traffic Signaling Mechanism via Gated Transformer](https://doi.org/10.1145/3580305.3599530)|Qiang Wu, Mingyuan Li, Jun Shen, Linyuan Lü, Bo Du, Ke Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TransformerLight:+A+Novel+Sequence+Modeling+Based+Traffic+Signaling+Mechanism+via+Gated+Transformer)|0|
|[MedLink: De-Identified Patient Health Record Linkage](https://doi.org/10.1145/3580305.3599427)|Zhenbang Wu, Cao Xiao, Jimeng Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MedLink:+De-Identified+Patient+Health+Record+Linkage)|0|
|[A Sequence-to-Sequence Approach with Mixed Pointers to Topic Segmentation and Segment Labeling](https://doi.org/10.1145/3580305.3599245)|Jinxiong Xia, Houfeng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Sequence-to-Sequence+Approach+with+Mixed+Pointers+to+Topic+Segmentation+and+Segment+Labeling)|0|
|[Graph Contrastive Learning with Generative Adversarial Network](https://doi.org/10.1145/3580305.3599370)|Cheng Wu, Chaokun Wang, Jingcao Xu, Ziyang Liu, Kai Zheng, Xiaowei Wang, Yang Song, Kun Gai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Contrastive+Learning+with+Generative+Adversarial+Network)|0|
|[A Causality Inspired Framework for Model Interpretation](https://doi.org/10.1145/3580305.3599240)|Chenwang Wu, Xiting Wang, Defu Lian, Xing Xie, Enhong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Causality+Inspired+Framework+for+Model+Interpretation)|0|
|[Imputation-based Time-Series Anomaly Detection with Conditional Weight-Incremental Diffusion Models](https://doi.org/10.1145/3580305.3599391)|Chunjing Xiao, Zehua Gou, Wenxin Tai, Kunpeng Zhang, Fan Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Imputation-based+Time-Series+Anomaly+Detection+with+Conditional+Weight-Incremental+Diffusion+Models)|0|
|[Spatial Heterophily Aware Graph Neural Networks](https://doi.org/10.1145/3580305.3599510)|Congxi Xiao, Jingbo Zhou, Jizhou Huang, Tong Xu, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatial+Heterophily+Aware+Graph+Neural+Networks)|0|
|[A Dual-Agent Scheduler for Distributed Deep Learning Jobs on Public Cloud via Reinforcement Learning](https://doi.org/10.1145/3580305.3599241)|Mingzhe Xing, Hangyu Mao, Shenglin Yin, Lichen Pan, Zhengchao Zhang, Zhen Xiao, Jieyi Long||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Dual-Agent+Scheduler+for+Distributed+Deep+Learning+Jobs+on+Public+Cloud+via+Reinforcement+Learning)|0|
|[How does the Memorization of Neural Networks Impact Adversarial Robust Models?](https://doi.org/10.1145/3580305.3599381)|Han Xu, Xiaorui Liu, Wentao Wang, Zitao Liu, Anil K. Jain, Jiliang Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+does+the+Memorization+of+Neural+Networks+Impact+Adversarial+Robust+Models?)|0|
|[Internal Logical Induction for Pixel-Symbolic Reinforcement Learning](https://doi.org/10.1145/3580305.3599393)|Jiacheng Xu, Chao Chen, Fuxiang Zhang, Lei Yuan, Zongzhang Zhang, Yang Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Internal+Logical+Induction+for+Pixel-Symbolic+Reinforcement+Learning)|0|
|[Node Classification Beyond Homophily: Towards a General Solution](https://doi.org/10.1145/3580305.3599446)|Zhe Xu, Yuzhong Chen, Qinghai Zhou, Yuhang Wu, Menghai Pan, Hao Yang, Hanghang Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Node+Classification+Beyond+Homophily:+Towards+a+General+Solution)|0|
|[CriticalFL: A Critical Learning Periods Augmented Client Selection Framework for Efficient Federated Learning](https://doi.org/10.1145/3580305.3599293)|Gang Yan, Hao Wang, Xu Yuan, Jian Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CriticalFL:+A+Critical+Learning+Periods+Augmented+Client+Selection+Framework+for+Efficient+Federated+Learning)|0|
|[Fragility Index: A New Approach for Binary Classification](https://doi.org/10.1145/3580305.3599356)|Chen Yang, Ziqiang Zhang, Bo Cao, Zheng Cui, Bin Hu, Tong Li, Daniel Zhuoyu Long, Jin Qi, Feng Wang, Ruohan Zhan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fragility+Index:+A+New+Approach+for+Binary+Classification)|0|
|[IDToolkit: A Toolkit for Benchmarking and Developing Inverse Design Algorithms in Nanophotonics](https://doi.org/10.1145/3580305.3599385)|JiaQi Yang, Yucheng Xu, JiaLei Shen, KeBin Fan, DeChuan Zhan, Yang Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IDToolkit:+A+Toolkit+for+Benchmarking+and+Developing+Inverse+Design+Algorithms+in+Nanophotonics)|0|
|[MAPLE: Semi-Supervised Learning with Multi-Alignment and Pseudo-Learning](https://doi.org/10.1145/3580305.3599423)|Juncheng Yang, Chao Li, Zuchao Li, Wei Yu, Bo Du, Shijun Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MAPLE:+Semi-Supervised+Learning+with+Multi-Alignment+and+Pseudo-Learning)|0|
|[EXTRACT and REFINE: Finding a Support Subgraph Set for Graph Representation](https://doi.org/10.1145/3580305.3599339)|Kuo Yang, Zhengyang Zhou, Wei Sun, Pengkun Wang, Xu Wang, Yang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EXTRACT+and+REFINE:+Finding+a+Support+Subgraph+Set+for+Graph+Representation)|0|
|[κHGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning](https://doi.org/10.1145/3580305.3599532)|Menglin Yang, Min Zhou, Lujia Pan, Irwin King||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=κHGCN:+Tree-likeness+Modeling+via+Continuous+and+Discrete+Curvature+Learning)|0|
|[Specify Robust Causal Representation from Mixed Observations](https://doi.org/10.1145/3580305.3599512)|Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, Jun Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Specify+Robust+Causal+Representation+from+Mixed+Observations)|0|
|[Counterfactual Learning on Heterogeneous Graphs with Greedy Perturbation](https://doi.org/10.1145/3580305.3599289)|Qiang Yang, Changsheng Ma, Qiannan Zhang, Xin Gao, Chuxu Zhang, Xiangliang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Learning+on+Heterogeneous+Graphs+with+Greedy+Perturbation)|0|
|[LightPath: Lightweight and Scalable Path Representation Learning](https://doi.org/10.1145/3580305.3599415)|Sean Bin Yang, Jilin Hu, Chenjuan Guo, Bin Yang, Christian S. Jensen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LightPath:+Lightweight+and+Scalable+Path+Representation+Learning)|0|
|[Test Accuracy vs. Generalization Gap: Model Selection in NLP without Accessing Training or Testing Data](https://doi.org/10.1145/3580305.3599518)|Yaoqing Yang, Ryan Theisen, Liam Hodgkinson, Joseph E. Gonzalez, Kannan Ramchandran, Charles H. Martin, Michael W. Mahoney||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Test+Accuracy+vs.+Generalization+Gap:+Model+Selection+in+NLP+without+Accessing+Training+or+Testing+Data)|0|
|[DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection](https://doi.org/10.1145/3580305.3599295)|Yiyuan Yang, Chaoli Zhang, Tian Zhou, Qingsong Wen, Liang Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DCdetector:+Dual+Attention+Contrastive+Representation+Learning+for+Time+Series+Anomaly+Detection)|0|
|[Improving the Expressiveness of K-hop Message-Passing GNNs by Injecting Contextualized Substructure Information](https://doi.org/10.1145/3580305.3599390)|Tianjun Yao, Yingxu Wang, Kun Zhang, Shangsong Liang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+the+Expressiveness+of+K-hop+Message-Passing+GNNs+by+Injecting+Contextualized+Substructure+Information)|0|
|[PAT: Geometry-Aware Hard-Label Black-Box Adversarial Attacks on Text](https://doi.org/10.1145/3580305.3599461)|Muchao Ye, Jinghui Chen, Chenglin Miao, Han Liu, Ting Wang, Fenglong Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PAT:+Geometry-Aware+Hard-Label+Black-Box+Adversarial+Attacks+on+Text)|0|
|[Less is More: SlimG for Accurate, Robust, and Interpretable Graph Mining](https://doi.org/10.1145/3580305.3599413)|Jaemin Yoo, MengChieh Lee, Shubhranshu Shekhar, Christos Faloutsos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Less+is+More:+SlimG+for+Accurate,+Robust,+and+Interpretable+Graph+Mining)|0|
|[FLAMES2Graph: An Interpretable Federated Multivariate Time Series Classification Framework](https://doi.org/10.1145/3580305.3599354)|Raneen Younis, Zahra Ahmadi, Abdul Hakmeh, Marco Fisichella||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FLAMES2Graph:+An+Interpretable+Federated+Multivariate+Time+Series+Classification+Framework)|0|
|[Towards Variance Reduction for Reinforcement Learning of Industrial Decision-making Tasks: A Bi-Critic based Demand-Constraint Decoupling Approach](https://doi.org/10.1145/3580305.3599527)|Jianyong Yuan, Jiayi Zhang, Zinuo Cai, Junchi Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Variance+Reduction+for+Reinforcement+Learning+of+Industrial+Decision-making+Tasks:+A+Bi-Critic+based+Demand-Constraint+Decoupling+Approach)|0|
|[Spatio-temporal Diffusion Point Processes](https://doi.org/10.1145/3580305.3599511)|Yuan Yuan, Jingtao Ding, Chenyang Shao, Depeng Jin, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatio-temporal+Diffusion+Point+Processes)|0|
|[Hyperbolic Graph Topic Modeling Network with Continuously Updated Topic Tree](https://doi.org/10.1145/3580305.3599384)|Delvin Ce Zhang, Rex Ying, Hady W. Lauw||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyperbolic+Graph+Topic+Modeling+Network+with+Continuously+Updated+Topic+Tree)|0|
|[Quantifying Node Importance over Network Structural Stability](https://doi.org/10.1145/3580305.3599480)|Fan Zhang, Qingyuan Linghu, Jiadong Xie, Kai Wang, Xuemin Lin, Wenjie Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantifying+Node+Importance+over+Network+Structural+Stability)|0|
|[HiMacMic: Hierarchical Multi-Agent Deep Reinforcement Learning with Dynamic Asynchronous Macro Strategy](https://doi.org/10.1145/3580305.3599379)|Hancheng Zhang, Guozheng Li, Chi Harold Liu, Guoren Wang, Jian Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HiMacMic:+Hierarchical+Multi-Agent+Deep+Reinforcement+Learning+with+Dynamic+Asynchronous+Macro+Strategy)|0|
|[A Study of Situational Reasoning for Traffic Understanding](https://doi.org/10.1145/3580305.3599246)|Jiarui Zhang, Filip Ilievski, Kaixin Ma, Aravinda Kollaa, Jonathan Francis, Alessandro Oltramari||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Study+of+Situational+Reasoning+for+Traffic+Understanding)|0|
|[MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation](https://doi.org/10.1145/3580305.3599435)|Jiaxing Zhang, Dongsheng Luo, Hua Wei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MixupExplainer:+Generalizing+Explanations+for+Graph+Neural+Networks+with+Data+Augmentation)|0|
|[Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time Series](https://doi.org/10.1145/3580305.3599543)|Jiawen Zhang, Shun Zheng, Wei Cao, Jiang Bian, Jia Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Warpformer:+A+Multi-scale+Modeling+Approach+for+Irregular+Clinical+Time+Series)|0|
|[Navigating Alignment for Non-identical Client Class Sets: A Label Name-Anchored Federated Learning Framework](https://doi.org/10.1145/3580305.3599443)|Jiayun Zhang, Xiyuan Zhang, Xinyang Zhang, Dezhi Hong, Rajesh K. Gupta, Jingbo Shang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Navigating+Alignment+for+Non-identical+Client+Class+Sets:+A+Label+Name-Anchored+Federated+Learning+Framework)|0|
|[DyTed: Disentangled Representation Learning for Discrete-time Dynamic Graph](https://doi.org/10.1145/3580305.3599319)|Kaike Zhang, Qi Cao, Gaolin Fang, Bingbing Xu, Hongjian Zou, Huawei Shen, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DyTed:+Disentangled+Representation+Learning+for+Discrete-time+Dynamic+Graph)|0|
|[Rumor Detection with Diverse Counterfactual Evidence](https://doi.org/10.1145/3580305.3599494)|Kaiwei Zhang, Junchi Yu, Haichao Shi, Jian Liang, XiaoYu Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rumor+Detection+with+Diverse+Counterfactual+Evidence)|0|
|[Local Boosting for Weakly-Supervised Learning](https://doi.org/10.1145/3580305.3599417)|Rongzhi Zhang, Yue Yu, Jiaming Shen, Xiquan Cui, Chao Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Local+Boosting+for+Weakly-Supervised+Learning)|0|
|[Capacity Constrained Influence Maximization in Social Networks](https://doi.org/10.1145/3580305.3599267)|Shiqi Zhang, Yiqian Huang, Jiachen Sun, Wenqing Lin, Xiaokui Xiao, Bo Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Capacity+Constrained+Influence+Maximization+in+Social+Networks)|0|
|[AdaProp: Learning Adaptive Propagation for Graph Neural Network based Knowledge Graph Reasoning](https://doi.org/10.1145/3580305.3599404)|Yongqi Zhang, Zhanke Zhou, Quanming Yao, Xiaowen Chu, Bo Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdaProp:+Learning+Adaptive+Propagation+for+Graph+Neural+Network+based+Knowledge+Graph+Reasoning)|0|
|[Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers](https://doi.org/10.1145/3580305.3599544)|Yu Zhang, Bowen Jin, Xiusi Chen, Yanzhen Shen, Yunyi Zhang, Yu Meng, Jiawei Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weakly+Supervised+Multi-Label+Classification+of+Full-Text+Scientific+Papers)|0|
|[DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting](https://doi.org/10.1145/3580305.3599315)|Lifan Zhao, Shuming Kong, Yanyan Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DoubleAdapt:+A+Meta-learning+Approach+to+Incremental+Learning+for+Stock+Trend+Forecasting)|0|
|[Spatial Clustering Regression of Count Value Data via Bayesian Mixture of Finite Mixtures](https://doi.org/10.1145/3580305.3599509)|Peng Zhao, HouCheng Yang, Dipak K. Dey, Guanyu Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatial+Clustering+Regression+of+Count+Value+Data+via+Bayesian+Mixture+of+Finite+Mixtures)|0|
|[Skill Disentanglement for Imitation Learning from Suboptimal Demonstrations](https://doi.org/10.1145/3580305.3599506)|Tianxiang Zhao, Wenchao Yu, Suhang Wang, Lu Wang, Xiang Zhang, Yuncong Chen, Yanchi Liu, Wei Cheng, Haifeng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Skill+Disentanglement+for+Imitation+Learning+from+Suboptimal+Demonstrations)|0|
|[GraphGLOW: Universal and Generalizable Structure Learning for Graph Neural Networks](https://doi.org/10.1145/3580305.3599373)|Wentao Zhao, Qitian Wu, Chenxiao Yang, Junchi Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphGLOW:+Universal+and+Generalizable+Structure+Learning+for+Graph+Neural+Networks)|0|
|[Generative Causal Interpretation Model for Spatio-Temporal Representation Learning](https://doi.org/10.1145/3580305.3599363)|Yu Zhao, Pan Deng, Junting Liu, Xiaofeng Jia, Jianwei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Causal+Interpretation+Model+for+Spatio-Temporal+Representation+Learning)|0|
|[Maintaining the Status Quo: Capturing Invariant Relations for OOD Spatiotemporal Learning](https://doi.org/10.1145/3580305.3599421)|Zhengyang Zhou, Qihe Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, Yang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Maintaining+the+Status+Quo:+Capturing+Invariant+Relations+for+OOD+Spatiotemporal+Learning)|0|
|[Dual-view Molecular Pre-training](https://doi.org/10.1145/3580305.3599317)|Jinhua Zhu, Yingce Xia, Lijun Wu, Shufang Xie, Wengang Zhou, Tao Qin, Houqiang Li, TieYan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual-view+Molecular+Pre-training)|0|
|[On Structural Expressive Power of Graph Transformers](https://doi.org/10.1145/3580305.3599451)|Wenhao Zhu, Tianyu Wen, Guojie Song, Liang Wang, Bo Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Structural+Expressive+Power+of+Graph+Transformers)|0|
|[WinGNN: Dynamic Graph Neural Networks with Random Gradient Aggregation Window](https://doi.org/10.1145/3580305.3599551)|Yifan Zhu, Fangpeng Cong, Dan Zhang, Wenwen Gong, Qika Lin, Wenzheng Feng, Yuxiao Dong, Jie Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WinGNN:+Dynamic+Graph+Neural+Networks+with+Random+Gradient+Aggregation+Window)|0|
|[DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling](https://doi.org/10.1145/3580305.3599318)|Yuchen Zhuang, Yue Yu, Lingkai Kong, Xiang Chen, Chao Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DyGen:+Learning+from+Noisy+Labels+via+Dynamics-Enhanced+Generative+Modeling)|0|
|[Learning to Solve Grouped 2D Bin Packing Problems in the Manufacturing Industry](https://doi.org/10.1145/3580305.3599860)|Wenxuan Ao, Guozhen Zhang, Yong Li, Depeng Jin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Solve+Grouped+2D+Bin+Packing+Problems+in+the+Manufacturing+Industry)|0|
|[Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive Text Summarization (TL;DR) of Scientific Contents](https://doi.org/10.1145/3580305.3599830)|Yash Kumar Atri, Vikram Goyal, Tanmoy Chakraborty||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fusing+Multimodal+Signals+on+Hyper-complex+Space+for+Extreme+Abstractive+Text+Summarization+(TL;DR)+of+Scientific+Contents)|0|
|[Web-Scale Academic Name Disambiguation: The WhoIsWho Benchmark, Leaderboard, and Toolkit](https://doi.org/10.1145/3580305.3599930)|Bo Chen, Jing Zhang, Fanjin Zhang, Tianyi Han, Yuqing Cheng, Xiaoyan Li, Yuxiao Dong, Jie Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Web-Scale+Academic+Name+Disambiguation:+The+WhoIsWho+Benchmark,+Leaderboard,+and+Toolkit)|0|
|[FS-REAL: Towards Real-World Cross-Device Federated Learning](https://doi.org/10.1145/3580305.3599829)|Daoyuan Chen, Dawei Gao, Yuexiang Xie, Xuchen Pan, Zitao Li, Yaliang Li, Bolin Ding, Jingren Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FS-REAL:+Towards+Real-World+Cross-Device+Federated+Learning)|0|
|[A Data-driven Region Generation Framework for Spatiotemporal Transportation Service Management](https://doi.org/10.1145/3580305.3599760)|Liyue Chen, Jiangyi Fang, Zhe Yu, Yongxin Tong, Shaosheng Cao, Leye Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Data-driven+Region+Generation+Framework+for+Spatiotemporal+Transportation+Service+Management)|0|
|[Binary Classifier Evaluation on Unlabeled Segments using Inverse Distance Weighting with Distance Learning](https://doi.org/10.1145/3580305.3599781)|Xu Chen, Katerina Marazopoulou, Wesley Lee, Christine Agarwal, Jason Sukumaran, Aude Hofleitner||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Binary+Classifier+Evaluation+on+Unlabeled+Segments+using+Inverse+Distance+Weighting+with+Distance+Learning)|0|
|[Evolve Path Tracer: Early Detection of Malicious Addresses in Cryptocurrency](https://doi.org/10.1145/3580305.3599817)|Ling Cheng, Feida Zhu, Yong Wang, Ruicheng Liang, Huiwen Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evolve+Path+Tracer:+Early+Detection+of+Malicious+Addresses+in+Cryptocurrency)|0|
|[Conditional Neural ODE Processes for Individual Disease Progression Forecasting: A Case Study on COVID-19](https://doi.org/10.1145/3580305.3599792)|Ting Dang, Jing Han, Tong Xia, Erika Bondareva, Chloë SiegeleBrown, Jagmohan Chauhan, Andreas Grammenos, Dimitris Spathis, Pietro Cicuta, Cecilia Mascolo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conditional+Neural+ODE+Processes+for+Individual+Disease+Progression+Forecasting:+A+Case+Study+on+COVID-19)|0|
|[Time-to-Event Modeling with Hypernetwork based Hawkes Process](https://doi.org/10.1145/3580305.3599912)|Manisha Dubey, P. K. Srijith, Maunendra Sankar Desarkar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Time-to-Event+Modeling+with+Hypernetwork+based+Hawkes+Process)|0|
|[Discovering Novel Biological Traits From Images Using Phylogeny-Guided Neural Networks](https://doi.org/10.1145/3580305.3599808)|Mohannad Elhamod, Mridul Khurana, Harish Babu Manogaran, Josef C. Uyeda, Meghan A. Balk, Wasila M. Dahdul, Yasin Bakis, Henry L. Bart Jr., Paula M. Mabee, Hilmar Lapp, James P. Balhoff, Caleb Charpentier, David Carlyn, WeiLun Chao, Charles V. Stewart, Daniel I. Rubenstein, Tanya Y. BergerWolf, Anuj Karpatne||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Discovering+Novel+Biological+Traits+From+Images+Using+Phylogeny-Guided+Neural+Networks)|0|
|[Demystifying Fraudulent Transactions and Illicit Nodes in the Bitcoin Network for Financial Forensics](https://doi.org/10.1145/3580305.3599803)|Youssef Elmougy, Ling Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Demystifying+Fraudulent+Transactions+and+Illicit+Nodes+in+the+Bitcoin+Network+for+Financial+Forensics)|0|
|[RecruitPro: A Pretrained Language Model with Skill-Aware Prompt Learning for Intelligent Recruitment](https://doi.org/10.1145/3580305.3599894)|Chuyu Fang, Chuan Qin, Qi Zhang, Kaichun Yao, Jingshuai Zhang, Hengshu Zhu, Fuzhen Zhuang, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecruitPro:+A+Pretrained+Language+Model+with+Skill-Aware+Prompt+Learning+for+Intelligent+Recruitment)|0|
|[A Lightweight, Efficient and Explainable-by-Design Convolutional Neural Network for Internet Traffic Classification](https://doi.org/10.1145/3580305.3599762)|Kevin Fauvel, Fuxing Chen, Dario Rossi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Lightweight,+Efficient+and+Explainable-by-Design+Convolutional+Neural+Network+for+Internet+Traffic+Classification)|0|
|[ILRoute: A Graph-based Imitation Learning Method to Unveil Riders' Routing Strategies in Food Delivery Service](https://doi.org/10.1145/3580305.3599844)|Tao Feng, Huan Yan, Huandong Wang, Wenzhen Huang, Yuyang Han, Hongsen Liao, Jinghua Hao, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ILRoute:+A+Graph-based+Imitation+Learning+Method+to+Unveil+Riders'+Routing+Strategies+in+Food+Delivery+Service)|0|
|[Influence Maximization with Fairness at Scale](https://doi.org/10.1145/3580305.3599847)|Yuting Feng, Ankitkumar Patel, Bogdan Cautis, Hossein Vahabi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Influence+Maximization+with+Fairness+at+Scale)|0|
|[Yggdrasil Decision Forests: A Fast and Extensible Decision Forests Library](https://doi.org/10.1145/3580305.3599933)|Mathieu GuillameBert, Sebastian Bruch, Richard Stotz, Jan Pfeifer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Yggdrasil+Decision+Forests:+A+Fast+and+Extensible+Decision+Forests+Library)|0|
|[Towards Equitable Assignment: Data-Driven Delivery Zone Partition at Last-mile Logistics](https://doi.org/10.1145/3580305.3599915)|Baoshen Guo, Shuai Wang, Haotian Wang, Yunhuai Liu, Fanshuo Kong, Desheng Zhang, Tian He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Equitable+Assignment:+Data-Driven+Delivery+Zone+Partition+at+Last-mile+Logistics)|0|
|[An Interpretable, Flexible, and Interactive Probabilistic Framework for Melody Generation](https://doi.org/10.1145/3580305.3599772)|Stephen Hahn, Rico Zhu, Simon Mak, Cynthia Rudin, Yue Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Interpretable,+Flexible,+and+Interactive+Probabilistic+Framework+for+Melody+Generation)|0|
|[Efficient Continuous Space Policy Optimization for High-frequency Trading](https://doi.org/10.1145/3580305.3599813)|Li Han, Nan Ding, Guoxuan Wang, Dawei Cheng, Yuqi Liang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Continuous+Space+Policy+Optimization+for+High-frequency+Trading)|0|
|[Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering](https://doi.org/10.1145/3580305.3599819)|Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M. Summers, Yingying Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Expert+Knowledge-Aware+Image+Difference+Graph+Representation+Learning+for+Difference-Aware+Medical+Visual+Question+Answering)|0|
|[Graph Learning in Physical-informed Mesh-reduced Space for Real-world Dynamic Systems](https://doi.org/10.1145/3580305.3599835)|Yeping Hu, Bo Lei, Victor M. Castillo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Learning+in+Physical-informed+Mesh-reduced+Space+for+Real-world+Dynamic+Systems)|0|
|[Multimodal Indoor Localisation in Parkinson's Disease for Detecting Medication Use: Observational Pilot Study in a Free-Living Setting](https://doi.org/10.1145/3580305.3599872)|Ferdian Jovan, Catherine Morgan, Ryan McConville, Emma L. Tonkin, Ian Craddock, Alan L. Whone||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Indoor+Localisation+in+Parkinson's+Disease+for+Detecting+Medication+Use:+Observational+Pilot+Study+in+a+Free-Living+Setting)|0|
|[Ball Trajectory Inference from Multi-Agent Sports Contexts Using Set Transformer and Hierarchical Bi-LSTM](https://doi.org/10.1145/3580305.3599779)|Hyunsung Kim, HanJun Choi, Chang Jo Kim, Jinsung Yoon, SangKi Ko||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ball+Trajectory+Inference+from+Multi-Agent+Sports+Contexts+Using+Set+Transformer+and+Hierarchical+Bi-LSTM)|0|
|[Neural Insights for Digital Marketing Content Design](https://doi.org/10.1145/3580305.3599875)|Fanjie Kong, Yuan Li, Houssam Nassif, Tanner Fiez, Ricardo Henao, Shreya Chakrabarti||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Insights+for+Digital+Marketing+Content+Design)|0|
|[Revisiting Hate Speech Benchmarks: From Data Curation to System Deployment](https://doi.org/10.1145/3580305.3599896)|Atharva Kulkarni, Sarah Masud, Vikram Goyal, Tanmoy Chakraborty||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Hate+Speech+Benchmarks:+From+Data+Curation+to+System+Deployment)|0|
|[Towards Suicide Prevention from Bipolar Disorder with Temporal Symptom-Aware Multitask Learning](https://doi.org/10.1145/3580305.3599917)|Daeun Lee, Sejung Son, Hyolim Jeon, Seungbae Kim, Jinyoung Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Suicide+Prevention+from+Bipolar+Disorder+with+Temporal+Symptom-Aware+Multitask+Learning)|0|
|[Learning Slow and Fast System Dynamics via Automatic Separation of Time Scales](https://doi.org/10.1145/3580305.3599858)|Ruikun Li, Huandong Wang, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Slow+and+Fast+System+Dynamics+via+Automatic+Separation+of+Time+Scales)|0|
|[Diga: Guided Diffusion Model for Graph Recovery in Anti-Money Laundering](https://doi.org/10.1145/3580305.3599806)|Xujia Li, Yuan Li, Xueying Mo, Hebing Xiao, Yanyan Shen, Lei Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diga:+Guided+Diffusion+Model+for+Graph+Recovery+in+Anti-Money+Laundering)|0|
|[HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline](https://doi.org/10.1145/3580305.3599837)|Yang Li, Xinyan Chen, Wenxuan Guo, Xijun Li, Wanqian Luo, Junhua Huang, HuiLing Zhen, Mingxuan Yuan, Junchi Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HardSATGEN:+Understanding+the+Difficulty+of+Hard+SAT+Formula+Generation+and+A+Strong+Structure-Hardness-Aware+Baseline)|0|
|[Learning Joint Relational Co-evolution in Spatial-Temporal Knowledge Graph for SMEs Supply Chain Prediction](https://doi.org/10.1145/3580305.3599855)|Youru Li, Zhenfeng Zhu, Xiaobo Guo, Linxun Chen, Zhouyin Wang, Yinmeng Wang, Bing Han, Yao Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Joint+Relational+Co-evolution+in+Spatial-Temporal+Knowledge+Graph+for+SMEs+Supply+Chain+Prediction)|0|
|[Analysis of COVID-19 Offensive Tweets and Their Targets](https://doi.org/10.1145/3580305.3599773)|Song Liao, Ebuka Okpala, Long Cheng, Mingqi Li, Nishant Vishwamitra, Hongxin Hu, Feng Luo, Matthew Costello||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analysis+of+COVID-19+Offensive+Tweets+and+Their+Targets)|0|
|[Balancing Approach for Causal Inference at Scale](https://doi.org/10.1145/3580305.3599778)|Sicheng Lin, Meng Xu, Xi Zhang, ShihKang Chao, YingKai Huang, Xiaolin Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Balancing+Approach+for+Causal+Inference+at+Scale)|0|
|[Uncertainty-Aware Probabilistic Travel Time Prediction for On-Demand Ride-Hailing at DiDi](https://doi.org/10.1145/3580305.3599925)|Hao Liu, Wenzhao Jiang, Shui Liu, Xi Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncertainty-Aware+Probabilistic+Travel+Time+Prediction+for+On-Demand+Ride-Hailing+at+DiDi)|0|
|[WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences](https://doi.org/10.1145/3580305.3599931)|Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, Jie Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WebGLM:+Towards+An+Efficient+Web-Enhanced+Question+Answering+System+with+Human+Preferences)|0|
|[Impact-Oriented Contextual Scholar Profiling using Self-Citation Graphs](https://doi.org/10.1145/3580305.3599845)|Yuankai Luo, Lei Shi, Mufan Xu, Yuwen Ji, Fengli Xiao, Chunming Hu, Zhiguang Shan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Impact-Oriented+Contextual+Scholar+Profiling+using+Self-Citation+Graphs)|0|
|[A Look into Causal Effects under Entangled Treatment in Graphs: Investigating the Impact of Contact on MRSA Infection](https://doi.org/10.1145/3580305.3599763)|Jing Ma, Chen Chen, Anil Vullikanti, Ritwick Mishra, Gregory Madden, Daniel Borrajo, Jundong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Look+into+Causal+Effects+under+Entangled+Treatment+in+Graphs:+Investigating+the+Impact+of+Contact+on+MRSA+Infection)|0|
|[SAInf: Stay Area Inference of Vehicles using Surveillance Camera Records](https://doi.org/10.1145/3580305.3599952)|Zhipeng Ma, Chuishi Meng, Huimin Ren, Sijie Ruan, Jie Bao, Xiaoting Wang, Tianrui Li, Yu Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SAInf:+Stay+Area+Inference+of+Vehicles+using+Surveillance+Camera+Records)|0|
|[Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network](https://doi.org/10.1145/3580305.3599804)|Jinzhu Mao, Liu Cao, Chen Gao, Huandong Wang, Hangyu Fan, Depeng Jin, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Vulnerable+Nodes+in+Urban+Infrastructure+Interdependent+Network)|0|
|[DRL4Route: A Deep Reinforcement Learning Framework for Pick-up and Delivery Route Prediction](https://doi.org/10.1145/3580305.3599811)|Xiaowei Mao, Haomin Wen, Hengrui Zhang, Huaiyu Wan, Lixia Wu, Jianbin Zheng, Haoyuan Hu, Youfang Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DRL4Route:+A+Deep+Reinforcement+Learning+Framework+for+Pick-up+and+Delivery+Route+Prediction)|0|
|[Deep Offline Reinforcement Learning for Real-world Treatment Optimization Applications](https://doi.org/10.1145/3580305.3599800)|Mila Nambiar, Supriyo Ghosh, Priscilla Ong, Yu En Chan, Yong Mong Bee, Pavitra Krishnaswamy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Offline+Reinforcement+Learning+for+Real-world+Treatment+Optimization+Applications)|0|
|[Rewiring Police Officer Training Networks to Reduce Forecasted Use of Force](https://doi.org/10.1145/3580305.3599899)|Ritika Pandey, Jeremy G. Carter, James Hill, George O. Mohler||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rewiring+Police+Officer+Training+Networks+to+Reduce+Forecasted+Use+of+Force)|0|
|[Extreme Multi-Label Classification for Ad Targeting using Factorization Machines](https://doi.org/10.1145/3580305.3599822)|Martin Pavlovski, Srinath Ravindran, Djordje Gligorijevic, Shubham Agrawal, Ivan Stojkovic, Nelson SeguraNunez, Jelena Gligorijevic||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extreme+Multi-Label+Classification+for+Ad+Targeting+using+Factorization+Machines)|0|
|[un-xPass: Measuring Soccer Player's Creativity](https://doi.org/10.1145/3580305.3599924)|Pieter Robberechts, Maaike Van Roy, Jesse Davis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=un-xPass:+Measuring+Soccer+Player's+Creativity)|0|
|[QTNet: Theory-based Queue Length Prediction for Urban Traffic](https://doi.org/10.1145/3580305.3599890)|Ryu Shirakami, Toshiya Kitahara, Koh Takeuchi, Hisashi Kashima||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QTNet:+Theory-based+Queue+Length+Prediction+for+Urban+Traffic)|0|
|[Deep Transfer Learning for City-scale Cellular Traffic Generation through Urban Knowledge Graph](https://doi.org/10.1145/3580305.3599801)|Shiyuan Zhang, Tong Li, Shuodi Hui, Guangyu Li, Yanping Liang, Li Yu, Depeng Jin, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Transfer+Learning+for+City-scale+Cellular+Traffic+Generation+through+Urban+Knowledge+Graph)|0|
|[Hierarchical Reinforcement Learning for Dynamic Autonomous Vehicle Navigation at Intelligent Intersections](https://doi.org/10.1145/3580305.3599839)|Qian Sun, Le Zhang, Huan Yu, Weijia Zhang, Yu Mei, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Reinforcement+Learning+for+Dynamic+Autonomous+Vehicle+Navigation+at+Intelligent+Intersections)|0|
|[TrustGeo: Uncertainty-Aware Dynamic Graph Learning for Trustworthy IP Geolocation](https://doi.org/10.1145/3580305.3599920)|Wenxin Tai, Bin Chen, Fan Zhou, Ting Zhong, Goce Trajcevski, Yong Wang, Kai Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TrustGeo:+Uncertainty-Aware+Dynamic+Graph+Learning+for+Trustworthy+IP+Geolocation)|0|
|[Automatic Music Playlist Generation via Simulation-based Reinforcement Learning](https://doi.org/10.1145/3580305.3599777)|Federico Tomasi, Joseph Cauteruccio, Surya Kanoria, Kamil Ciosek, Matteo Rinaldi, Zhenwen Dai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Music+Playlist+Generation+via+Simulation-based+Reinforcement+Learning)|0|
|[Stabilising Job Survival Analysis for Disability Employment Services in Unseen Environments](https://doi.org/10.1145/3580305.3599908)|Ha Xuan Tran, Thuc Duy Le, Jiuyong Li, Lin Liu, Xiaomei Li, Jixue Liu, Tony Waters||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stabilising+Job+Survival+Analysis+for+Disability+Employment+Services+in+Unseen+Environments)|0|
|[Fair Multilingual Vandalism Detection System for Wikipedia](https://doi.org/10.1145/3580305.3599823)|Mykola Trokhymovych, Muniza Aslam, AiJou Chou, Ricardo BaezaYates, Diego SáezTrumper||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Multilingual+Vandalism+Detection+System+for+Wikipedia)|0|
|[Auto-Validate by-History: Auto-Program Data Quality Constraints to Validate Recurring Data Pipelines](https://doi.org/10.1145/3580305.3599776)|Dezhan Tu, Yeye He, Weiwei Cui, Song Ge, Haidong Zhang, Shi Han, Dongmei Zhang, Surajit Chaudhuri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Auto-Validate+by-History:+Auto-Program+Data+Quality+Constraints+to+Validate+Recurring+Data+Pipelines)|0|
|[The Missing Indicator Method: From Low to High Dimensions](https://doi.org/10.1145/3580305.3599911)|Mike Van Ness, Tomas M. Bosschieter, Roberto HalpinGregorio, Madeleine Udell||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Missing+Indicator+Method:+From+Low+to+High+Dimensions)|0|
|[Interdependent Causal Networks for Root Cause Localization](https://doi.org/10.1145/3580305.3599849)|Dongjie Wang, Zhengzhang Chen, Jingchao Ni, Liang Tong, Zheng Wang, Yanjie Fu, Haifeng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interdependent+Causal+Networks+for+Root+Cause+Localization)|0|
|[Learning to Discover Various Simpson's Paradoxes](https://doi.org/10.1145/3580305.3599859)|Jingwei Wang, Jianshan He, Weidi Xu, Ruopeng Li, Wei Chu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Discover+Various+Simpson's+Paradoxes)|0|
|[Removing Camouflage and Revealing Collusion: Leveraging Gang-crime Pattern in Fraudster Detection](https://doi.org/10.1145/3580305.3599895)|Lewen Wang, Haozhe Zhao, Cunguang Feng, Weiqing Liu, Congrui Huang, Marco Santoni, Manuel Cristofaro, Paola Jafrancesco, Jiang Bian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Removing+Camouflage+and+Revealing+Collusion:+Leveraging+Gang-crime+Pattern+in+Fraudster+Detection)|0|
|[ShuttleSet: A Human-Annotated Stroke-Level Singles Dataset for Badminton Tactical Analysis](https://doi.org/10.1145/3580305.3599906)|WeiYao Wang, YungChang Huang, TsiUi Ik, WenChih Peng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ShuttleSet:+A+Human-Annotated+Stroke-Level+Singles+Dataset+for+Badminton+Tactical+Analysis)|0|
|[VRDU: A Benchmark for Visually-rich Document Understanding](https://doi.org/10.1145/3580305.3599929)|Zilong Wang, Yichao Zhou, Wei Wei, ChenYu Lee, Sandeep Tata||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VRDU:+A+Benchmark+for+Visually-rich+Document+Understanding)|0|
|[DNet: Distributional Network for Distributional Individualized Treatment Effects](https://doi.org/10.1145/3580305.3599809)|Guojun Wu, Ge Song, Xiaoxiang Lv, Shikai Luo, Chengchun Shi, Hongtu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DNet:+Distributional+Network+for+Distributional+Individualized+Treatment+Effects)|0|
|[A Predict-Then-Optimize Couriers Allocation Framework for Emergency Last-mile Logistics](https://doi.org/10.1145/3580305.3599766)|Kaiwen Xia, Li Lin, Shuai Wang, Haotian Wang, Desheng Zhang, Tian He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Predict-Then-Optimize+Couriers+Allocation+Framework+for+Emergency+Last-mile+Logistics)|0|
|[Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications](https://doi.org/10.1145/3580305.3599833)|Han Xie, Da Zheng, Jun Ma, Houyu Zhang, Vassilis N. Ioannidis, Xiang Song, Qing Ping, Sheng Wang, Carl Yang, Yi Xu, Belinda Zeng, Trishul Chilimbi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-Aware+Language+Model+Pre-Training+on+a+Large+Graph+Corpus+Can+Help+Multiple+Graph+Applications)|0|
|[NEON: Living Needs Prediction System in Meituan](https://doi.org/10.1145/3580305.3599874)|Xiaochong Lan, Chen Gao, Shiqi Wen, Xiuqi Chen, Yingge Che, Han Zhang, Huazhou Wei, Hengliang Luo, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NEON:+Living+Needs+Prediction+System+in+Meituan)|0|
|[AlerTiger: Deep Learning for AI Model Health Monitoring at LinkedIn](https://doi.org/10.1145/3580305.3599802)|Zhentao Xu, Ruoying Wang, Girish Balaji, Manas Bundele, XiaoFei Liu, Leo Liu, Tie Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AlerTiger:+Deep+Learning+for+AI+Model+Health+Monitoring+at+LinkedIn)|0|
|[Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation](https://doi.org/10.1145/3580305.3599774)|Bing Xue, Ahmed Sameh Said, Ziqi Xu, Hanyang Liu, Neel Shah, Hanqing Yang, Philip R. O. Payne, Chenyang Lu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Assisting+Clinical+Decisions+for+Scarcely+Available+Treatment+via+Disentangled+Latent+Representation)|0|
|[Contextual Self-attentive Temporal Point Process for Physical Decommissioning Prediction of Cloud Assets](https://doi.org/10.1145/3580305.3599794)|Fangkai Yang, Jue Zhang, Lu Wang, Bo Qiao, Di Weng, Xiaoting Qin, Gregory Weber, Durgesh Nandini Das, Srinivasan Rakhunathan, Ranganathan Srikanth, Qingwei Lin, Dongmei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextual+Self-attentive+Temporal+Point+Process+for+Physical+Decommissioning+Prediction+of+Cloud+Assets)|0|
|[From Labels to Decisions: A Mapping-Aware Annotator Model](https://doi.org/10.1145/3580305.3599828)|Evan Yao, Jagdish Ramakrishnan, Xu Chen, VietAn Nguyen, Udi Weinsberg||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Labels+to+Decisions:+A+Mapping-Aware+Annotator+Model)|0|
|[M3PT: A Multi-Modal Model for POI Tagging](https://doi.org/10.1145/3580305.3599862)|Jingsong Yang, Guanzhou Han, Deqing Yang, Jingping Liu, Yanghua Xiao, Xiang Xu, Baohua Wu, Shenghua Ni||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M3PT:+A+Multi-Modal+Model+for+POI+Tagging)|0|
|[Self-supervised Classification of Clinical Multivariate Time Series using Time Series Dynamics](https://doi.org/10.1145/3580305.3599954)|Yakir Yehuda, Daniel Freedman, Kira Radinsky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-supervised+Classification+of+Clinical+Multivariate+Time+Series+using+Time+Series+Dynamics)|0|
|[DGI: An Easy and Efficient Framework for GNN Model Evaluation](https://doi.org/10.1145/3580305.3599805)|Peiqi Yin, Xiao Yan, Jinjing Zhou, Qiang Fu, Zhenkun Cai, James Cheng, Bo Tang, Minjie Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DGI:+An+Easy+and+Efficient+Framework+for+GNN+Model+Evaluation)|0|
|[Learning Multivariate Hawkes Process via Graph Recurrent Neural Network](https://doi.org/10.1145/3580305.3599857)|Kanghoon Yoon, Youngjun Im, Jingyu Choi, Taehwan Jeong, Jinkyoo Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Multivariate+Hawkes+Process+via+Graph+Recurrent+Neural+Network)|0|
|[Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning](https://doi.org/10.1145/3580305.3599831)|Shuo Yu, Hongyan Xue, Xiang Ao, Feiyang Pan, Jia He, Dandan Tu, Qing He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generating+Synergistic+Formulaic+Alpha+Collections+via+Reinforcement+Learning)|0|
|[LibAUC: A Deep Learning Library for X-Risk Optimization](https://doi.org/10.1145/3580305.3599861)|Zhuoning Yuan, Dixian Zhu, ZiHao Qiu, Gang Li, Xuanhui Wang, Tianbao Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LibAUC:+A+Deep+Learning+Library+for+X-Risk+Optimization)|0|
|[Towards a Generic Framework for Mechanism-guided Deep Learning for Manufacturing Applications](https://doi.org/10.1145/3580305.3599913)|Hanbo Zhang, Jiangxin Li, Shen Liang, Peng Wang, Themis Palpanas, Chen Wang, Wei Wang, Haoxuan Zhou, Jianwei Song, Wen Lu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+Generic+Framework+for+Mechanism-guided+Deep+Learning+for+Manufacturing+Applications)|0|
|[GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation](https://doi.org/10.1145/3580305.3599832)|Jing Zhang, Xiaokang Zhang, Daniel ZhangLi, Jifan Yu, Zijun Yao, Zeyao Ma, Yiqi Xu, Haohua Wang, Xiaohan Zhang, Nianyi Lin, Sunrui Lu, Juanzi Li, Jie Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GLM-Dialog:+Noise-tolerant+Pre-training+for+Knowledge-grounded+Dialogue+Generation)|0|
|[Robust Multimodal Failure Detection for Microservice Systems](https://doi.org/10.1145/3580305.3599902)|Chenyu Zhao, Minghua Ma, Zhenyu Zhong, Shenglin Zhang, Zhiyuan Tan, Xiao Xiong, LuLu Yu, Jiayi Feng, Yongqian Sun, Yuzhi Zhang, Dan Pei, Qingwei Lin, Dongmei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Multimodal+Failure+Detection+for+Microservice+Systems)|0|
|[CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X](https://doi.org/10.1145/3580305.3599790)|Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, Jie Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CodeGeeX:+A+Pre-Trained+Model+for+Code+Generation+with+Multilingual+Benchmarking+on+HumanEval-X)|0|
|[MIDLG: Mutual Information based Dual Level GNN for Transaction Fraud Complaint Verification](https://doi.org/10.1145/3580305.3599865)|Wen Zheng, Bingbing Xu, Emiao Lu, Yang Li, Qi Cao, Xuan Zong, Huawei Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MIDLG:+Mutual+Information+based+Dual+Level+GNN+for+Transaction+Fraud+Complaint+Verification)|0|
|[Road Planning for Slums via Deep Reinforcement Learning](https://doi.org/10.1145/3580305.3599901)|Yu Zheng, Hongyuan Su, Jingtao Ding, Depeng Jin, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Road+Planning+for+Slums+via+Deep+Reinforcement+Learning)|0|
|[AI Explainability 360 Toolkit for Time-Series and Industrial Use Cases](https://doi.org/10.1145/3580305.3599182)|Giridhar Ganapavarapu, Sumanta Mukherjee, Natalia Martinez Gil, Kanthi K. Sarpatwar, Amaresh Rajasekharan, Amit Dhurandhar, Vijay Arya, Roman Vaculín||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+Explainability+360+Toolkit+for+Time-Series+and+Industrial+Use+Cases)|0|
|[Hands-on Tutorial: "Explanations in AI: Methods, Stakeholders and Pitfalls"](https://doi.org/10.1145/3580305.3599181)|Mia C. Mayer, Muhammad Bilal Zafar, Luca Franceschi, Huzefa Rangwala||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hands-on+Tutorial:+"Explanations+in+AI:+Methods,+Stakeholders+and+Pitfalls")|0|
|[Graph Neural Networks in TensorFlow](https://doi.org/10.1145/3580305.3599177)|Bryan Perozzi, Sami AbuElHaija, Anton Tsitsulin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Neural+Networks+in+TensorFlow)|0|
|[PyHealth: A Deep Learning Toolkit for Healthcare Applications](https://doi.org/10.1145/3580305.3599178)|Chaoqi Yang, Zhenbang Wu, Patrick Jiang, Zhen Lin, Junyi Gao, Benjamin P. Danek, Jimeng Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PyHealth:+A+Deep+Learning+Toolkit+for+Healthcare+Applications)|0|
|[GraphStorm an Easy-to-use and Scalable Graph Neural Network Framework: From Beginners to Heroes](https://doi.org/10.1145/3580305.3599179)|Jian Zhang, Da Zheng, Xiang Song, Theodore Vasiloudis, Israt Nisa, Jim Lu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphStorm+an+Easy-to-use+and+Scalable+Graph+Neural+Network+Framework:+From+Beginners+to+Heroes)|0|
|[Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques](https://doi.org/10.1145/3580305.3599572)|Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, Zhou Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Next-Generation+Intelligent+Assistants+Leveraging+LLM+Techniques)|0|
|[XAI for Predictive Maintenance](https://doi.org/10.1145/3580305.3599578)|João Gama, Slawomir Nowaczyk, Sepideh Pashami, Rita P. Ribeiro, Grzegorz J. Nalepa, Bruno Veloso||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=XAI+for+Predictive+Maintenance)|0|
|[Distributed Optimization for Big Data Analytics: Beyond Minimization](https://doi.org/10.1145/3580305.3599554)|Hongchang Gao, Xinwen Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributed+Optimization+for+Big+Data+Analytics:+Beyond+Minimization)|0|
|[Privacy in Advertising: Analytics and Modeling](https://doi.org/10.1145/3580305.3599570)|Badih Ghazi, Ravi Kumar, Pasin Manurangsi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy+in+Advertising:+Analytics+and+Modeling)|0|
|[Causal Discovery from Temporal Data](https://doi.org/10.1145/3580305.3599552)|Chang Gong, Di Yao, Chuzhe Zhang, Wenbin Li, Jingping Bi, Lun Du, Jin Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Discovery+from+Temporal+Data)|0|
|[Generative AI meets Responsible AI: Practical Challenges and Opportunities](https://doi.org/10.1145/3580305.3599557)|Krishnaram Kenthapadi, Himabindu Lakkaraju, Nazneen Rajani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+AI+meets+Responsible+AI:+Practical+Challenges+and+Opportunities)|0|
|[Getting an h-Index of 100 in 20 Years or Less!](https://doi.org/10.1145/3580305.3599558)|Eamonn Keogh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Getting+an+h-Index+of+100+in+20+Years+or+Less!)|0|
|[Uncertainty Quantification in Deep Learning](https://doi.org/10.1145/3580305.3599577)|Lingkai Kong, Harshavardhan Kamarthi, Peng Chen, B. Aditya Prakash, Chao Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncertainty+Quantification+in+Deep+Learning)|0|
|[Mining of Real-world Hypergraphs: Patterns, Tools, and Generators](https://doi.org/10.1145/3580305.3599567)|Geon Lee, Jaemin Yoo, Kijung Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+of+Real-world+Hypergraphs:+Patterns,+Tools,+and+Generators)|0|
|[Knowledge Graph Reasoning and Its Applications](https://doi.org/10.1145/3580305.3599564)|Lihui Liu, Hanghang Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Reasoning+and+Its+Applications)|0|
|[Fast Text Generation with Text-Editing Models](https://doi.org/10.1145/3580305.3599579)|Eric Malmi, Yue Dong, Jonathan Mallinson, Aleksandr Chuklin, Jakub Adámek, Daniil Mirylenka, Felix Stahlberg, Sebastian Krause, Shankar Kumar, Aliaksei Severyn||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Text+Generation+with+Text-Editing+Models)|0|
|[Pretrained Language Representations for Text Understanding: A Weakly-Supervised Perspective](https://doi.org/10.1145/3580305.3599569)|Yu Meng, Jiaxin Huang, Yu Zhang, Yunyi Zhang, Jiawei Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pretrained+Language+Representations+for+Text+Understanding:+A+Weakly-Supervised+Perspective)|0|
|[Precision Health in the Age of Large Language Models](https://doi.org/10.1145/3580305.3599568)|Hoifung Poon, Tristan Naumann, Sheng Zhang, Javier González Hernández||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Precision+Health+in+the+Age+of+Large+Language+Models)|0|
|[Trustworthy Transfer Learning: Transferability and Trustworthiness](https://doi.org/10.1145/3580305.3599576)|Jun Wu, Jingrui He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trustworthy+Transfer+Learning:+Transferability+and+Trustworthiness)|0|
|[Graph Neural Networks: Foundation, Frontiers and Applications](https://doi.org/10.1145/3580305.3599560)|Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao, Xiaojie Guo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Neural+Networks:+Foundation,+Frontiers+and+Applications)|0|
|[Graph and Geometry Generative Modeling for Drug Discovery](https://doi.org/10.1145/3580305.3599559)|Minkai Xu, Meng Liu, Wengong Jin, Shuiwang Ji, Jure Leskovec, Stefano Ermon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+and+Geometry+Generative+Modeling+for+Drug+Discovery)|0|
|[Data-centric AI: Techniques and Future Perspectives](https://doi.org/10.1145/3580305.3599553)|Daochen Zha, KweiHerng Lai, Fan Yang, Na Zou, Huiji Gao, Xia Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data-centric+AI:+Techniques+and+Future+Perspectives)|0|
|[Hyperbolic Graph Neural Networks: A Tutorial on Methods and Applications](https://doi.org/10.1145/3580305.3599562)|Min Zhou, Menglin Yang, Bo Xiong, Hui Xiong, Irwin King||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyperbolic+Graph+Neural+Networks:+A+Tutorial+on+Methods+and+Applications)|0|
|[Fragile Earth: AI for Climate Sustainability - From Wildfire Disaster Management to Public Health and Beyond](https://doi.org/10.1145/3580305.3599217)|Naoki Abe, Kathleen Buckingham, Yuzhou Chen, Bistra Dilkina, Emre Eftelioglu, Auroop R. Ganguly, Yulia R. Gel, James Hodson, Ramakrishnan Kannan, Huikyo Lee, Jiafu Mao, Rose Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fragile+Earth:+AI+for+Climate+Sustainability+-+From+Wildfire+Disaster+Management+to+Public+Health+and+Beyond)|0|
|[AdKDD 2023](https://doi.org/10.1145/3580305.3599582)|Abraham Bagherjeiran, Nemanja Djuric, KuangChih Lee, Linsey Pang, Vladan Radosavljevic, Suju Rajan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdKDD+2023)|0|
|[Robust NLP for Finance (RobustFin)](https://doi.org/10.1145/3580305.3599211)|Sameena Shah, Xiaodan Zhu, Gerard de Melo, Armineh Nourbakhsh, Xiaomo Liu, Zhiqiang Ma, Charese Smiley, Zhiyu Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+NLP+for+Finance+(RobustFin))|0|
|[From Innovation to Scale (I2S) - Discuss and Learn How to Successfully Build, Commercialize, and Scale AI Innovations in Challenging Market Conditions](https://doi.org/10.1145/3580305.3599580)|Ankur M. Teredesai, Michael Zeller, Shenghua Bao, Wee Hyong Tok, Linsey Pang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Innovation+to+Scale+(I2S)+-+Discuss+and+Learn+How+to+Successfully+Build,+Commercialize,+and+Scale+AI+Innovations+in+Challenging+Market+Conditions)|0|
|[Deep Learning on Graphs: Methods and Applications (DLG-KDD2023)](https://doi.org/10.1145/3580305.3599207)|Lingfei Wu, Jian Pei, Jiliang Tang, Yinglong Xia, Xiaojie Guo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Learning+on+Graphs:+Methods+and+Applications+(DLG-KDD2023))|0|
|[Scenario-Adaptive Feature Interaction for Click-Through Rate Prediction](https://doi.org/10.1145/3580305.3599936)|Erxue Min, Da Luo, Kangyi Lin, Chunzhen Huang, Yang Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scenario-Adaptive+Feature+Interaction+for+Click-Through+Rate+Prediction)|-1|
|[Sequential Learning Algorithms for Contextual Model-Free Influence Maximization](https://doi.org/10.1145/3580305.3599498)|Alexandra Iacob, Bogdan Cautis, Silviu Maniu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sequential+Learning+Algorithms+for+Contextual+Model-Free+Influence+Maximization)|-1|
|[LightToken: A Task and Model-agnostic Lightweight Token Embedding Framework for Pre-trained Language Models](https://doi.org/10.1145/3580305.3599416)|Haoyu Wang, Ruirui Li, Haoming Jiang, Zhengyang Wang, Xianfeng Tang, Bin Bi, Monica Xiao Cheng, Bing Yin, Yaqing Wang, Tuo Zhao, Jing Gao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LightToken:+A+Task+and+Model-agnostic+Lightweight+Token+Embedding+Framework+for+Pre-trained+Language+Models)|-1|
|[Automatic Temporal Relation in Multi-Task Learning](https://doi.org/10.1145/3580305.3599261)|Menghui Zhou, Po Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Temporal+Relation+in+Multi-Task+Learning)|-1|
|[Empowering General-purpose User Representation with Full-life Cycle Behavior Modeling](https://doi.org/10.1145/3580305.3599331)|Bei Yang, Jie Gu, Ke Liu, Xiaoxiao Xu, Renjun Xu, Qinghui Sun, Hong Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empowering+General-purpose+User+Representation+with+Full-life+Cycle+Behavior+Modeling)|-1|
|[Towards Understanding and Enhancing Robustness of Deep Learning Models against Malicious Unlearning Attacks](https://doi.org/10.1145/3580305.3599526)|Wei Qian, Chenxu Zhao, Wei Le, Meiyi Ma, Mengdi Huai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Understanding+and+Enhancing+Robustness+of+Deep+Learning+Models+against+Malicious+Unlearning+Attacks)|-1|
|[Virtual Node Tuning for Few-shot Node Classification](https://doi.org/10.1145/3580305.3599541)|Zhen Tan, Ruocheng Guo, Kaize Ding, Huan Liu|Arizona State University|Few-shot Node Classification (FSNC) is a challenge in graph representation learning where only a few labeled nodes per class are available for training. To tackle this issue, meta-learning has been proposed to transfer structural knowledge from base classes with abundant labels to target novel classes. However, existing solutions become ineffective or inapplicable when base classes have no or limited labeled nodes. To address this challenge, we propose an innovative method dubbed Virtual Node Tuning (VNT). Our approach utilizes a pretrained graph transformer as the encoder and injects virtual nodes as soft prompts in the embedding space, which can be optimized with few-shot labels in novel classes to modulate node embeddings for each specific FSNC task. A unique feature of VNT is that, by incorporating a Graph-based Pseudo Prompt Evolution (GPPE) module, VNT-GPPE can handle scenarios with sparse labels in base classes. Experimental results on four datasets demonstrate the superiority of the proposed approach in addressing FSNC with unlabeled or sparsely labeled base classes, outperforming existing state-of-the-art methods and even fully supervised baselines.|少镜头节点分类(FSNC)是图表示学习中的一个挑战，每个类只有少量标记节点可用于训练。为了解决这个问题，元学习已经被提出来将结构化知识从有大量标签的基类转移到新类中。但是，当基类没有或只有有限的标记节点时，现有的解决方案将变得无效或不适用。为了应对这一挑战，我们提出了一种称为虚拟节点优化(Virtual Node Tuning，VNT)的创新方法。该方法利用预先训练好的图形变换器作为编码器，在嵌入空间中注入虚拟节点作为软提示，并通过新类中的少镜头标签优化，调节节点嵌入，以适应每个特定的 FSNC 任务。VNT 的一个独特特性是，通过合并一个基于图的伪提示演化(PseudoPrompt Evolution，GPPE)模块，VNT-GPPE 可以处理基类中带有稀疏标签的场景。在四个数据集上的实验结果表明，该方法在处理未标记或稀疏标记基类的 FSNC 问题上具有优越性，其性能优于现有的最先进的方法，甚至优于完全监督的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Virtual+Node+Tuning+for+Few-shot+Node+Classification)|-1|
|[Select and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning](https://doi.org/10.1145/3580305.3599951)|Weiguang Han, Boyi Zhang, Qianqian Xie, Min Peng, Yanzhao Lai, Jimin Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Select+and+Trade:+Towards+Unified+Pair+Trading+with+Hierarchical+Reinforcement+Learning)|-1|
